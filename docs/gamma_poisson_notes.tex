\documentclass[11pt]{article}
\usepackage{amssymb,amsmath}

\begin{document}
\title{The Gamma-Poisson Model}
\author{John Pearson}
\maketitle

\section{Observation Model}
We assume a Poisson process with $T$ observation times, $N$ observation units, and $K$ latent states. The counts for each unit at each time are then given by 

\begin{equation}
    N_{tu} \sim \mathrm{Poisson}(\mu_{tu})    
\end{equation}
with 
\begin{equation}
   \mu_{tu} = (ZB)_{tu} = \sum_k z_{tk} b_{ku} 
\end{equation}
wit $z$ the time series of latent states (assumed binary) and $b$ the regression coefficients. In addition, we assume the $z_t$ have independent Markov transition dynamics within each latent state (see below).

In addition, it will be more convenient for our purposes to rewrite the above expression using $b_{ku} = \log \lambda_{ku}$, so that 
\begin{equation}
    p(N_{tu}|\lambda, z) = \frac{\prod_k \lambda_{ku}^{N_{tu} z_{tk}}}{N_{tu}!} e^{-\prod_k \lambda_{ku}^{z_{tk}}}
\end{equation}
which can be rewritten
\begin{equation}
    \log p(N_{tu}|\lambda, z) = N_{tu} \sum_k z_{tk} \log \lambda_{ku} - \prod_k \lambda_{ku}^{z_{tk}} - \log N_{tu}!
\end{equation}
Clearly, if we treat $N$ and $z$ as constants, we have
\begin{equation}
    \lambda_{ku}|N, z \sim \mathrm{Gamma}(\sum_t N_{tu}z_{tk} + 1, \sum_t\prod_{k'\neq k} \lambda_{k'u}^{z_{tk'}})
\end{equation}

\section{Hidden Markov Model}
Here, we assume a hidden Markov model for each latent state. That is, we take
\begin{align}
    A_{ij} &\equiv p(z_{t+1}=i|z_t = j) \\
    \pi_i &\equiv p(z_0 = i)
\end{align}
(Note that this means that the \emph{columns} of $A$ sum to 1, which is the opposite of the usual convention. In other words, in matrix notation, $z_{t+1} = A \cdot z_t$.)

Given this notation, it is straightforward to write the probability of a sequence of hidden states, conditioned on the chain parameters
\begin{equation}
    \log p(z|A, \pi) = \sum_t \log A_{z_{t+1} z_t} + \log \pi_{z_0}
\end{equation}

\section{Priors}
We would also like to put priors on the parameters of the model. For $\lambda$, this is straightforward:
\begin{equation}
    \lambda_{ku} \sim \mathrm{Gamma}(c_{ku}, d_{ku})
\end{equation}

For the parameters of the Markov chain, we have the restriction that the columns must sum to 1, so we place priors on the probability of $z_{0k} = 1$
\begin{equation}
    \pi_{k} \sim \mathrm{Beta}(\rho_{k1}, \rho_{k2})
\end{equation}
and the probabilities of transition \emph{into} the $z_{tk} = 1$ state:
\begin{equation}
    A_{k1i} \sim \mathrm{Beta}(\nu_{ki1}, \nu_{ki2})
\end{equation}

\section{Variational Ansatz}
We would like to approximate the joint posterior density
\begin{equation}
    p(\lambda, z, A, \pi|N) \propto p(N, z|\lambda, A, \pi) p(\lambda) p(A) p(\pi)
\end{equation}
with a structured mean field form that factorizes over chains:
\begin{equation}
     q(\lambda, z, A, \pi) = \prod_{ku} q(\lambda_{ku}) q(z_k) q(A_k) q(\pi_k)
\end{equation} 
Given the form taken by the model above, we can readily write down the ansatz
\begin{align} 
    \lambda_{ku} &\sim \mathrm{Gamma}(\alpha_{ku}, \beta_{ku}) \\
    z_{1:T, k} &\sim \mathrm{HMM}(\eta_k, \tilde{A}_k, \tilde{\pi}_k) \\
    A_{k1i} &\sim \mathrm{Beta}(\gamma_{ki1}, \gamma_{ki2}) \\
    \pi_k &\sim \mathrm{Beta}(\delta_{k1}, \delta_{k2})
\end{align}
where $\mathrm{HMM}(\ldots)$ denotes the joint posterior over hidden states at each moment in time:
\begin{equation}
    p(z_{1:T}|N, \theta) = \frac{p(z_{1:T}, N|\theta)}{Z}
\end{equation}
where $Z \equiv p(N|\theta)$ and $\theta \equiv (\lambda, A, \pi)$. As we shall see, this posterior is straightforward to calculate via the forward-backward algorithm.

\section{HMM Inference}
Given parameters $\theta \equiv (\lambda, A, \pi)$, the well-known Forward-Backward Algorithm returns the following:
\begin{align}
    \xi_{t} \equiv p(z_{t}|N, \theta) \qquad &\text{posterior marginals}\\
    \Xi_{t, ij} \equiv p(z_{t+1} = j, z_{t} = i|N, \theta) \qquad &\text{two-slice marginals}\\
    \log Z_{t} = \log p(N_{t+1, \bullet}|N_{t\bullet}, \theta) \qquad &\text{partition function}
\end{align}
The first two will be helpful in calculating expections with respect to $q(z)$, while the last gives us the normalization for the joint posterior over all $z$: $\log Z = \sum_t \log Z_{t} = \log p(N_{1:T}|\theta)$. 

\section{Evidence Lower Bound (ELBO)}
We begin by writing the joint distribution of the data:
\begin{multline}
    \log p(N, \lambda, z, A,\pi) = \sum_{ktu}\left[ N_{tu} z_{tk} \log \lambda_{ku} - \prod_k \lambda_{ku}^{z_{tk}} - \log N_{tu}!\right] \\
    + \sum_{tk} \log (A_k)_{z_{t+1, k} z_{t, k}} + \log (\pi_k)_{z_{0k}} \\
    + \log p(\lambda) + \log p(A) + \log p(\pi) 
\end{multline}

We would like to calculate 
\begin{multline}
    \mathcal{L} = \mathbb{E}_q\left[\log \frac{p}{q}\right] = \mathbb{E}_{q(\pi)} \left[\log \frac{p(\pi)}{q(\pi)} \right] + \mathbb{E}_{q(A)} \left[\log \frac{p(A)}{q(A)} \right] + \mathbb{E}_{q(\lambda)} \left[\log \frac{p(\lambda)}{q(\lambda)} \right] \\ + \mathbb{E}_{q}\left[ \log \frac{p(N, z|\lambda, A, \pi)}{q(z)}\right] 
\end{multline}
which we will do in pieces. 

\subsection{$\pi$}
First, we have
\begin{equation}
    \mathbb{E}_{q(\pi)} \left[\log \frac{p(\pi)}{q(\pi)} \right] = \sum_k \left[(\rho_{k1} - 1)\overline{\log \pi_{k1}} + (\rho_{k2} - 1) \overline{\log \pi_{k0}} - \log B(\rho_{k1}, \rho_{k2}) + H(\pi_k) \right]
\end{equation}
with
\begin{align}
    \overline{\log \pi_{k1}} &= \psi(\delta_{k1}) - \psi(\delta_{k1} + \delta_{k2}) \\
    \overline{\log \pi_{k0}} &= \psi(\delta_{k2}) - \psi(\delta_{k1} + \delta_{k2})
\end{align}
with $\psi$ the digamma function and 
\begin{equation}
    H(\pi_k) = H_b(\delta_{k1}, \delta_{k2})
\end{equation}
with $H_b$ the entropy of the beta distribution:
\begin{equation}
    H_b(\alpha, \beta) = \log B(\alpha, \beta) - (\alpha - 1) \psi(\alpha) - (\beta - 1) \psi(\beta) + (\alpha + \beta - 2)\psi(\alpha + \beta)
\end{equation}

\subsection{$A$} 
Similarly,
\begin{equation}
    \mathbb{E}_{q(A)} \left[\log \frac{p(A)}{q(A)} \right] = 
\sum_{ik} \left[ (\nu_{ki1} - 1) \overline{\log A_{k1i}} + (\nu_{ki2} - 1) \overline{\log A_{k0i}} - \log B(\nu_{ki1}, \nu_{ki2}) + H(A_{k1i}) \right]
\end{equation}
where 
\begin{align}
    \overline{\log A_{k1i}} &= \psi(\gamma_{ki1}) - \psi(\gamma_{ki1} + \gamma_{ki2}) \\
    \overline{\log A_{k0i}} &= \psi(\gamma_{ki2}) - \psi(\gamma_{ki1} + \gamma_{ki2})
\end{align}
and again
\begin{equation}
    H(A_{k1i}) = H_b(\gamma_{ki1}, \gamma_{ki2})
\end{equation}

\subsection{$\lambda$}
Next, we want to calculate terms involving $\lambda$, for which we will use some properties of Gamma distributions:
\begin{align}
   \mathbb{E}[\lambda] &= \frac{\alpha}{\beta} \\
   \mathbb{E}[\log \lambda] &= \psi(\alpha) - \log \beta \\
   H_g(\alpha, \beta) &= \alpha - \log \beta + \log \Gamma(\alpha) + (1 - \alpha)\psi(\alpha) 
\end{align}
to write
\begin{equation}
    \mathbb{E}_{q(\lambda)} \left[\log \frac{p(\lambda)}{q(\lambda)} \right] = \sum_{ku} \left[ (c_{ku} - 1)\overline{\log \lambda_{ku}} + d_{ku} \frac{\alpha_{ku}}{\beta_{ku}} + H_g(\alpha_{ku}, \beta_{ku})\right]
\end{equation}
with
\begin{equation}
    \overline{\log \lambda_{ku}} = \psi(\alpha_{ku}) - \log \beta_{ku}
\end{equation}

\subsection{Observation model}
Finally, we would like to calculate
\begin{multline}
    \mathbb{E}_{q}\left[ \log \frac{p(N, z|\lambda, A, \pi)}{q(z)}\right] = \sum_{ktu} N_{tu}\xi_{tk}\overline{\log \lambda_{ku}} - \sum_{tu} \prod_k \left( 1 - \xi_{tk} + \xi_{tk} \frac{\alpha_{ku}}{\beta_{ku}}\right) 
    \\
    + \sum_{kt} \left[\mathrm{tr}\left(\Xi_{kt} \overline{\log A_k^T}\right) + \xi_{0k}^T \overline{\log \pi_k} \right]
    \\
    - \sum_{kt} \left[ \xi_{tk}^T \eta_{tk} + \mathrm{tr}\left(\Xi_{kt} \tilde{A}_k^T\right) + \xi_{0k}^T \tilde{\pi}_k - \log Z_{kt} \right]
\end{multline}
where we make use of the outputs of the forward-backward algorithm and the vector/matrix representations of $\tilde{A}$, $\tilde{\pi}$, $\eta$, $\xi$, and $\Xi$. 

\section{Variational Updates}
Technically, the variational parameters above are $(\alpha, \beta, \gamma, \delta, \eta, \tilde{A}, \tilde{\pi})$. However, $\xi = \xi(\eta, \tilde{A}, \tilde{\pi})$ and $\Xi = \Xi(\eta, \tilde{A}, \tilde{\pi})$ so we can instead treat these as the effective variational parameters. This then readily gives as updates:
\begin{align}
    \eta_{tk} &\leftarrow 
    \begin{pmatrix}
        -\sum_u F_{tku} \\
        \sum_u N_{tu} \overline{\log \lambda_{ku}} -
        \sum_u \frac{\alpha_{ku}}{\beta_{ku}} F_{tku} 
    \end{pmatrix} \\
    \tilde{A}_{k} &\leftarrow \overline{\log A_k} \\
    \tilde{\pi}_k &\leftarrow \overline{\log \pi_k}
\end{align}
where we define
\begin{equation}
    F_{tku} \equiv \prod_{j \neq k} \left( 1 - \xi_{tj} + \xi_{tj} \frac{\alpha_{ju}}{\beta_{ju}}\right)
\end{equation}
We note, after Beal, that as a result of these updates, $\tilde{A}$ and $\tilde{\pi}$ are subadditive (i.e., they do not sum to 1), but that the forward-backward algorithm nonetheless returns a correctly normalized posterior.


Similarly, variation with respect to the prior parameters for $(\lambda, A, \pi)$ gives
\begin{align}
    \alpha_{ku} &\leftarrow \sum_t N_{tu} \xi_{tk} + c_{ku} \\
    \beta_{ku} &\leftarrow \sum_t F_{tku}\xi_{tk} + d_{ku} \\
    \gamma_{ki1} &\leftarrow \nu_{ki1} + \sum_t \Xi_{kt, 1i} \\
    \gamma_{ki2} &\leftarrow \nu_{ki2} + \sum_t \Xi_{kt, 0i} \\
    \delta_{k1} &\leftarrow \rho_{k1} + \xi_{0k} \\
    \delta_{k2} &\leftarrow \rho_{k2} + 1 - \xi_{0k} 
\end{align}
These, then, give the proper updates for coordinate ascent.

\section{Multiple observations}
We have thus far used the time index $t$ assuming no ambiguity between stimulus time and presentation time. In the case that each stimulus is presented once, this presents no problem, but here we extend the analysis to the case where some stimuli are presented multiple times. To do so, we will continue to let $t$ designate the time within the stimulus but introduce an additional index $m$ that takes on a unique value for each stimulus presentation. Thus, different trials will have different values of $m$, but may have the same value of $t$.

We can easily modify the observation model to incorporate this setup. Instead of observing a single count $N_{tu}$ for a given unit at a given stimulus time, we observe a \emph{set} of counts $\{ N_{m u} \vert t(m) = t \}$. (Here, we abuse notation by writing $t$ as both the index and the mapping from presentation index to stimulus time.) If we assume trials are independent, the log likelihoods of these observations simply add, in which case we simply replace sums over $t$ with sums over $m$: 
\begin{align}
    \sum_{ktu} N_{tu} \xi_{tk} \overline{\log \lambda_{ku}} &\rightarrow \sum_{km u} N_{m u} \xi_{t(m) k} \overline{\log \lambda_{ku}} \\
    \sum_{tu} \prod_k \left( 1 - \xi_{tk} + \xi_{tk} \frac{\alpha_{ku}}{\beta_{ku}}\right) &\rightarrow \sum_{m u} \prod_k \left( 1 - \xi_{t(m) k} + \xi_{t(m) k} \frac{\alpha_{ku}}{\beta_{ku}}\right)
\end{align}
Clearly, if we group the sum over $m$ into sums over trials at the same stimulus time $t$, we can easily rewrite these expressions in the form
\begin{align}
    \sum_{km u} N_{m u} \xi_{t(m) k} \overline{\log \lambda_{ku}} &= \sum_{ktu} \hat{N}_{tu} \xi_{tk} \overline{\log \lambda_{ku}} \\
    \sum_{m u} \prod_k \left( 1 - \xi_{t(m) k} + \xi_{t(m) k} \frac{\alpha_{ku}}{\beta_{ku}}\right) &= \sum_{tu} M_{tu}\prod_k \left( 1 - \xi_{tk} + \xi_{tk} \frac{\alpha_{ku}}{\beta_{ku}}\right)
\end{align}
where we have defined
\begin{align}
    \hat{N}_{tu} = \sum_{m:\, t(m) = t} N_{m u} \\
    M_{tu} = \sum_{m:\, t(m) = t} 1
\end{align}
as the cumulative spike count and effective number of observations at each stimulus time, respectively. Note that the result of this process has simply been to replace $N \rightarrow \hat{N}$, $F \rightarrow M F$, meaning we can write modified update equations as follows:
\begin{align}
    \eta_{tk} &\leftarrow 
    \begin{pmatrix}
        -\sum_u M_{tu} F_{tku} \\
        \sum_u \hat{N}_{tu} \overline{\log \lambda_{ku}} -
        \sum_u M_{tu}\frac{\alpha_{ku}}{\beta_{ku}} F_{tku} 
    \end{pmatrix} \\
    \alpha_{ku} &\leftarrow \sum_t \hat{N}_{tu} \xi_{tk} + c_{ku} \\
    \beta_{ku} &\leftarrow \sum_t M_{tu} F_{tku}\xi_{tk} + d_{ku} \\
\end{align}
In fact, for simplicity in what follows, we will drop the hat notation on $N$, with the understanding that when $N$ appears with a $t$ index, it is the sum over all counts at that time.

\section{Overdispersion}
To model overdispersion in our count data, we can include an additional moment-by-moment fluctuation in the firing rate of each unit:
\begin{equation}
    (\lambda_{\mathrm{eff}})_{tu} = \prod_k \lambda_{ku}^{z_{tk}}\theta_{tu}
\end{equation}
If, like $\lambda$, $\theta$ is assumed to be gamma-distributed, then this is equivalent (upon marginalizing $\theta$) to a negative binomial model for the counts $N$. However, it will be simpler for the variational updating if we retain $\theta$. We will assume that the overdispersion varies across units but not in time:
\begin{align}
    p(\theta_{mu}) &= \mathrm{Gamma}(s_u, r_u) \\
    q(\theta_{mu}) &= \mathrm{Gamma}(\omega_{mu}, \zeta_{mu})
\end{align}
and include additional terms in the pieces of the evidence lower bound above:
\begin{align}
    \sum_{ktu} N_{tu} \xi_{tk} \overline{\log \lambda_{ku}} &\rightarrow \sum_{kmu} N_{mu} \xi_{t(m)k} \overline{\log \lambda_{ku}} + \sum_{mu} N_{mu} \overline{\log \theta_{mu}} \\
    &= \sum_{ktu} N_{tu} \xi_{tk} \overline{\log \lambda_{ku}} + \sum_{mu} N_{mu} \overline{\log \theta_{mu}} \\
    - \sum_{tu} \prod_k \left( 1 - \xi_{tk} + \xi_{tk} \frac{\alpha_{ku}}{\beta_{ku}}\right) &\rightarrow - \sum_{mu} \frac{\omega_{mu}}{\zeta_{mu}} \prod_k \left( 1 - \xi_{t(m)k} + \xi_{t(m)k} \frac{\alpha_{ku}}{\beta_{ku}}\right)
\end{align}
Note in particular that the overdispersion term is assumed to always be present, and that it is unique to every stimulus presentation ($m$ index), not merely every moment during the stimulus itself ($t$ index). Nevertheless, the fact that the prior for this term depends only on the unit, $u$, means that we can aggregate sufficient statistics across repetitions, eliminating the $m$ index in favor of $t$.

yielding
\begin{multline}
    \mathbb{E}_{q(\theta)} \left[ \log \frac{p(\theta)}{q(\theta)} \right] =
    \sum_{mu} \left[ (s_u - 1) \overline{\log \theta_{mu}} - r_u \overline{\theta_{mu}} + H_g(\omega_u, \zeta_u) \right] =\\
    \sum_{mu} \left[ (s_u - 1) (\psi(\omega_{mu}) - \log \zeta_{mu}) - r_u \frac{\omega_{mu}}{\zeta_{mu}} + H_g(\omega_{mu}, \zeta_{mu}) \right]
\end{multline}
with $H_g$ the differential entropy for the gamma distribution, as above. By analogy with $\lambda$, we can easily write down the updates for $\omega$ and $\zeta$
\begin{align}
    \omega_{mu} &\leftarrow N_{mu} + s_u \\
    \zeta_{mu} &\leftarrow F_{t(m)u} + r_u 
\end{align}
with 
\begin{equation}
    F_{tu} \equiv \prod_k \left( 1 - \xi_{tk} + \xi_{tk} \frac{\alpha_{ku}}{\beta_{ku}}\right)
\end{equation}
Similarly, the update for the local emission probability, $\eta$ takes the form
\begin{equation}
    \eta_{tk} \leftarrow 
    \begin{pmatrix}
        -\sum_{u, m: \, t(m) = t} \frac{\omega_{mu}}{\zeta_{mu}} F_{t(m)ku} \\
        \sum_u N_{tu} \overline{\log \lambda_{ku}} -
        \sum_{u, m: \, t(m) = t} \frac{\omega_{mu}} {\zeta_{mu}}\frac{\alpha_{ku}}{\beta_{ku}} F_{t(m)ku} 
    \end{pmatrix} 
\end{equation}
All other updates should remain unchanged.

\section{External covariates}
\subsection{Gamma priors}
In addition, we might also want to include external covariates $X(t)$ with an effect on firing rate. Thus we will include an $X$-dependent gain change of firing rate:
\begin{equation}
    \lambda_{u}(t) \propto \prod_i \upsilon_{iu}^{x_{iu}(t)}
\end{equation}
where $i$ indexes the particular regressor. Thus the total expression for firing rate for a given unit $u$ at time $t$ for a given presentation $m$ with overdispersion is
\begin{equation}
    \lambda_{mu} = \theta_{mu}\left(\prod_k \lambda_{ku}^{z_{t(m)k}} \right) \left(\prod_i \upsilon_{iu}^{x_{t(m)iu}} \right)
\end{equation}
Note that we have discretized $t$ as above and assume that the external covariates $x$ depend only on the stimulus time, not the particular presentation, so that they carry an index $t(m)$, while $\theta$ carries an index particular to each repeated presentation.

It will be convenient for us to assume that both the priors and the variational posterior for $\upsilon$ are gamma in form:
\begin{align}
    p(\upsilon_{iu}) &= \mathrm{Gamma}(v_{iu}, w_{iu}) \\
    q(\upsilon_{iu}) &= \mathrm{Gamma}(a_{iu}, b_{iu}) 
\end{align}
This contributes an additional factor to the ELBO
\begin{multline}
    \mathbb{E}_{q(\upsilon)} \left[ \log \frac{p(\upsilon)}{q(\upsilon)} \right] =
    \sum_{iu} \left[ (v_{iu} - 1) \overline{\log \upsilon_{iu}} - w_{iu} \frac{a_{iu}}{b_{iu}} + H_g(a_{iu}, b_{iu}) \right] =\\
    \sum_{iu} \left[ (v_{iu} - 1) (\psi(a_{iu}) - \log b_{iu}) - w_{iu} \frac{a_{iu}}{b_{iu}} + H_g(a_{iu}, b_{iu}) \right]
\end{multline}
plus corrections to the observation probability piece of $\mathbb{E}_q[\log p(N, z|\lambda, A, \pi) / q(z)]$:
\begin{multline}
    \sum_{ktu} N_{tu} \xi_{tk} \overline{\log \lambda_{ku}} + \sum_{mu} N_{mu} \overline{\log \theta_{mu}} + \sum_{tiu} N_{tu} x_{tiu} \overline{\log \upsilon_{iu}} \\
    - \sum_{mu} \frac{\omega_{mu}}{\zeta_{mu}} \prod_k \left( 1 - \xi_{t(m)k} + \xi_{t(m)k} \frac{\alpha_{ku}}{\beta_{ku}}\right)\prod_i \left(\frac{a_{iu}}{b_{iu}} \right)^{x_{t(m)iu}}
\end{multline}

This leads straightforwardly to updates for the posterior parameters of $q(\upsilon)$:
\begin{align}
    a_{iu} &\leftarrow \sum_t N_{tu} x_{tiu} + v_{iu} \\
\end{align}
For $b_{iu}$, we can more easily calculate the gradient with respect to the parameter $\log b$:
\begin{align}
    \nabla_{\log b_{iu}} \mathcal{L} = &-(v_{iu} - 1) + 
    w_{iu} \frac{a_{iu}}{b_{iu}} - 1 - \sum_t N_{tu} x_{tiu} \\
    & + \sum_m x_{t(m)iu} \frac{\omega_{mu}}{\zeta_{mu}} 
    \prod_k \left(1 - \xi_{t(m)k} + \xi_{t(m)k} 
    \frac{\alpha_{ku}}{\beta_{ku}}\right)
    \prod_j \left(\frac{a_{ju}}{b_{ju}}\right)^{x_{t(m)ju}} \\
    &= -a_{iu} + w_{iu} \frac{a_{iu}}{b_{iu}} 
    + \sum_m x_{t(m)iu} \frac{\omega_{mu}}{\zeta_{mu}} 
    \prod_k \left(1 - \xi_{t(m)k} + \xi_{t(m)k} 
    \frac{\alpha_{ku}}{\beta_{ku}}\right)
    \prod_j \left(\frac{a_{ju}}{b_{ju}}\right)^{x_{t(m)ju}} 
\end{align}
Thus, the updated value of $b_{iu}$, $b^*_{iu}$, is the solution to the transcendental equation
\begin{equation}
    b_{iu} = w_{iu} + \left( \frac{b_{iu}}{a_{iu}}\right) \sum_{m} x_{t(m)iu}\left[ \frac{\omega_{mu}}{\zeta_{mu}} \prod_k \left( 1 - \xi_{t(m)k} + \xi_{t(m)k} \frac{\alpha_{ku}}{\beta_{ku}}\right)\prod_{j} \left(\frac{a_{ju}}{b_{ju}} \right)^{x_{t(m)ju}} \right]  
\end{equation}

Alternately, since we expect $\lvert (a/b) - 1 \rvert \ll 1$ when $x \gg 1$, we can write $b_{iu} = a_{iu}e^{-\epsilon_{iu}}$, in which case
\begin{multline}
    \mathcal{L}_b = \sum_{iu} \left[a_{iu} \epsilon_{iu} - w_{iu} e^{\epsilon_{iu}}\right] \\
     - \sum_{mu} \frac{\omega_{mu}}{\zeta_{mu}} 
    \prod_k \left(1 - \xi_{t(m)k} + \xi_{t(m)k} 
    \frac{\alpha_{ku}}{\beta_{ku}}\right)
    e^{\sum_j \epsilon_{ju} x_{t(m)ju}}
\end{multline}
and
\begin{multline}
    \nabla_\epsilon \mathcal{L}_b = a_{iu} - w_{iu} e^{\epsilon_{iu}} \\
    - \sum_m x_{t(m)iu} \frac{\omega_{mu}}{\zeta_{mu}} 
    \prod_k \left(1 - \xi_{t(m)k} + \xi_{t(m)k} 
    \frac{\alpha_{ku}}{\beta_{ku}}\right)
    e^{\sum_j \epsilon_{ju} x_{t(m)ju}} 
\end{multline}

 In addition, the additional regressor terms necessitate changes to the update rules given above:
\begin{align}
    \beta_{ku} &\leftarrow \sum_t M_{tu} F_{tku} \xi_{tk} G_{tu} \\
    \zeta_{mu} &\leftarrow F_{t(m)u} G_{t(m)u} + r_u \\
    \eta_{tk} &\leftarrow 
    \begin{pmatrix}
        -\sum_{u, m: \, t(m) = t} \frac{\omega_{mu}}{\zeta_{mu}} F_{t(m)ku} 
        G_{t(m)u}\\
        \sum_u N_{tu} \overline{\log \lambda_{ku}} -
        \sum_{u, m: \, t(m) = t} \frac{\omega_{mu}} {\zeta_{mu}}\frac{\alpha_{ku}}{\beta_{ku}} F_{t(m)ku}  G_{t(m)u}
    \end{pmatrix} 
\end{align}
where 
\begin{align}
    G_{tu} &\equiv \prod_i G_{tiu} \\
    G_{tiu} &\equiv \left(\frac{a_{iu}}{b_{iu}} \right)^{x_{tiu}}
\end{align}

Possible strategies here:
\begin{itemize}
    \item Update $a$ exactly. Use a Newton's Method approach to solve for $b$.
    \item Update $a$ exactly. Use a single Newton step on $b$.
    \item Gradient descent on $a$ and $b$ together.
\end{itemize}

\subsection{Log-Normal priors}
Here, we will write the change in log firing as linear in $x(t)$:
\begin{equation}
    \log \lambda_{tu} \propto \sum_{iu} b_{iu} x_{tiu}
\end{equation}
We will also assume a normal distribution for both prior and posterior on $b$:
\begin{align}
    p(b_{iu}) &= \mathcal{N}(m_{iu}, \varsigma^2_{iu}) \\
    q(b_{iu}) &= \mathcal{N}(\mu_{iu}, \sigma^2_{iu})
\end{align}
Using standard standard formulas for the entropy of normal distributions, along with the fact that $\mathbb{E}[e^{tx}] = \exp(t\mu + t^2\sigma^2/2)$ for $x \sim \mathcal{N}(\mu, \sigma^2)$, it is then straightforward to write down the portion of the ELBO dependent on $\mu$ and $\sigma^2$:
\begin{multline}
    \mathcal{L} = \sum_{iu} \left[ -\frac{1}{\varsigma^2_{iu}} \left( \sigma^2_{iu} + (\mu_{iu} - m_{iu})^2 \right) + \frac{1}{2} \log \frac{\sigma^2_{iu}}{\varsigma^2_{iu}} \right] + \sum_{tiu} N_{tu} x_{tiu} \mu_{iu} \\
    - \sum_{mu} \overline{\theta_{mu}} F_{t(m)u} G_{t(m)u}
\end{multline}
where we have defined
\begin{equation}
    G_{tu} = \prod_i e^{\mu_{iu} x_{tiu} + \frac{1}{2} \sigma^2_{iu} x^2_{tiu}} = \exp\left(\sum_i\left( \mu_{iu} x_{tiu} + \frac{1}{2} \sigma^2_{iu} x^2_{tiu}\right)\right)
\end{equation}
In this case, the equations that must be solved to update $\mu$ and $\sigma^2$ take the form
\begin{align}
    \mu_{iu} &= m_{iu} + \varsigma^2_{iu} \sum_t N_{tu} x_{tiu} - \varsigma^2_{iu} \sum_{m} \overline{\theta_{mu}} F_{t(m)u} G_{t(m)u} x_{t(m)iu} \\
    \frac{1}{\sigma^2_{iu}} &= \frac{1}{\varsigma^2_{iu}} + \sum_{m} \overline{\theta_{mu}} F_{t(m)u} G_{t(m)u} x^2_{t(m)iu}
\end{align}
Since $G_{tu}$ depends on both $\mu_{iu}$ and $\sigma^2_{iu}$, these are both transcendental equations. In the case of $x(t) = c$ a constant in time, they can be solved via variable transform using the Lambert $W$ function, but for time-varying $x$, root-finding algorithms will be needed.

\subsubsection{Approximating the sum of exponentials}
We would like a cheap way of evaluating the objective $\mathcal{L}$ above, but the last term involves all variables and is a sum over time (the other sum over time decomposes easily). In fact, there is a nice trick to approximate this term. 

We can begin by writing the non-$G$ terms, which are constants in $\mu$ and $\sigma^2$ in exponential form:
\begin{equation}
    \theta_{mu} F_{t(m)u} = e^{A_{mu}}
\end{equation}
so that we can write the final term as
\begin{equation}
    \sum_{mu} \exp\left(A_{mu} +  \sum_i \left[ \mu_{iu} x_{t(m)iu} + 
    \frac{1}{2} \sigma^2_{iu} x^2_{t(m)iu}\right]\right)
\end{equation}
But recall that this is equal to the expectation
\begin{equation}
    \sum_{mu}\mathbb{E}\left[ \theta_{mu} \prod_k \lambda_{ku}^{z_{t(m)k}} 
    \prod_i e^{b_{iu} x_{t(m)iu}}\right]
\end{equation}
and often in practice, the expectations we are summing are quite close to 1 (exponents close to 0). This means, of course, that we can approximate 
\begin{equation}
    \sum_n e^{x_n} \approx \sum_n (1 + x_n + \ldots) \approx e^{\sum_n x_n}
\end{equation}
In fact, even in cases where a scaling constant $\Lambda$ exists such that $\Lambda e^{x_n} \approx 1$ for all $n$ (i.e., $x - \log \Lambda \approx 0$), we have
\begin{equation}
    \sum_n e^{x_n} \approx \Lambda^{-1} \sum_n e^{x_i + \log \Lambda} \approx
    \Lambda^{N - 1} e^{\sum_n x_n}
\end{equation}
Thus, we might well consider approximating the last term in $\mathcal{L}$ as
\begin{multline}
    \sum_{mu} \exp\left(A_{mu} +  \sum_i \left[ \mu_{iu} x_{t(m)iu} + 
    \frac{1}{2} \sigma^2_{iu} x^2_{t(m)iu}\right]\right) \approx \\
    \exp\left(\sum_{mu} A_{mu} +  \sum_{imu} \left[ \mu_{iu} x_{t(m)iu} + 
    \frac{1}{2} \sigma^2_{iu} x^2_{t(m)iu}\right]\right)
\end{multline}
which leads to dramatically simpler expressions for the gradient and Hessian.

\subsubsection{Natural Gradients}
In case it's useful later, here are the natural gradients. Since the natural paramter of the normal distribution is $\eta = (\mu/\sigma^2, -1/2\sigma^2)$, we can write for the natural gradient
\begin{equation}
    \nabla_\eta \mathcal{L} = \left( \sigma^2 \frac{\partial \mathcal{L}}{\partial \mu}, 2\sigma^4 \frac{\partial \mathcal{L}}{\partial \sigma^2}\right)
\end{equation}
and use
\begin{align}
    \frac{\partial \mathcal{L}}{\partial \mu_{iu}} &= 
    -\frac{\mu_{iu}}{\varsigma^2_{iu}} + \frac{m_{iu}}{\varsigma^2_{iu}} + \sum_t N_{tu} x_{tiu} - \sum_{m} \overline{\theta_{mu}} F_{t(m)u} G_{t(m)u} x_{t(m)iu} \\
    \frac{\partial \mathcal{L}}{\partial \sigma^2_{iu}} &= 
    \frac{1}{2\sigma^2_{iu}} - \frac{1}{2\varsigma^2_{iu}} - \frac{1}{2} \sum_{m} \overline{\theta_{mu}} F_{t(m)u} G_{t(m)u} x^2_{t(m)iu}
\end{align}

\section{Hierarchy over units}
Up to this point, we have not made the very natural assumption that parameters for a given neural unit are drawn from a common distribution. Equivalently, we might want to make inferences not only about coefficients for single units, but about distributions for the population as a whole. That is, we might want to assume
\begin{align}
    \lambda_{ku} &\sim \mathrm{Ga}(c_k, d_k) \\
\end{align}
With appropriate priors on $c_k$ and $d_k$

\subsection{Derivation of closed-form hyperpriors}
What form should the distribution of the hyperpriors on $c$ and $d$ take? (In this section, we will suppress the index $k$.) To begin, we can write the likelihood for the $\lambda_u$ terms:
\begin{equation}
    \log p(\lambda) = \sum_u \log p(\lambda_u|c, d) = \sum_u \left[ 
    (c - 1) \log \lambda_u - d\lambda_u + c \log d - \log \Gamma(c) 
    \right]
\end{equation}
The $\log \Gamma(c)$ term presents a challenge, but we can use bounds on the gamma function of the form
\begin{equation}
    \sqrt{2\pi} \le \frac{z!}{z^{z+\frac{1}{2}} e^{-z}} \le e
\end{equation}
along with the identities $z! = \Gamma(z + 1)$ and $\log \Gamma(z + 1) = \log \Gamma (z) + \log z$ to write
\begin{equation}
    \log \sqrt{2\pi} + \left(z - \frac{1}{2}\right)\log z - z \le
    \log \Gamma (z) \le 1 + \left(z - \frac{1}{2}\right) \log z - z
\end{equation}
which we can use to get a lower bound on $-\log \Gamma (z)$. That is
\begin{multline}
    \log p(\lambda) \ge \sum_u \left[ 
    (c - 1) \log \lambda_u - d\lambda_u + c \log d - 
    \left(1 + \left(c - \frac{1}{2}\right) \log c - c\right)
    \right] \\
    = \sum_u \left[
    (c - 1) (\log \lambda_u + 1) - d\lambda_u + c \log \frac{d}{c} + 
    \frac{1}{2} \log c
    \right] 
\end{multline}
Here, it is the $c\log c$ term that presents a challenge for finding a conjugate prior, but the choice $d = \theta c$, corresponding to a $\mathrm{Ga}(c, c\theta)$ prior on firing rate effects --- equivalent to a mean effect of $\frac{1}{\theta}$ --- allows us to cancel terms and write
\begin{equation}
    \log p(\lambda) \ge \sum_u \left[ 
    (c - 1) (\log \lambda_u + 1) - c\theta\lambda_u + c \log \theta + \frac{1}{2}\log c\right]
\end{equation}
Clearly, the conditional distributions for $c$ and $\theta$ have the form 
\begin{align}
    c &\sim \mathrm{Ga}\left(\frac{U}{2} + 1, \sum_u \left[ 
    \theta\lambda_{u} - \log \theta - \log \lambda_{u} - 1
    \right]\right) \\
    \theta &\sim \mathrm{Ga}\left(
    cU + 1, c \sum_u \lambda_u
    \right)
\end{align}
As a result, we can easily implement a hierarchical model with closed-form updates by choosing Gamma priors on $c_k$ and $\theta_k$. More intuitively, we might want to think of putting Inverse-Gamma priors on $\theta^{-1} = \mathbb{E}[\lambda]$ and $c^{-1} = \mathbb{E}[\lambda]^2 \mathrm{var}[\lambda]$. A sparse prior on $\lambda$ then consists of a pair of priors that concentrates $\theta^{-1}$ around 1 and $c^{-1}$ around 0. In this case, given the model
\begin{align}
    \theta &\sim \text{Ga}(a, b) \\
    c &\sim \text{Ga}(g, h)     
\end{align} 
The posterior distributions are
\begin{align}
    \theta &\sim \text{Ga}\left(a + Uc, b + c\sum_u \lambda_u \right) \\
    c &\sim \text{Ga}\left(g + \frac{U}{2}, h - U(\log \theta + 1) 
    + \sum_u \left[ \theta \lambda_u - \log \lambda_u \right]\right) 
\end{align}

\section{Semi-Markov Models}
This section follows the technical report by Murphy (2002), as well as papers by Mithcell and Jamieson (1993) and Mitchell, Harper, and Jamieson (1995).

We again begin with the parameters defining the Hidden Markov Model (HMM): $A_{ji} = p({i \rightarrow j})$ is the transition matrix between states, with the columns of $A$ summing to 1 (again, the opposite of the usual convention, such that time evolution is equivalent to $A$ acting to the right on a state vector). Likewise, the initial probability of state $i$ is $\pi_i = p(z_0 = i)$. Finally, we will write $\psi_{tj} = p(y_t|z_t = j)$ for the local evidence for the observation $y_t$. (Note that, for the forward-backward algorithm, $\psi$ does not have to be normalized along its second index. In fact, we have the freedom to rescale $\psi$ by an overall constant \emph{at each $t$}.)

Now, for the semi-Markov case, we assume that states $\tau$ persist for (integer) durations $d_\tau$ governed by a probability mass function $p(d_\tau)$ such that $\sum_{\tau} d_{\tau} = T$. We can then write the marginal probability of the observation sequence over all time $T$ (divided into $n_\tau$ segments) as
\begin{align}
    p(y_{0:T}) &= \sum_{n_\tau} p(n_\tau)\sum_z \prod_{\tau = 0}^{n_\tau} p(z_{\tau+1}|z_\tau) p(d_{\tau}|z_{\tau})
    \prod_{i = t_{i(\tau)}}^{t_{f(\tau)}} p(y_i|z_\tau) \\
    &= \sum_{n_\tau} p(n_\tau) \sum_z \pi(z_0) \prod_{\tau = 1}^{n_\tau} A_{z_{\tau+1}, z_{\tau}} p(d_{\tau}|z_{\tau})\prod_{i = t_{i(\tau)}}^{t_{f(\tau)}} \psi_{iz_{\tau}}
\end{align}
with $t_{i(\tau)}$ and $t_{f(\tau)}$ the start and end points of the given state and we have made the strong conditional independence assumptions that state durations depend only on the current state, state transitions are independent of duration, and observations within each state are iid.

\subsection{Forward-Backward Algorithm for HSMM}
If the durations $d_\tau$ were known, the normal FB algorithm could be implemented as usual, paying attention only to the state transitions and aggregating over observations within each state. However, for the case of unknown state durations, we write:

\subsubsection{Backward}
As usual define
\begin{align}
    \beta_t(i) & = p(\text{future evidence}|\text{just finished \emph{i}}) \\
    &= \sum_j A(j, i) \sum_d p(d|j) \beta_{t + d}(j) \psi_{(t+1):(t+d)}(j)
\end{align} 
In addition, we will also make use of
\begin{align}
    \beta^*_t(i) & = p(\text{future evidence}|\text{start \emph{i} at \;} t + 1) \\
    &= \sum_d p(d|i) \beta_{t + d}(i) \psi_{(t+1):(t+d)}(i) \\
    \Rightarrow \quad
    \beta_t (i) &= \sum_j A(j, i) \beta_t^*(j)
\end{align} 
Our initial condition will be $\beta_T(i) = 1$.

\subsubsection{Forward}
Here, we define
\begin{align}
    \alpha_t (i) & = p(\text{just finished \emph{i}}, \text{past evidence}) \\
    &= \sum_d \psi_{(t-d+1):t}(i) p(d|i) \sum_j A(i, j) \alpha_{t - d}(j) \\
    & = \sum_d \psi_{(t-d+1):t}(i) p(d|i) \alpha^*_{t - d}(i) \\
    \alpha^*_t(i) &= \sum_j A(i, j) \alpha_t (j)
\end{align} 
where as above, the latter is the probability of starting in $i$ at time $t + 1$. Here, the initial condition is $\alpha^*_0(i) = \pi(i)$

\subsubsection{Posterior Marginals and Normalization}
We would like to calculate $\xi_t(i) \equiv p(z_t = i|y_{1:T})$ as in the standard FB algorithm, this is given (up to normalization) by
\begin{equation}
    \xi_t(i) = \frac{\alpha_t(i) \beta_t(i)}{Z_t}
\end{equation}
However, as Murphy points out, finding a sensible normalization for $\alpha$ and $\beta$ that prevents underflow in the forward and backward steps is a challenge. As such, we will elect to work in log space. Defining $\tilde{\alpha} \equiv \log \alpha$, etc., and
\begin{equation}
    \mathrm{lse}_i(x_i) \equiv \log \left( 
    \sum_i e^{x_i}
    \right)
\end{equation}
we can take logs and rewrite the forward equation as
\begin{align}
    \tilde{\alpha}_{t, i} &= \mathrm{lse}_d(\tilde{B}_{tid} + \tilde{D}_{id} + \tilde{\alpha}^*_{t - d, i}) \\
    \tilde{\alpha}^*_{t, i} &= \mathrm{lse}_j(\tilde{A}_{i, j} + \tilde{\alpha}_{t, j})
\end{align}
with 
\begin{align}
    B_{tid} &\equiv \psi_{(t-d+1):t}(i) \\
    D_{id} &\equiv p(d|i)
\end{align}
Similarly, the backward steps can be rewritten
\begin{align}
    \tilde{\beta}_{t, i} &= \mathrm{lse}_j (\tilde{\beta^*_{t, j}} + \tilde{A}_{j, i}) \\
    \tilde{\beta}^*_{t, i} &= \mathrm{lse}_d (\tilde{\beta}_{t + d, i} + 
    \tilde{B}_{t + d, i, d} + \tilde{D}_{id})
\end{align}

\subsubsection{Two-slice Marginals}
For updating $A$ and $\pi$, we need to calculate the ML estimates of each (to be combined with priors, if doing Bayesian inference). Again, following Murphy, we have
\begin{equation}
    \hat{A}_{ji} \propto \sum_t \beta^*_t(j) A(j, i) \alpha_t(i) 
\end{equation}
where the constant of proportionality is determined by the requirement that $\sum_j \hat{A}_{ji} = 1$ for all $i$.
Equivalently, we might want to calculate the two-slice marginals:
\begin{equation}
    \Xi_{t+1, t}(j, i) \propto \beta^*_t(j) A(j, i) \alpha_t(i)
\end{equation}
where the normalization restriction in this case is that $\sum_{i, j} \Xi_{t + 1, t} (j, i) = 1$. Likewise the maximum likelihood estimate for the initial state is
\begin{equation}
    \hat{\pi}_i \propto \pi_i \beta^*_0(i)
\end{equation}

\subsubsection{Estimating $p(d)$}

\end{document}