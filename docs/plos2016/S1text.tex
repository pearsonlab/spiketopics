% Template for PLoS
% Version 3.1 February 2015
%
% To compile to pdf, run:
% latex plos.template
% bibtex plos.template
% latex plos.template
% latex plos.template
% dvipdf plos.template
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended
% to minimize problems and delays during our production
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% Once your paper is accepted for publication,
% PLEASE REMOVE ALL TRACKED CHANGES in this file and leave only
% the final text of your manuscript.
%
% There are no restrictions on package use within the LaTeX files except that
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% Please do not create a heading level below \subsection. For 3rd level headings, use \paragraph{}.
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file.
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission.
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig." instead of "Figure".
% See http://www.plosone.org/static/figureGuidelines for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - tabs/spacing/line breaks within cells to alter layout or alignment
% - vertically-merged cells (no tabular environments within tabular environments, do not use \multirow)
% - colors, shading, or graphic objects
% See http://www.plosone.org/static/figureGuidelines#tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://www.plosone.org/static/latexGuidelines
%
% Please be sure to include all portions of an equation in the math environment.
%
% Do not include text that is not math in the math environment. For example, CO2 will be CO\textsubscript{2}.
%
% Please add line breaks to long display equations when possible in order to fit size of the column.
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% fixltx2e package for \textsubscript
\usepackage{fixltx2e}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
%\DisableLigatures[f]{encoding = *, family = * }

% rotating package for sideways tables
\usepackage{rotating}

\usepackage{tikz}
\usepackage{subfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{calc}
\usepackage{siunitx}
%\usepackage{graphics}


% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% Leave date blank
\date{}

% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\fancyhead[R]{\fontsize{12}{14} \selectfont Neuron's Eye View: Inferring Features of Complex Stimuli from Neural Responses}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\sf PLOS}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

% Generelt
\newcommand{\ud}{\mathrm{d}} % d
\newcommand{\R}{\mathbb{R}} % Real numbers
\newcommand{\T}{\mathscr{T}}

% Operatorer og funktionaler
\newcommand{\I}{\mathbb{I}} % I

\newcommand{\CE}[2]{\mathrm{E}\left[\,#1\,|\,#2\,\right]} % Conditional expectation

\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bc}{\boldsymbol{c}}
\newcommand{\bd}{\boldsymbol{d}}
\newcommand{\bP}{\boldsymbol{\Phi}}
\newcommand{\bZ}{Z}%{\boldsymbol{Z}}
\newcommand{\bV}{V}%{\boldsymbol{V}}
\newcommand{\br}{\boldsymbol{r}}
\newcommand{\bt}{\boldsymbol{t}}
\newcommand{\btheta}{\boldsymbol{\vartheta}}
\newcommand{\bht}{\hat{\boldsymbol{\vartheta}}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bx}{{\boldsymbol{x}}}
\newcommand{\bv}{v}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\vv}{\boldsymbol{v}}
\newcommand{\bepsilon}{\boldsymbol{\varepsilon}}

%% END MACROS SECTION

\setcounter{secnumdepth}{3}
\begin{document}
\vspace*{0.35in}

% Title must be 250 characters or less.
% Please capitalize all terms in the title except conjunctions, prepositions, and articles.
\begin{flushleft}
{\Large
\textbf{S1 Text. Mathematical details\\}
\bigskip
\textbf{Derivation of ELBO and Inference} % Please use "title case" (capitalize all terms in the title except conjunctions, prepositions, and articles).
}
\newline
% % Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
% Xin (Cindy) Chen\textsuperscript{1,3},
% Jeffrey M. Beck\textsuperscript{2,3},
% John M. Pearson\textsuperscript{1,3*},
% \\
% \bigskip
% \textbf{1} Duke Institute for Brain Sciences, Duke University, Durham, North Carolina, USA
% \\
% \textbf{2} Department of Neurobiology, Duke University Medical Center, Durham, North Carolina, USA
% \\
% \textbf{3} Center for Cognitive Neuroscience, Duke University, Durham, North Carolina, USA
% \\

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
%
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
%\Yinyang These authors contributed equally to this work.

% Deceased author note
%\dag Deceased

% Group/Consortium Author Note
%\textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
% * john.pearson@duke.edu

\end{flushleft}
% Please keep the abstract below 300 words

\section{Evidence Lower Bound (ELBO)}
Here we derive the evidence lower bound (ELBO) used as a variational objective by our inference algorithm. That is, we want to calculate
\begin{equation}
    \mathcal{L} \equiv \mathbb{E}_q \left[\log \frac{p(\Theta|N)}{q(\Theta)} \right] = \mathbb{E}_q \left[\log p(\Theta|N) \right] + \mathcal{H}[q(\Theta)]
\end{equation}
From \cite{beal2003variational}, this can be written
\begin{multline}
    \mathcal{L} = \mathbb{E}_{q(\pi)} \left[\log \frac{p(\pi)}{q(\pi)} \right]
    + \mathbb{E}_{q(A)} \left[\log \frac{p(A)}{q(A)} \right]
    + \mathbb{E}_{q}\left[ \log \frac{p(N, z|\lambda, A, \pi)}{q(z)}\right] \\
    + \mathbb{E}_{q(\theta)} \left[\log \frac{p(\theta)}{q(\theta)} \right]
    + \mathbb{E}_{q(\lambda)} \left[\log \frac{p(\lambda)}{q(\lambda)} \right]
    + \mathbb{E}_{q(\gamma)} \left[\log \frac{p(\gamma)}{q(\gamma)} \right]
\end{multline}
For the first two terms, updates are standard and covered in \cite{beal2003variational}. The rest we do piece-by-piece below:

\subsection{Log evidence}
\label{sec:log_evidence}
We would like to calculate $\mathbb{E}_{q}\left[ \log \frac{p(N, z|x, \Theta)}{q(z)}\right]$. To do this, we make use of expectations calculated via the posteriors returned from the forward-backward algorithm
\begin{align}
    \xi_{t} &\equiv p(z_{t}|N, \theta) &
    \Xi_{t, ij} &\equiv p(z_{t+1} = j, z_{t} = i|N, \theta) &
    \log Z_{t} &= \log p(N_{t+1}|N_{t}, \Theta)
\end{align}
Here, we have suppressed the latent feature index $k$ and abuse notation by writing the observation index as $t$, but in the case of multiple observations at a given time, we pool across units and presentations: $N_t \equiv \sum_{m; t(m) = t} N_m$. From this, we can write
\begin{multline}
    \label{eq:log_evidence}
     \mathbb{E}_{q}\left[ \log p(N, z|x, \Theta) \right] =
     \sum_{mkr} \left[N_m \left(
        \overline{\log \theta_m} + \overline{\log \lambda_{0u(m)}} +
        \xi_{t(m)k}\overline{\log \lambda_{zuk}} +
        x_{t(m)r} \overline{\log \lambda_{xu(m)r}}
     \right) \right] \\
     - \sum_m \overline{\theta_m} \mathbb{E}_q\left[\Lambda_{t(m)u(m)} \right]
      + \sum_{tk} \left[ \text{tr}\left(\Xi_{tk} \overline{\log A_k^T} \right)
     + \xi_{0k}^T \overline{\log \pi_k}
     \right]
    + \text{constant}
 \end{multline}
where for notational simplicity, we have replaced expectations with overlines: $\overline{x} \equiv \mathbb{E}[x]$.
In what follows, we will drop the irrelevant constant. For $\overline{\log y}$, where $y \in \lbrace \theta, \lambda_0, \lambda_z, \lambda_x \rbrace$, the assumption $q(y) = \text{Ga}(\alpha, \beta)$ gives
\begin{equation}
    \overline{\log y} = \psi(\alpha) - \log \beta
\end{equation}
with $\psi(x)$ the digamma function. Likewise, the expectation $\overline{\theta}$ is straightforward. For the expectation of the rate, we have
\begin{equation}
    \label{eff_rate}
    \mathbb{E}_q\left[
        \lambda_{0u} \prod_k (\lambda_{zuk})^{z_{tk}} \prod_r (\lambda_{xur})^{x_{tr}}
    \right] = \frac{\alpha_{0u}}{\beta_{0u}}
    \prod_k \left(1 - \xi_{tk} + \xi_{tk} \frac{\alpha_{zuk}}{\beta_{zuk}} \right)
    \prod_r \frac{1}{\beta_{xur}^{x_{tr}}} \frac{\Gamma(\alpha_{xur} + x_{tr})}{\Gamma(\alpha_{xur})}
\end{equation}
However, for $\alpha \gg x$, we have $\Gamma(\alpha + x)/\Gamma(\alpha) \approx \alpha^x$, so that we can write
\begin{equation}
    \label{HFG}
    \mathbb{E}_q[\Lambda_{tu}] = H_{0u} F_{tu} G_{tu}
\end{equation}
with $G_{tu} \approx \prod_r (\alpha_{xur}/\beta_{xur})^{x_{tr}}$. In addition, it will later be useful to have the expectation over \emph{all except} a particular feature $k$ or $r$, for which we define
\begin{align}
    \label{F}
    F_{tuk} &\equiv \prod_{k'\neq k} \left(1 - \xi_{tk'} + \xi_{tk'} \frac{\alpha_{zuk'}}{\beta_{zuk'}} \right) \\
    \label{G}
    G_{tur} &\equiv \prod_{r' \neq r} \left(\frac{\alpha_{xur'}}{\beta_{xur'}} \right)^{x_{tr'}}
\end{align}
Finally, we want the entropy of the variational posterior over $z$, $\mathbb{E}_q[-\log q(z)]$. We can write this in the form
\begin{equation}
    -\sum_{tk} \left[
        \xi_{tk}^T\eta_{tk} + \text{tr}\left(\Xi_{tk} \tilde{A}_k^T \right)
        + \xi_{0k}^T\tilde{\pi}_k
        - \log Z_{tk}
    \right]
\end{equation}
with $(\eta, \tilde{A}, \tilde{\pi})$ the parameters of the variational posterior corresponding to the emission, transition, and initial state probabilities of the Markov chain (interpreted as matrices) and $Z$ the normalization constant. From \cite{beal2003variational}, we have that variational updates should give
\begin{align}
    \label{A_update}
    \tilde{A}_k &\leftarrow \overline{\log A_k} \\
    \label{pi_update}
    \tilde{\pi}_k &\leftarrow \overline{\log \pi_k}
\end{align}
while the effective emission probabilities in the ``on'' ($z = 1$) state of the HMM are
\begin{align}
    \label{eta_update}
    \eta_{tk} &\leftarrow \delta_{z_{tk}, 1} \sum_{m; t(m) = t} N_m \overline{\log \lambda_{zu(m)k}}
    - \sum_{m; t(m) = t} \overline{\theta_m} H_{0u(m)} F_{tku(m)} G_{tu(m)}
\end{align}
Given these update rules, we can then alternate between calculating $(\eta, \tilde{A}, \tilde{\pi})$, performing forward-backward to get $(\xi, \Xi, \log Z)$ and recalculating $(\eta, \tilde{A}, \tilde{\pi})$.

\subsection{Overdispersion, firing rate effects}
Both the case of $p(\theta)$ and $p(\lambda)$ are straightforward. If we ignore subscripts and write $p(y) = \text{Ga}(a, b)$, $q(y) = \text{Ga}(\alpha, \beta)$, then
\begin{equation}
    \mathbb{E}_q \left[\log \frac{p(y)}{q(y)} \right] =
    (\overline{a} - 1) \overline{\log y} + \overline{b} \overline{y} + \mathcal{H}[q(y)]
\end{equation}
where again, $\overline{\log y}$, $\overline{y}$ and $\mathcal{H}[q(y)]$ are straightforward properties of the Gamma distribution. Expectations of the prior parameters are listed in Table \ref{expectation_table}. Note in the last line that there is no expectation, since we have not assumed a hierarchy over firing rate effects for the covariates, $x$.

\begin{table}[ht]
\caption{Expectations of prior parameters for overdispersion and firing rates}
\label{expectation_table}
\begin{center}
\begin{tabular}{lcc}
\multicolumn{1}{c}{\bf Variable}  &\multicolumn{1}{c}{\bf $\mathbb{E}_q[a]$} &\multicolumn{1}{c}{\bf $\mathbb{E}_q[b]$}
\\ \hline
$\theta$ &$\overline{s}$ &$\overline{s}$ \\
$\lambda_0$ &$\overline{c_0}$ &$\overline{c_0 d_0}$ \\
$\lambda_z$ &$\overline{c_z}$ &$\overline{c_z d_z}$ \\
$\lambda_x$ &$a_x$ &$b_x$ \\
\end{tabular}
\end{center}
\end{table}

\subsection{Hyperparameters}
As shown in the main text, the hyperparameters $c$ and $d$, given gamma priors, have conjugate gamma posteriors, so that their contribution to the evidence lower bound, $\mathbb{E}_q \left[\log \frac{p(\gamma)}{q(\gamma)} \right]$ is a sum of terms of the form
\begin{align}
    (a_{c} - 1) \overline{\log c} + b_{c} \overline{c} + \mathcal{H}[q(c)] +
    (a_{d} - 1) \overline{\log d} + b_{d} \overline{d} + \mathcal{H}[q(d)]
\end{align}
In other words, these are straightforward gamma expectations, functions of the prior parameters $a$ and $b$ for each variable and the corresponding posterior parameters $\alpha$ and $\beta$. Similarly, the overdispersion terms are exactly the same with the substitutions $c \rightarrow s$, $d\rightarrow 1$.

As we will see below, the expectations under the variational posterior of $s$, $c$, and $d$ are themselves straightforward to calculate.

\subsection{Autocorrelated noise}
In the main text, we assumed the $\theta_m$ to be uncorrelated. However, it is possible to model temporal autocorrelation among the $\theta_m$ when observations correspond to the same neuron at successive time points. More specifically, let us replace the observation index $m$ by $\tau$, the experimental clock time. We then write a particular observation as corresponding to a stimulus time $t(\tau)$ and a set of units $u(\tau)$, which, for simplicity, we will assume fixed and simply write as $u$.\footnote{The generalization to partially overlapping neurons and stimuli is straightforward but complex and notationally cumbersome.} To model the autocorrelation of noise across successive times, we then write
\begin{align}
    N_{m} &\sim \text{Pois}(\Lambda_{t(\tau), u} \theta_{\tau u}) &
    \text{Assuming } \theta_{\tau u} = \phi_{\tau u} \theta_{\tau - 1, u}
\end{align}

If $\phi_{0u} = \theta_{0u}$, we then have $\theta_{\tau u} = \prod_{\tau' \le \tau} \phi_{\tau' u}$. In essence, this is a log-autoregressive process in which the innovations are not necessarily normally distributed. In fact, if we further assume that $p(\phi_{\tau u}) = \mathrm{Ga}(s_u, r_u)$ and $q(\phi_{\tau u}) = \mathrm{Ga}(\omega_{\tau u}, \zeta_{\tau u})$, we can once again make use of conjugate updates.

Given these assumptions, we need to make the following modifications for the third and fifth terms in the evidence lower bound of Eq (\ref{eq:log_evidence}):
\begin{align}
    \sum_{mkr} N_m \xi_{t(m)k}\overline{\log \lambda_{zuk}} &\rightarrow
        \sum_{mkr} N_m \xi_{t(m)k}\overline{\log \lambda_{zuk}} +
        \sum_{\tau u} \overline{\log\phi_{\tau u}} \sum_{\tau' \ge \tau} N_{\tau' u} \\
    - \sum_m \overline{\theta_m} \mathbb{E}_q\left[\Lambda_{t(m)u(m)} \right] &\rightarrow
        - \sum_{\tau u} H_{0u} F_{\tau ku} G_{\tau u} \prod_{\tau' \le \tau} \frac{\omega_{\tau' u}}{\zeta_{\tau' u}}
\end{align}

Thus the effective emission probabilities in State 1 now become:
\begin{align}
    \eta_{tk} &\rightarrow \delta_{z_{tk}, 1} \sum_{m} N_m \overline{\log \lambda_{zuk}}
    - \sum_{u; t(\tau) = t} H_{0u} F_{\tau ku} G_{\tau u} \prod_{\tau' \le \tau} \frac{\omega_{\tau' u}}{\zeta_{\tau' u}}
\end{align}


\subsection{Latent states, semi-Markov dynamics}

We model each of the latent states $z_{tk}$ as an independent Markov process for each feature $k$. That is, each $k$ indexes an independent Markov chain with initial state probability $\pi_k\sim \text{Dir}(\alpha_\pi)$ and transition matrix $A_k\sim \text{Dir}(\alpha_A)$. For the semi-Markov case, we assume that the dwell times in each state are distributed independently for each chain according to an integer-valued, truncated lognormal distribution with support on the integers $1\dots D$:
\begin{align}
    \label{semi-markov}
    p_k(d|z = j) &= \text{Log-Normal}(d|m_{jk}, s^2_{jk}) / W_{jk}  \\
    W_{jk} &= \sum_{d = 1}^D \text{Log-Normal}(d|m_{jk}, s^2_{jk})
\end{align}
Note that we have allowed the dwell time distribution to depend on both the feature $k$ and the state of the Markov chain $j$. In addition, we put independent Normal-Gamma priors on the mean $(m_{kj})$ and precision $(\tau_{kj} \equiv s_{kj}^{-2})$ parameters of the distribution: $(m, \tau) \sim \text{NG}(\mu, \lambda, \alpha, \beta)$.

In this case, we additionally need to perform inference on the parameters $(m, \tau)$ of the dwell time distributions for each state. In the case of continuous dwell times, our model in Equation \ref{semi-markov} would have $W = 1$ and be conjugate to the Normal-Gamma prior on $(m, \tau)$, but the restriction to discrete dwell times requires us to again lower bound the variational objective:
\begin{equation}
    \mathbb{E}_q\left[-\log W_{jk} \right] =
    \mathbb{E}_q\left[- \log \left( \sum_{d=1}^D p(d|j)\right) \right]
    \ge -\log \sum_{d = 1}^D \mathbb{E}_q\left[p(d|j)\right]
\end{equation}
This correction for truncation must then be added to $\mathbb{E}_q[p(z|\Theta)]$. For inference in the semi-Markov case, we use an extension of the forward-backward algorithm\cite{Yu2006-bb}, as well as the algorithms in \cite{Mitchell1993-sl, Mitchell1995-go}, at the expense of computational complexity $\mathcal{O}(SDT)$ $(S = 2)$ per latent state, to calculate $q(z_k)$. For the $4SK$ hyperparameters of the Normal-Gamma distribution, we perform an explicit BFGS optimization on the $4S$ parameters of each chain on each iteration (detailed in Subsection \ref{semimarkovdd}).

The new ELBO for the semi-Markov model thus adds to the terms involving $\Xi$ and $\xi_0$ in (\ref{eq:log_evidence}) an additional piece:
\begin{multline}
    \sum_{d j k} C(d, j, k) \mathbb{E}_q \left[\log p_k(d|j) - \log \left(\sum_{d=1}^D p_k(d|j) \right) \right] \ge \\
    \sum_{d j k} C(d, j, k) \left[\mathbb{E}_q \left[\log p_k(d|j)\right] - \log \sum_{d=1}^D \mathbb{E}_q\left[p_k(d|j) \right] \right]
\end{multline}
where as noted above, the second term inside the expectation arises from the need to normalize $p(d|j)$ over discrete times and the inequality follows from Jensen's Inequality. Here, $d$ is the duration variable, $j$ labels states of the chain (here 0 and 1), and $k$, as elsewhere, labels chains\cite{Mitchell1993-sl,Mitchell1995-go,Yu2006-bb}. $C$ is defined for each state and chain as the probability of a dwell time $d$ in state $j$ \emph{conditioned on} the event of just having transitioned to $j$\footnote{Thus $C$ is equivalent to $\mathcal{D}_{t|T}$ in \cite{Yu2006-bb}.}
\footnote{
We also note that, just as the emission, transition, and initial state probabilities have their counterparts in variational parameters $(\eta, \tilde{A}, \tilde{\pi})$ in $q(z)$, so $C$ is matched by a term $-\sum_{djk} C(d, j, k) \nu_{djk}$ in $\mathbb{E}_q[-\log q]$. And in analogy with the HMM case, variation with respect to $\nu$ gives
\begin{equation}
    \nu_{djk} = \mathbb{E}_q \left[\log p_k(d|j)\right] - \log \sum_{d=1}^D \mathbb{E}_q\left[p_k(d|j) \right],
\end{equation}
implying that there are cancellations between $\log p(N, z|\Theta)$ and $\log q(z)$ in $\mathcal{L}$ and care must be taken when calculating it\cite{beal2003variational}.}

Thus the objective to be optimized is
\begin{equation}
    \label{semi_markov_opt}
    \sum_{d j k} C(d, j, k) \left[\mathbb{E}_q \left[\log p_k(d|j)\right] - \log \sum_{d=1}^D \mathbb{E}_q\left[p_k(d|j) \right] \right] + \mathbb{E}_q\left[\log \frac{p(m, \tau)}{q(m, \tau)} \right]
\end{equation}
We focus on each of these and their updates below.

\subsubsection{Contributions to the ELBO}
For the case of $p(d|m, s^2)$ Log-Normal and $(m, \tau = s^{-2})$ Normal-Gamma, the terms $\mathbb{E}_q[\log p(d|j)]$ involve only routine expectations of natural parameters of the Normal-Gamma, and similarly for the expected log prior and entropy in the last term. Writing the delay distribution in the exponential family form with base measure $h$ and natural parameter $\eta$
\begin{equation}
    p(d|z=i) = h(d) \exp(\eta_i \cdot T(d) - A(\eta_i))
\end{equation}
yields for the lognormal distribution $\log d \sim \mathcal{N}(m, s^2)$
\begin{align}
    h(d) &= \frac{1}{\sqrt{2\pi} d} \\
    T(d) &= \begin{bmatrix}
    \log d \\
    (\log d) ^2
    \end{bmatrix} \\
    \eta &= \begin{bmatrix}
    \frac{m}{s^2} \\
    -\frac{1}{2s^2}
    \end{bmatrix} \\
    A(\eta) &= \frac{m^2}{2s^2} + \log s
\end{align}

If we then put a Normal-Gamma posterior on $(m, \tau)$ with parameters $(\mu, \lambda, \alpha, \beta)$, we can easily calculate
\begin{align}
    \mathbb{E}_q[\eta] &= \mathbb{E}_q
    \begin{bmatrix}
    m\tau \\
    -\frac{1}{2}\tau
    \end{bmatrix} =
    \begin{bmatrix}
    \mu\frac{\alpha}{\beta} \\
    -\frac{\alpha}{2\beta}
    \end{bmatrix} \\
    \mathbb{E}_q[A(\eta)] &= \mathbb{E}_q\left[
    \frac{1}{2}m^2\tau - \frac{1}{2} \log \tau
    \right] \\
    &= \frac{1}{2} \mathbb{E}_\tau\left[
    \left(\mu^2 + \frac{1}{\lambda \tau}\right)\tau
    \right]
    -\frac{1}{2} (\psi(\alpha) - \log \beta) \\
    &= \frac{1}{2} \left(\mu^2 \frac{\alpha}{\beta} + \frac{1}{\lambda} \right)
    -\frac{1}{2} (\psi(\alpha) - \log \beta)
\end{align}
The relevant piece of the evidence lower bound for updating $(\mu, \lambda, \alpha, \beta)$ is
\begin{equation}
    \sum_{i, d} C(d, i) [ \log h(d) + \mathbb{E}_q[\eta_i] \cdot T(d) -
    \mathbb{E}_q[A(\eta_i)] + \mathbb{E}_q\left[\log \frac{p(m, \tau)}{q(m, \tau)} \right]
\end{equation}
again, with $q(m, \tau) = \text{Normal-Gamma}(\mu, \lambda, \alpha, \beta)$. We can thus write
\begin{align}
    \label{semi_markov_elogp}
    \mathbb{E}_q[\log p(d|j)] =& -\frac{1}{2}\log 2\pi C_0 - C_1 +
    \mathbb{E}_q[m\tau] C_1 -\frac{1}{2}\mathbb{E}_q[\tau] C_2  \nonumber \\
    &- \frac{1}{2} C_0 \mathbb{E}_q[m^2\tau] + \frac{1}{2} C_0 \mathbb{E}_q[\log \tau] \nonumber \\
    =& \; G + F \cdot T(\alpha, \beta, \lambda, \mu)
\end{align}
where
\begin{align}
    G &\equiv -\frac{1}{2}\log 2\pi C_0 - C_1 \\
    F \cdot T &=
    \begin{bmatrix}
        \frac{C_0}{2} &
        -\frac{C_2}{2} &
        C_1 &
        -\frac{C_0}{2}
    \end{bmatrix} \cdot \mathbb{E}_q
    \begin{bmatrix}
        \log \tau \\
        \tau \\
        m\tau \\
        m^2 \tau
    \end{bmatrix} \\
    C_0 &\equiv \sum_{id} C_{id} \\
    C_1 &\equiv \sum_{id} C_{id} \log d \\
    C_2 &\equiv \sum_{id} C_{id} (\log d)^2
\end{align}

For calculating the final term in (\ref{semi_markov_opt}), we can make use of conjugacy (since $p(m, \tau|\alpha_0, \beta_0, \lambda_0, \mu_0)$ is itself Normal-Gamma) and the fact that (\ref{semi_markov_elogp}) only depends linearly on the expected sufficient statistics of $q$ to write the contribution of $\mathbb{E}_q[\log p(m, \tau)]$ as an update to $F$:
\begin{equation}
    F' =
    \begin{bmatrix}
        \frac{C_0}{2} + \alpha_0 \\
        -\frac{C_2}{2} -\beta_0 - \frac{\lambda_0\mu_0^2}{2} \\
        C_1 + \lambda_0\mu_0 \\
        -\frac{C_0}{2} - \frac{\lambda_0}{2}
    \end{bmatrix}
\end{equation}
Moreover, we can make use of the standard entropy formula for the Normal-Gamma distribution:
\begin{align}
    \label{normal_gamma_entropy}
    H[q] &= \mathbb{E}_q[-\log q] = \mathbb{E}_q[-\log q(\tau) - \log q(m|\tau)] \nonumber\\
    &= H_g(\alpha, \beta) + \mathbb{E}_q\left[
    -\log \sqrt{\frac{\lambda\tau}{2\pi}} +
    \frac{\lambda\tau}{2} (m - \mu)^2
    \right] \nonumber\\
    &= \alpha - \log \beta + \log \Gamma(\alpha) + (1 - \alpha) \psi(\alpha) \nonumber\\
    & - \frac{1}{2} \log \frac{\lambda}{2\pi} - \frac{1}{2} \left(\psi(\alpha) - \log \beta\right)
    + \frac{1}{2}
\end{align}

Only slightly more complicated is the term arising from the normalization constant, which in the standard $(\mu, \lambda, \alpha, \beta)$ parameterization of the Normal-Gamma takes the form
\begin{equation}
    \label{e_normal_gamma}
    \mathbb{E}_q[p(d|m, \tau)] = \int d\tau dm \; p(d|i, m, \tau) q(m, \tau)
    = \frac{1}{\sqrt{2\pi}d} \sqrt{\frac{\lambda}{1 + \lambda}}
    \frac{\Gamma(\alpha + 1/2)}{\Gamma(\alpha)}
    \frac{\beta^{-\frac{1}{2}}}{\hat{\beta}^{\alpha + \frac{1}{2}}}
\end{equation}
with
\begin{equation}
    \hat{\beta} \equiv 1 + \frac{1}{2\beta} \frac{\lambda}{1 + \lambda}
    (\log d - \mu)^2
\end{equation}
This can be derived either by performing the integral directly or by noting that the result should be proportional to the posterior (Normal-Gamma, by conjugacy) of $(m, \tau)$ after having observed a single data point, $\log d$.

Taken together, (\ref{semi_markov_elogp}) (with $F\rightarrow F'$), (\ref{normal_gamma_entropy}), and (\ref{e_normal_gamma}) allow (\ref{semi_markov_opt}) to be calculated directly as a function of the variational parameters $(\alpha, \beta, \lambda, \mu)$ of $q(m, \tau)$. Were it not for the normalization constant arising from the restriction of discrete $d$, the updates resulting from optimizing this objective would reduce by conjugacy to simple paramater updates. With that additional term, we are forced to use a brute-force BFGS optimization step instead (see \ref{semimarkovdd} below).


\subsubsection{Forward-Backward Algorithm for HSMM: Notation}
Here, we follow Yu and Kobayashi, mutatis mutandis. Define:
\begin{align*}
    \psi_t(m) &= p(y_t|z_t=m) &\text{observation probability}\\
    \alpha_{t|x}(m, d) &= p(z_t=m, d_t=d|y_{1:x}) &\text{condition on data}\\
    \gamma_{t|x}(m) &= \sum_d \alpha_{t|x} (m, d) &\text{marginalize out }d\\
    \psi^*_t(m) &= \frac{\alpha_{t|t}(m, d)}{\alpha_{t|t-1}(m, d)} =
    \frac{\psi_t(m)}{p(y_t|y_{1:t-1})} \\
    r_t^{-1} &= p(y_t|y_{1:t-1}) = \sum_{m,d} \alpha_{t|t-1}(m, d)\psi_t(m) \\
    &= \sum_m \gamma_{t|t-1}(m) \psi_t(m) \\
    Z &= p(y_{1:T}) = \left(\prod_{t=1}^T r_t\right)^{-1} &\text{probability of data} \\
    \mathcal{D}_{t|x}(m, d) &= p(z_t=m, d_t=d, d_{t-1}=1|y_{1:x}) &\rightarrow (m, d) \\
    \mathcal{T}_{t|x}(n, m) &= p(z_t=n, z_{t-1}=m, d_{t-1}=1|y_{1:x}) &m \rightarrow n \\
    \mathcal{E}_t(m) &= p(z_t=m, d_t=1|y_{1:t}) = \alpha_{t|t-1}(m, 1) \psi^*_t(m) &m \text{ ends at } t \\
    \mathcal{S}_t(m) &= p(d_t=1, z_{t+1}=m|y_{1:t}) = \sum_n A(m, n)\mathcal{E}_t(n) &m \text{ starts at } t+1
\end{align*}
and for the backward pieces
\begin{align*}
    \beta_t(m, d) &= \frac{p(z_t=m, d_t=d|y_{1:T})}{p(z_t=m, d_t=d|y_{1:t-1})}
    = \frac{p(y_{t:T}|z_t=m, d_t=d)}{p(y_{t:T}|y_{1:t-1})} \\
    \mathcal{E}^*_t(m) &= \frac{p(y_{t:T}|z_t=m, d_{t-1} = 1)}{p(y_{t:T}|y_{1:t-1})} = \sum_d p_m(d) \beta_t(m, d) \\
    \mathcal{S}^*_t(m) &= \frac{p(y_{t:T}|z_{t-1}=m, d_{t-1}=1)}{p(y_{t:T}|y_{1:t-1})} = \sum_n \mathcal{E}^*_t(n) A(n, m)
\end{align*}
These quantities then allow us to write the posterior probabilities
\begin{align*}
    \alpha_{t|T}(m, d) &= \beta_t(m, d) \alpha_{t|t-1}(m, d) &\text{posterior prob of }(m, d) \\
    \mathcal{T}_{t|T}(n, m) &= \mathcal{E}^*_t(n) A(n, m) \mathcal{E}_{t - 1}(m) &\text{posterior prob of } m \rightarrow n \\
    \mathcal{D}_{t|T}(m, d) &= \beta_t(m, d)p_m(d) \mathcal{S}_{t-1}(m) &\text{posterior prob of entering }(m, d) \text{ at } t
\end{align*}

\subsubsection{Forward}
From Yu and Kobayashi, we have the following forward pass:
\begin{enumerate}
    \item $\alpha_{1|0}(m, d) = \pi_m p_m(d)$ for time 1; otherwise, use
    \begin{equation}
        \alpha_{t|t-1}(m, d) = S_{t-1}(m) p_m(d) + \psi^*_{t-1}(m) \alpha_{t-1|t-2}(m, d + 1)
    \end{equation}
    \item Use $\alpha_{t|t-1}$ and $\psi_t$ to get $r_t$; together, these give
    $\psi_t^*$. Store $r_t$
    \item Calculate and save $\mathcal{E}_t$.
    \item Calculate $\mathcal{S}_t$.
    \item Loop.
\end{enumerate}

\subsubsection{Backward}
Again following Yu and Kobayashi:
\begin{enumerate}
    \item Initialize $\beta_T(m, d) = \psi^*_T(m) = \psi_T(m) r_T$
    \item $\psi^*_t(m) = \psi_t(m) r_t$
    \item Calculate $\beta_t(m, d)$ by
    \begin{equation}
        \beta_t(m, d) =
        \begin{cases}
            \mathcal{S}^*_{t+1}(m)\psi^*_t(m), & d=1 \\
            \beta_{t+1}(m, d - 1) \psi^*_t(m), & d > 1
        \end{cases}
    \end{equation}
    where, again,
    \begin{align}
        \mathcal{S}^*_{t + 1}(m) &= \sum_{n} \mathcal{E}^*_{t+1}(n) A(n, m) \\
        \mathcal{E}^*_{t + 1}(n) &= \sum_d \beta_{t+1}(n, d) p_n(d)
    \end{align}
    \item Use the above equation to calculate $\mathcal{E}^*_t$ and $\mathcal{S}^*_t$. Save $\mathcal{E}^*$.
\end{enumerate}

\subsubsection{Sufficient Statistics}
Finally, we can calculate and return:
\begin{align}
    \log Z &= - \sum_{t=1}^T \log r_t \\
    \xi_t(m) &= \sum_d \alpha_{t|T}(m,d) = \gamma_{t|T}(m) = \sum_d \alpha_{t|t-1}(m, d) \beta_t(m, d) \\
    \Xi_{t+1, t}(n, m) &= \mathcal{T}_{t+1|T}(n, m) = \mathcal{E}^*_{t+1}(n) A(n, m) \mathcal{E}_t(m) \\
    C_t(m, d) &= \mathcal{D}_{t|T}(m, d) =
    \begin{cases}
        \pi_m p_m(d) \beta_1(m, d) & t = 1 \\
        \mathcal{S}_{t-1}(m)p_m(d) \beta_t(m, d) & t > 1 \\
    \end{cases}
\end{align}
Note that, because $\Xi_{t+1, t}$ is the \emph{joint} absolute probability of the transition (i.e., not conditioned on $d_t = 1$) it is \emph{not} normalized when summing over $m$ and $n$. The same is true for $C$ when summing over $m$ and $d$.



\section{Inference}
\subsection{Conjugate updates}
For updates on the overdispersion, firing rate, and hyperparameter variables, we have the simple conjugate update rules depicted in Table \ref{conj_updates}. These can be derived either from free-form variational arguments, or exponential family rules, but are in any case straightforward\cite{Blei2006-oh}. If we assume a $\text{Ga}(a, b)$ prior and $\text{Ga}(\alpha, \beta)$ variational posterior, all of these have a simple algebraic update in terms of sufficient statistics.

\begin{table}[ht]
\caption{Conjugate updates for Gamma distributions}
\label{conj_updates}
\begin{center}
\begin{tabular}{lcc}
\multicolumn{1}{c}{\bf Variable}  &\multicolumn{1}{c}{\bf $\alpha - a$} &\multicolumn{1}{c}{\bf $\beta - b$}
\\ \hline
$\theta_m$         &$N_m$  &$H_{0u(m)}F_{t(m)u(m)}G_{t(m)u(m)}$ \\
$s_u$         &$\frac{1}{2}\sum_m \delta_{u(m), u}$  &$\sum_m \delta_{u(m), u} [\overline{\theta_m} - \overline{\log \theta_m} - 1]$ \\
$\lambda_{0u}$         &$\sum_{t} N_{tu}$  &$\overline{\theta}_u H_{0u}\sum_t F_{tu}G_{tu}$ \\
$\lambda_{zuk}$         &$\sum_t N_{tu} \xi_{tk}$  &$\overline{\theta}_u H_{0u}\sum_t F_{tuk}G_{tu}$ \\
$\lambda_{xur}$         &$\sum_t N_{tu} x_{tr}$  &cf. Section \ref{non-conj} \\
$c_{0}$         &$U/2$  &$\sum_u\mathbb{E}_q \left[d_{0} \lambda_{0u} - \log \lambda_{0u} - \log d_{0} - 1\right]$ \\
$c_{zk}$         &$U/2$  &$\sum_u\mathbb{E}_q \left[d_{zk} \lambda_{zuk} - \log \lambda_{zuk} - \log d_{zk} - 1\right]$ \\
$c_{xr}$         &$U/2$  &$\sum_u\mathbb{E}_q \left[d_{xr} \lambda_{xur} - \log \lambda_{xur} - \log d_{xr} - 1\right]$ \\
$d_{0}$         &$U\mathbb{E}_q[c_{0}]$  &$\sum_u\mathbb{E}_q \left[c_{0}\lambda_{0u}\right]$ \\
$d_{zk}$         &$U\mathbb{E}_q[c_{zk}]$  &$\sum_u\mathbb{E}_q \left[c_{zk}\lambda_{zuk}\right]$ \\
$d_{xr}$         &$U\mathbb{E}_q[c_{xr}]$  &$\sum_u\mathbb{E}_q \left[c_{xr}\lambda_{xur}\right]$ \\
$\phi_{j u}$ for $j \in \{1, \ldots, \tau\}$  &$\sum_{\tau' \ge j} N_{\tau' u}$  &$\sum_{u, \tau \ge j} F_{t(\tau)} \prod_{\substack{\tau' \le \tau \\ \tau' \ne j}} \frac{\omega_{\tau' u}}{\zeta_{\tau' u}}$ \\
\end{tabular}
\end{center}
\end{table}

Here, we overload notation to write $N_{tu} = \sum_m \delta_{u(m), u} \delta_{t(m), t}\, N_m$, $\overline{\theta}_u = \sum_m \delta_{u(m), u}\overline{\theta}_m$, and make use of $H$, $G$, and $F$ as defined in Eq (\ref{HFG}) - (\ref{G}). Indeed, our implementation caches $F$ and $G$ for a substantial speedup (at the cost of additional memory requirements). For autocorrelated noise, the update rules are for the $j$-th item in the series of autocorrelation.

\subsection{Non-conjugate updates}
\label{non-conj}
We employ explicit optimization steps for two updates in our iterative algorithm. In each case, we employ an off-the-shelf optimization routine, though more efficient alternatives are likely possible.

\subsubsection{Covariate firing rate effects}
\label{beta_x}
Because we do not restrict the covariates $x(t)$ to be binary, Equation \ref{eff_rate} no longer yields a conditional log probability for $\lambda_x$ in the exponential family. This leaves us with a transcendental equation to solve for $\beta_{xr}$. However, from Table \ref{conj_updates}, we see that $\alpha_{xr} \gg \sum_t x_{tr}$, allowing us to approximate $G_t \approx \prod_r (\alpha_{xr}/\beta_{xr})^{x_{tr}}$ as we have done above. Moreover, since the sum $\sum_t G_t \sim T\overline{G}$, we expect $\alpha_x / \beta_x \approx 1$ at the optimum for most reasonable datasets. We thus reparameterize $\beta_{xur} = \alpha_{xur}e^{-\epsilon_{ur}}$ and write the relevant $\epsilon$-dependent piece of the objective function $\mathcal{L}$ as
\begin{equation}
    \label{fropt}
    \mathcal{L}_\epsilon = \sum_{ur} \left[a_{xur}\epsilon_{ur} - b_{xur}e^{-\epsilon_{ur}} \right] - \sum_m \overline{\theta}_m H_{0u(m)} F_{t(m)u(m)}
    e^{-\sum_r \epsilon_{u(m)r} x_{t(m)u(m)r}}
\end{equation}
We also supply the optimizer with the gradient:
\begin{equation}
    \label{frgrad}
    \nabla_\epsilon \mathcal{L}_\epsilon = a_{xur} - b_{xur}e^{-\epsilon_{ur}}  - \sum_{m; u(m) = u} x_{t(m)ur} \overline{\theta}_m H_{0u} F_{t(m)u}
    e^{-\sum_r \epsilon_{ur} x_{t(m)ur}}
\end{equation}
where we sum only over observations with $u(m) = u$. On each iterate, we then optimize $\mathcal{L}_\epsilon$, initializing $\beta_x$ to the just-updated value of $\alpha_x$. In addition, we do not update firing rate effects for covariates separately, but optimize $\beta_{xur}$ for all $r$ together. Again, more efficient schemes are no doubt possible.

\subsection{Semi-Markov duration distribution}
\label{semimarkovdd}
If the dwell time distributions of states in the semi-Markov model were truly continuous, the parameters $(m, s^2)$ of a lognormal distribution $p(d|j)$ would have a Normal-Gamma conjugate prior, and updates would be closed-form. However, the requirement that the durations be integers and that there exist a maximal duration, $D$, over which these probability mass functions must normalize results in an extra term in $\mathbb{E}_q[\log p]$ arising from the normalization constant. In this case, we must explicitly optimize for the parameters of the Normal-Gamma prior on $(m, s^2)$. However, unlike the case of \ref{beta_x}, these parameters are only updated for one latent feature at a time. Since the Normal-Gamma distribution has only 4 parameters and the number of states of the latent variable is $S = 2$, this requires updating $4SK$ parameters, but only taken $4S = 8$ at a time, a much more manageable task.



\section{Experiments}
Code for all algorithms and analyses is available at \url{https://github.com/pearsonlab/spiketopics}.


\nolinenumbers
\newpage
%\section*{References}
% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% OR
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here.
%

% \begin{thebibliography}{10}
%
% \bibitem{watson2012social}
% K. K. Watson and M. L. Platt.
% \newblock Social signals in primate orbitofrontal cortex.
% \newblock Current Biology. vol. 22, no. 23, pp. 2268--2273, 2012.
%
% \end{thebibliography}

\bibliography{chen_beck_pearson}{}

\end{document}
