\documentclass[11pt]{article}
\usepackage{amssymb,amsmath}

\begin{document}
\title{The Log-Normal Model}
\author{John Pearson}
\maketitle

\section{Observation Model}
Assume an experiment with $M$ observations (one per unit, time combination). Each unit is responsive to $R$ regressors and $K$ latent states. Stimuli are indexed by a time variable with $T$ unique values. Spike counts are governed by a Poisson process given by
\begin{equation}
    N_{m} \sim \mathrm{Poiss}(e^{\eta_{m}})
\end{equation}
where $N_{m}$ is the count from observation $m$ and the effective rate $\eta_{m}$ is given by
\begin{equation}
    \label{loglambda}
    \eta_m = \lambda_{0u(m)} + \sum_k z_{t(m)k} b_{ku(m)} + \sum_r x_{mr} \beta_{ru(m)} + \epsilon_{m}
\end{equation}
Here, the coefficients $b_{ku}$ index responses of each unit to the (binary) latent states $z_{mk}$ while the $\beta_{ru}$ index responses to (non-latent) regressors $x_{mr}$. The final ($\epsilon$) term represents moment-to-moment fluctuations of each unit. We have also assumed functions $u(m)$ and $t(m)$ mapping each observation to its appropriate unit and time, respectively.

\section{Latent feature model}
We will assume a Hidden Markov Model (HMM) for the latent states $z$. We will likewise assume that these latent features are present in the stimulus, so that they are indexed by the stimulus time $t(m)$. When the meaning is clear, we will drop the dependency of stimulus time on observation number and simply write $t$.

To describe the transition model for the HMM, we take
\begin{align}
    A_{ij} &\equiv p(z_{t+1} = i|z_t = j) \\
    \pi_i &\equiv p(z_0 = i)
\end{align}
(Note that this means that the \emph{columns} of $A$ sum to 1, which is the opposite of the usual convention. In other words, in matrix notation, $z_{t+1} = A \cdot z_t$.)

Given this notation, it is straightforward to write the probability of a sequence of hidden states, conditioned on the chain parameters
\begin{equation}
    \log p(z|A, \pi) = \sum_t \log A_{z_{t+1} z_t} + \log \pi_{z_0}
\end{equation}

\section{Priors}
In addition to the model (\ref{loglambda}) above and the HMM ansatz, we posit the following hierarchical generative model for the parameters:
\begin{align}
    \lambda_{0u} &\sim \mathcal{N}(m_0, v^2_0) \\
    b_{ku} &\sim \mathcal{N}\left(0, (v^2_b)_{k}\right) \\
    \beta_{ku} &\sim \mathcal{N}\left(0, (v^2_\beta)_{k}\right) \\
    \epsilon_m &\sim \mathcal{N}\left(0, (v^2_\epsilon)_{u(m)}\right) \\
    (v^2_b)_{k} &\sim \text{Inv-Ga}\left(s_b, r_b \right) \\
    (v^2_\beta)_{r} &\sim \text{Inv-Ga}\left(s_\beta, r_\beta \right) \\
    (v^2_\epsilon)_{u} &\sim \text{Inv-Ga}\left(s_\epsilon, r_\epsilon \right) \\
    \pi_k &\sim \mathrm{Beta}\left((a_\pi)_k, (b_\pi)_k \right) \\
    \left(A_k \right)_{1i} &\sim \mathrm{Beta}\left((a_A)_{ki}, (b_A)_{ki} \right)
    \end{align}
where $A_{1i}$ is the probability of the transition $i \rightarrow 1$ (i.e., the transition \emph{into} the $z = 1$ state.)

Note also that, by putting sparse priors on the $v^2$ terms, we can implement automatic relevance determination (ARD) on the regression coefficients.

\section{Variational ansatz}
We would like to approximate the joint posterior density
\begin{multline}
    p(\lambda_0, b, \beta, A, \pi, z, \epsilon|N) \propto p(N|\lambda_0, z, b, \beta, \epsilon) p(z|A, \pi) \\
    \times p(\lambda_0) p(b) p(\beta) p(A) p(\pi) p(\epsilon) p(v^2_b) p(v^2_\beta) p(v^2_\eta) p(v^2_\epsilon)
\end{multline}
with a structured mean field form that factorizes over units and chains:
\begin{multline}
    q(\lambda_0, b, \beta, A, \pi, z, \epsilon) = \prod_m q(\epsilon_m) \prod_{ku} q(\lambda_{0u})
    q(b_{ku}) q(\beta_{ku}) q(z_k) q(A_k) q(\pi_k) \\
    \times q(v^2_{bk}) q(v^2_{\beta k}) q(v^2_{\eta u})
\end{multline}
For this, we will use the posterior variational ansatz
\begin{align}
    \lambda_{0u} &\sim \mathcal{N}\left(\mu_{0u}, \sigma^2_{0u}\right) \\
    b_{ku} &\sim \mathcal{N}\left((\mu_b)_{ku}, (\sigma^2_b)_{ku}\right) \\
    \beta_{ku} &\sim \mathcal{N}\left((\mu_\beta)_{ku}, (\sigma^2_\beta)_{ku}\right) \\
    \epsilon_m &\sim \mathcal{N}\left(\mu_{\epsilon m}, \sigma^2_{\epsilon m}\right) \\
    (v^2_b)_{k} &\sim \text{Inv-Ga}\left((\varsigma_b)_{k}, (\rho_b)_{k} \right) \\
    (v^2_\beta)_{r} &\sim \text{Inv-Ga}\left((\varsigma_\beta)_r, (\rho_\beta)_r \right) \\
    (v^2_\epsilon)_{u} &\sim \text{Inv-Ga}\left((\varsigma_\epsilon)_u, (\rho_\epsilon)_u \right) \\
    \pi_k &\sim \mathrm{Beta}\left((\gamma_\pi)_k, (\delta_\pi)_k \right) \\
    \left(A_k \right)_{1i} &\sim \mathrm{Beta}\left((\gamma_A)_{ki}, (\delta_A)_{ki} \right)
\end{align}

\section{HMM inference}
Given the parameters of our observation model $\theta = (\lambda_0, b, \beta, A, \pi)$, the well-known Forward-Backward Algorithm returns the following posteriors
\begin{align}
    \xi_t \equiv p(z_t|N, \theta) &\qquad \text{posterior marginals} \\
    \Xi_{t, ij} \equiv p(z_{t+1} = j, z_t = i|N, \theta) &\qquad \text{two-slice marginals} \\
    \log Z_t = \log p(N_{t+1 \bullet}|N_{t\bullet}, \theta) &\qquad \text{partition function}
\end{align}
The first two allow us to calculate expressions with respect to $q(z)$, while the last gives the normalization for the joint posterior over all $z$: $\log Z = \sum_t \log Z_t = \log p(N_{1:T}|\theta)$

\section{Evidence Lower Bound (ELBo)}
We would like to maximize a lower bound on the log evidence given by
\begin{equation}
    \mathcal{L} = \mathbb{E}_q \left[ \log \frac{p}{q} \right]
\end{equation}
Thanks to factorization in the priors and posterior ansatz, this can easily be broken down in pieces, one per variable type:

\subsection{$\lambda_0$}
We want
\begin{multline}
    \mathbb{E}_{q(\lambda_0)}\left[ \log \frac{p(\lambda_0)}{q(\lambda_0)}\right] = \sum_u \mathbb{E}_q\left[ -\frac{1}{2v_0^2} (\lambda_{0u} - m_0)^2
    - \frac{1}{2}\log 2\pi v_0^2
    + \frac{1}{2\sigma_{0u}^2} (\lambda_{0u} - \mu_{0u})^2
    + \frac{1}{2}\log 2\pi \sigma^2_{0u}
    \right] \\
    = \sum_u \left[
    -\frac{1}{2v_0^2} \left(\sigma^2_{0u} + (\mu_{0u} - m_0)^2 \right)
    + \log \frac{\sigma_{0u}}{v_0} + \frac{1}{2}
    \right]
\end{multline}
where we have used
\begin{equation}
    \mathbb{E}[X^2] = \mathrm{var}[X] + \mathbb{E}[X]^2
\end{equation}

\subsection{$b$}
As with the $\lambda$ case, we have
\begin{multline}
    \mathbb{E}_{q(b)}\left[ \log \frac{p(b)}{q(b)}\right] = \sum_{ku} \mathbb{E}_q\left[ -\frac{1}{2v_{bk}^2} b_{ku}^2
    - \frac{1}{2}\log 2\pi v_{bk}^2
    + \frac{1}{2\sigma_{bku}^2} (b_{ku} - \mu_{bku})^2
    + \frac{1}{2}\log 2\pi \sigma^2_{bku}
    \right] \\
    = \sum_{ku} \left[
    -\frac{1}{2}\frac{\varsigma_{bk}}{\rho_{bk}}\left(\sigma^2_{bku} + \mu^2_{bku} \right)
    + \frac{1}{2}(\psi(\varsigma_{bk}) - \log \rho_{bk}) - \log \sqrt{2\pi} + \frac{1}{2} \log 2\pi e \sigma^2_{bku}
    \right]
\end{multline}
Where we have used $v^2 \sim \text{Inv-Ga}(\varsigma, \rho)$ to define $\tau = v^{-2} \sim \mathrm{Ga}(\varsigma, \rho)$ and
\begin{align}
    \mathbb{E}[\tau] &= \frac{\varsigma}{\rho} \\
    \mathbb{E}[\log \tau] &= \psi(\varsigma) - \log \rho
\end{align}
with $\psi(x)$ the digamma function.

\subsection{$v^2_b$}
Here again, we will define $\tau = v^{-2} \sim \mathrm{Ga}(\varsigma, \rho)$ so that we can write
\begin{multline}
    \mathbb{E}_{q(\tau)}\left[\log \frac{p(\tau)}{q(\tau)}\right] =
    \sum_k \mathbb{E}_q \left[
    (s_{b} - 1) \log \tau_{bk} - r_{b} \tau_{bk} \right]
    + H_g(\varsigma_{bk}, \rho_{bk})
    + \mathrm{const} \\
    = \sum_k \left[
    (s_{b} - 1) (\psi(\varsigma_{bk}) - \log \rho_{bk})
    - r_{b} \frac{\varsigma_{bk}}{\rho_{bk}} + H_g(\varsigma_{bk}, \rho_{bk})
    \right]
\end{multline}
where we have discarded constants that do not depend on the variational parameters $\varsigma$ and $\rho$ and $H_g$ is the differential entropy of the gamma distribution:
\begin{equation}
    H_g(a, b) = a - \log b + \log \Gamma(a) + (1 - a)\psi(a)
\end{equation}

\subsection{$\beta$, $\epsilon$}
Formulas in this case are the same as those given above for $b$, with only trivial substitutions required.

\subsection{$\pi$}
First, we have
\begin{equation}
    \mathbb{E}_{q(\pi)} \left[\log \frac{p(\pi)}{q(\pi)} \right] = \sum_k \left[(a_{\pi k} - 1)\overline{\log \pi_{k1}} + (b_{\pi k} - 1) \overline{\log \pi_{k0}} - \log B(a_{\pi k}, b_{\pi k}) + H(\pi_k) \right]
\end{equation}
with
\begin{align}
    \overline{\log \pi_{k1}} &= \psi(\gamma_{\pi k}) - \psi(\gamma_{\pi k} + \delta_{\pi k}) \\
    \overline{\log \pi_{k0}} &= \psi(\delta_{\pi k}) - \psi(\gamma_{\pi k} + \delta_{\pi k})
\end{align}
with $\psi$ the digamma function and
\begin{equation}
    H(\pi_k) = H_b(\gamma_{\pi k}, \delta_{\pi k})
\end{equation}
with $H_b$ the entropy of the beta distribution:
\begin{equation}
    H_b(\alpha, \beta) = \log B(\alpha, \beta) - (\alpha - 1) \psi(\alpha) - (\beta - 1) \psi(\beta) + (\alpha + \beta - 2)\psi(\alpha + \beta)
\end{equation}

\subsection{$A$}
Similarly,
\begin{equation}
    \mathbb{E}_{q(A)} \left[\log \frac{p(A)}{q(A)} \right] =
\sum_{ik} \left[ (a_{Aki} - 1) \overline{\log A_{k1i}} + (b_{Aki} - 1) \overline{\log A_{k0i}} - \log B(a_{Aki}, b_{Aki}) + H(A_{k1i}) \right]
\end{equation}
where
\begin{align}
    \overline{\log A_{k1i}} &= \psi(\gamma_{Aki}) - \psi(\gamma_{Aki} + \delta_{Aki}) \\
    \overline{\log A_{k0i}} &= \psi(\delta_{Aki}) - \psi(\gamma_{Aki} + \delta_{Aki})
\end{align}
and again
\begin{equation}
    H(A_{k1i}) = H_b(\gamma_{Aki}, \delta_{Aki})
\end{equation}

\subsection{Observation model}
Finally, we would like to calculate the piece of the evidence lower bound arising from the Poisson observation model:
\begin{multline}
    \label{obsmodel}
    \mathbb{E}_q \left[ \log \frac{p(N, z|\theta)}{q(z)} \right] =
    \sum_{m} \left[
    N_{m} \mathbb{E}_q [\eta_m]
    - \mathbb{E}_q[e^{\eta_m}] \right]
    + \sum_{kt} \left[\mathrm{tr}\left(\Xi_{kt} \overline{\log A_k^T}\right) + \xi_{0k}^T \overline{\log \pi_k} \right] \\
    - \sum_{kt} \left[ \xi_{tk}^T \nu_{tk} + \mathrm{tr}\left(\Xi_{kt} \tilde{A}_k^T\right) + \xi_{0k}^T \tilde{\pi}_k - \log Z_{kt} \right]
\end{multline}
with $\xi$ and $\Xi$ defined as the expected value of $z$ and the two-slice marginals, as above, and $(\nu, \tilde{\pi}, \tilde{A})$ the variational parameters in the posterior $q(z)$. Roughly, the three terms above correspond to $\log p(N|z, \theta)$, $\log p(z|A, \pi)$, and $\log q(z)$.

The first two terms involving expectations of $\lambda_{mu}$ can be calculated from (\ref{loglambda}) above and the variational ansatz:
\begin{align}
    \mathbb{E}_q [\eta_m] &= \overline{\eta_m} =
    \mu_{0u(m)} + \sum_{k} \xi_{t(m) k} \mu_{bku(m)} + \sum_r x_{mr} \mu_{\beta r u(m)} + \mu_{\epsilon m} \\
    \mathbb{E}_q [e^{\eta_m}] &= \overline{e^{\eta_m}} = \exp \left(
    \overline{\eta_{m \setminus b}}
    + \frac{1}{2} \left[ \sigma^2_{0u(m)}
    + \sum_r x^2_{m r} \sigma^2_{\beta ru(m)} + \sigma^2_{\epsilon u(m)}
    \right]
    \right) \\ \nonumber
    &\times \left[
    \prod_k \left(
    1 - \xi_{t(m) k} + \xi_{t(m) k} e^{\mu_{bku(m)} + \sigma^2_{bku(m)} / 2}
    \right)\right]
\end{align}
with $\overline{\eta_{m \setminus b}}$ the same as $\overline{\eta_m}$ with $zb$ terms removed. We have also used the expression for the moment generating function of a normally distributed variable:
\begin{equation}
    \mathbb{E}[e^{tX}] = e^{t\mu + \frac{1}{2} t^2\sigma^2}
\end{equation}
and the fact that $z^2 = z$ since the latents are assumed to be binary.


\section{Variational Updates}

\subsection{$z$}
Technically, $\xi$ is not a variational parameter, but depends on the variational parameters in $q(z)$: $\xi = \xi(\nu, \tilde{A}, \tilde{\pi})$, and similarly for $\Xi$. For the actual variational parameters, the updates are straightforward:
\begin{align}
    \nu_{tk} &\leftarrow \sum_{t(m) = t}
    \begin{pmatrix}
        - F_{mk} \\
        N_{m} \mu_{bku(m)} -
        e^{\mu_{bku(m)} + \sigma^2_{bku(m)} / 2} F_{mk}
    \end{pmatrix} \\
    \tilde{A}_{k} &\leftarrow \overline{\log A_k} \\
    \tilde{\pi}_k &\leftarrow \overline{\log \pi_k}
\end{align}
Here, we sum over all observations at stimulus time $t$, and $\eta$ the local evidence for $z$ is calculated by setting the relevant $z$ to 0 or 1. For simplicity, we also define
\begin{equation}
    F_{mk} = \frac{\overline{e^{\eta_m}}}{1 - \xi_{t(m) k} + \xi_{t(m) k} e^{\mu_{bku(m)} + \sigma^2_{bku(m)} / 2}}
\end{equation}

We note, after Beal, that as a result of these updates, $\tilde{A}$ and $\tilde{\pi}$ are subadditive (i.e., they do not sum to 1), but that the forward-backward algorithm nonetheless returns a correctly normalized posterior.

\subsection{$\pi$, $A$}
Given conjugacy, these are trivial to write down:
\begin{align}
    \gamma_{\pi k} &\leftarrow a_{\pi k} + \xi_{0k} \\
    \delta_{\pi k} &\leftarrow b_{\pi k} + 1 - \xi_{0k} \\
    \gamma_{A ki} &\leftarrow a_{A ki} + \sum_t \Xi_{kt, 1i} \\
    \delta_{A ki} &\leftarrow b_{A ki} + \sum_t \Xi_{kt, 0i}
\end{align}

\subsection{$v^2$}
Here again, conjugacy makes the updates trivial for the $v^2$ terms. For both $b$ and $\beta$
\begin{align}
    \varsigma_k &\leftarrow s + \frac{U}{2} \\
    \rho_k &\leftarrow r + \frac{1}{2}\sum_u (\sigma^2_{ku} + \mu^2_{ku}) \\
\end{align}
while for $\epsilon$
\begin{align}
    \varsigma_u &\leftarrow s + \frac{M_u}{2} \\
    \rho_u &\leftarrow r + \frac{1}{2} \sum_m (\sigma^2_{mu} + \mu^2_{mu})
\end{align}

\subsection{$\lambda_0$ $b$, $\beta$, $\epsilon$}
Here, the nonlinearity in $\overline{e^{\eta_m}}$ makes updates less straightforward. In each case, we have something like
\begin{equation}
    \frac{\partial\mathcal{L}}{\partial \mu} = \frac{\partial\mathcal{E}}{\partial \mu} + \sum_{m} \left[N_{m} \frac{\partial\overline{\eta_{m}}}{\partial \mu} - \frac{\partial\overline{e^{\eta_{m}}}}{\partial \mu}\right] = 0
\end{equation}
which works out in the various cases as
\begin{align}
    \frac{\partial\mathcal{L}}{\partial \mu_{0u}} &= -(\mu_{0u} - m_0)\tau_0 + \sum_{u(m) = u} [N_{m} - \overline{e^{\eta_{m}}}] \\
    \frac{\partial\mathcal{L}}{\partial \mu_{bku}} &=
    - \mu_{bku} \overline{\tau_{bk}} + \sum_{u(m) = u} [N_{m} - e^{\mu_{bku(m)} + \sigma^2_{bku(m)} / 2} F_{mk} ] \xi_{t(m) k} \\
    \frac{\partial\mathcal{L}}{\partial \mu_{\beta ru}} &=
    - \mu_{\beta ru} \overline{\tau_{\beta r}} + \sum_{u(m) = u} [N_{m} - \overline{e^{\eta_{m}}}] x_{mr}  \\
    \frac{\partial\mathcal{L}}{\partial \mu_{\epsilon m}} &=
    - \mu_{\epsilon m} \overline{\tau_{\epsilon u(m)}} + N_{m} -
    \overline{e^{\eta_m}}
\end{align}
Unfortunately, $\overline{e^{\eta_m}}$ depends on the $\mu$ variables in question, requiring us to solve a transcendental equation in each case.

The case of variation with respect to $\sigma^2$ is similar:
\begin{align}
    \frac{\partial\mathcal{L}}{\partial \sigma^2_{0u}} &= -\frac{1}{2}\tau_0 + \frac{1}{2\sigma^2_{0u}} - \frac{1}{2} \sum_{u(m) = u} \overline{e^{\eta_m}} \\
    \frac{\partial\mathcal{L}}{\partial \sigma^2_{bku}} &=
    - \frac{1}{2} \overline{\tau_{bk}} + \frac{1}{2\sigma^2_{bku}} - \frac{1}{2}\sum_{u(m) = u} \xi_{t(m)k} e^{\mu_{bku(m)} + \sigma^2_{bku(m)} / 2} F_{mk}  \\
    \frac{\partial\mathcal{L}}{\partial \sigma^2_{\beta ru}} &=
    - \frac{1}{2} \overline{\tau_{\beta r}} + \frac{1}{2\sigma^2_{\beta ru}} - \frac{1}{2} \sum_{u(m) = u} x^2_{mr} \overline{e^{\eta_m}} \\
    \frac{\partial\mathcal{L}}{\partial \sigma^2_{\epsilon m}} &=
    -\frac{1}{2} \overline{\tau_{\epsilon u(m)}} + \frac{1}{2\sigma^2_{\epsilon m}} - \frac{1}{2} \overline{e^{\eta_m}}
\end{align}
where $\overline{\tau} = \mathbb{E}[1/v^2] = \varsigma / \rho$.

Note, however, that for unconstrained minimization, we need to shift parameterization to $\sigma^2 = e^\kappa$, with the result that

\begin{align}
    \frac{\partial\mathcal{L}}{\partial \kappa_{0u}} &= -\frac{1}{2}\tau_0 e^{\kappa_{0u}} + \frac{1}{2} - \frac{1}{2} \sum_{u(m) = u} e^{\kappa_{0u}} \overline{e^{\eta_m}} \\
    \frac{\partial\mathcal{L}}{\partial \kappa_{bku}} &=
    - \frac{1}{2} \overline{\tau_{bk}} e^{\kappa_{bku}} + \frac{1}{2} - \frac{1}{2}\sum_{u(m) = u} \xi_{t(m)k} e^{\mu_{bku(m)} + \frac{1}{2} e^{\kappa_{bku}} + \kappa_{bku}} F_{mk}  \\
    \frac{\partial\mathcal{L}}{\partial \kappa_{\beta ru}} &=
    - \frac{1}{2} \overline{\tau_{\beta r}} e^{\kappa_{\beta bru}} +
    \frac{1}{2} - \frac{1}{2} \sum_{u(m) = u} x^2_{mr} e^{\kappa_{\beta ru}} \overline{e^{\eta_m}} \\
    \frac{\partial\mathcal{L}}{\partial \kappa_{\epsilon m}} &=
    -\frac{1}{2} \overline{\tau_{\epsilon u(m)}} e^{\kappa_{\epsilon m}} + \frac{1}{2} - \frac{1}{2} e^{\kappa_{\epsilon m}} \overline{e^{\eta_m}}
\end{align}

\subsubsection{Hessians}
Because the structure of the observation model is a sum over (independent)observations and each observation corresponds to a particular $(t, u)$ the Hessian matrix takes on a particularly simple form within each sub-block:
\begin{align}
    \frac{\partial^2\mathcal{L}}{\partial \mu_{0u}\partial \mu_{0u'}} &=
    -\left[\tau_0 + \sum_{u(m)=u} \overline{e^{\eta_m}}\right]\delta_{uu'} \\
    \frac{\partial^2\mathcal{L}}{\partial \mu_{bku}\partial \mu_{bk'u'}} &=
    -\left[\overline{\tau_{bk}} + \xi_{t(m) k}\sum_{u(m) = u} e^{\mu_{bku(m)} + \sigma^2_{bku(m)} / 2} F_{mk} \right]\delta_{uu'} \delta_{k k'} \\
    \frac{\partial^2\mathcal{L}}{\partial \mu_{\beta ru}\partial \mu_{\beta r'u'}} &=
    -\left[\overline{\tau_{\beta r}} + \sum_{u(m) = u} x^2_{mr} \overline{e^{\eta_m}} \right]\delta_{uu'} \delta_{r r'} \\
\end{align}
to be continued...

\section{Bottleneck model}
Consider a log-linear Poisson model with a matrix of count observations of $U$ units at $T$ times.
\begin{equation}
    N_{tu} \sim \mathrm{Poisson}(e^{\eta_{tu}})
\end{equation}
Furthermore, assume that the variable $\eta$ is Gaussian:
\begin{equation}
    \eta_{t\cdot} \sim \mathcal{N}\left(a_{\cdot} + \sum_{r=1}^R x_{tr} b_{r\cdot} +
    \sum_{k=1}^K z_{tk} c_{k\cdot}, \Sigma_\varepsilon\right)
\end{equation}
where $z \in \{0, 1\}$ and the variables $x$ are known regressors. We can then write the observation model as
\begin{equation}
    p(N) = p(N|\eta) p(\eta|a, b, c, z, \Sigma_\varepsilon) p(a) p(b) p(c) p(z) p(\Sigma_\varepsilon)
\end{equation}
We will assume the following priors for the parameters:
\begin{align}
    a_u &\sim \mathcal{N}(m_a, s_a^2) \\
    b_{\cdot u} &\sim \mathcal{N}(m_b, S_b) \\
    c_{\cdot u} &\sim \mathcal{N}(m_c, S_c) \\
    z_{\cdot k} &\sim \mathrm{HMM}(A_k, \pi_k) \\
    \Sigma_\varepsilon &\sim \text{Inv-Wishart}
\end{align}
That is, the coefficients $a$, $b$, and $c$ for each unit are drawn from population normal distributions (perhaps with nontrivial covariance), while the hidden binary variables $z$ are drawn from separate hidden Markov models.

Now, the variational posterior $q$ can be chosen as a product of the following:
\begin{align}
    \eta_{t\cdot} &\sim \mathcal{N}(\mu^t_\eta, \Sigma^t_\eta) \\
    a_u &\sim \mathcal{N}(\mu^u_a, (\sigma^u_a)^2) \\
    b_{\cdot u} &\sim \mathcal{N}(\mu^u_b, \Sigma^u_b) \\
    c_{\cdot u} &\sim \mathcal{N}(\mu^u_c, \Sigma^u_c) \\
    z_{\cdot k} &\sim \mathrm{HMM}(\tilde{A}_k, \tilde{\pi}_k) \\
    \Sigma_\varepsilon &\sim \text{Inv-Wishart}
\end{align}

Some notes:
\begin{itemize}
    \item Because of the linear-Gaussian form of the model for $\eta$, the Gaussian variational posteriors of $a$, $b$, and $c$ can make use of conjugacy for rapid, exact coordinate ascent.
    \item For the priors, we specified a single population prior for each unit, but for the posteriors, we clearly need a posterior over \emph{each} unit. The same holds for the posteriors for $\eta$ at distinct times.\footnote{In this case, we might want a posterior that incorporates autocorrelation in time. File under future work.}
    \item The HMM inference can be performed exactly by the forward-backward algorithm. Our only approximation is to assume that the posterior $q(z)$ takes the form of a product over \emph{independent} chains. This is only approximate because these chains are coupled through the observations (and intermediately through $\eta$) in the true posterior.
    \item The only non-conjugacy in the model is in $\eta$, since we assume a Gaussian posterior and a Poisson observation model for the counts.
\end{itemize}

\subsection{LKJ covariance prior}
While an Inverse-Wishart prior on the noise covariance matrix $\Sigma_\varepsilon$ is conjugate and therefore a natural choice, there is no simple closed form way to calculate either $\mathbb{E}_q[\log p(\Sigma)]$ or $\mathcal{H}(\Sigma)$. Moreover, reasoning about covariances is comparatively more difficult than reasoning about correlations. Thus, rather than a distribution on $\Sigma_\varepsilon$, we can opt for a distribution on the pieces of the decomposition
\begin{align}
    \label{sigeps_def}
    \Sigma_\varepsilon &= S^{\frac{1}{2}} \Omega S^{\frac{1}{2}} \\
    S^{\frac{1}{2}} &= \mathrm{diag}(\boldsymbol{\sigma}) \\
    \sigma^2_i &\sim \text{Inv-Gamma}(a_i, b_i) \\
    \Omega &\sim \mathrm{LKJ}(\eta)
\end{align}
where the LKJ prior\footnote{Lewandowski, Kurowicka, and Joe: Generating Random Correlation Matrices based on vines and the extended onion method} is a prior on \emph{correlation matrices} where the density is proportional to $ |\Omega|^{\eta - 1}$. Thus, as $\eta$ grows, the correlation matrix increasingly resembles the unit matrix. Conveniently, LKJ give an algorithm for drawing from this distribution that relies on the fact that the underlying degrees of freedom are the canonical partial correlations (CPCs) of the variables: $\rho_{ij;1\ldots i - 1}$ ($j > i$).\footnote{For example, for $d=4$, these are the correlations $p_{12}$, $p_{13}$, $p_{14}$, $p_{23;1}$, $p_{24;1}$, and $p_{34;12}$.} These CPCs can then be related to the correlations in $\Omega$ by the definitions
\begin{align}
    \omega_{ij} &\equiv \rho_{ij} \\
    \label{rho_recursion}
    \rho_{ij;kL} &= \frac{\rho_{ij;L} - \rho_{ik;L}\rho_{jk;L}}
    {\sqrt{(1 - \rho_{ik;L}^2)(1 - \rho_{jk;L}^2)}}
\end{align}
In fact, one can show that if we choose $L < k < i < j$ (e.g., $\rho_{56;1234}$ for $k=4$, $L = 123$), then the $\rho_{ij}$ can all be computed recursively using \emph{only} CPCs, and no intermediate partials are shared across computations for different $i$ and $j$. As a result, no clever caching is necessary, and the computations could even be done in parallel.

Moreover, it is useful to know that the Cholesky factorization for $\Omega$ takes a particularly simple form. If $L = U^\top$ satisfies $LL^\top = \Omega$, then
\begin{equation}
    U =
    \begin{pmatrix}
    1 & r_{12} & r_{13} & r_{14} & \ldots \\
    & \sqrt{1 - r_{12}^2} & \sqrt{1 - r_{13}^2}r_{23} & \sqrt{1 - r_{14}^2}r_{24} & \ldots \\
    && \sqrt{(1 - r_{13}^2)(1 - r_{23}^2)} & \sqrt{(1 - r_{14}^2)(1 - r_{24}^2)}r_{34} & \ldots \\
    &&& \sqrt{(1 - r_{14}^2)(1 - r_{24}^2)(1 - r_{34}^2)} & \ldots \\
    \end{pmatrix}
\end{equation}
where $r_{ij} = [R]_{ij}$ is the matrix of canonical partial correlations: $r_{ij} = \rho_{ij;\lbrace k | k < i,j\rbrace}$. Thus we have
\begin{equation}
    |U| = \prod_i U_{ii} = \prod_{i < j} \sqrt{1 - r_{ij}^2}
\end{equation}
and
\begin{equation}
    \label{det_omega}
    |\Omega| = |U|^2 = |L|^2 = \prod_{i < j} (1 - r_{ij}^2)
\end{equation}

Just as importantly, the CPCs follow simple Beta distributions (rescaled to the interval (-1, 1)):
\begin{align}
    r_{i,j > i;1\ldots i - 1} &\sim \mathrm{Beta}(\beta_{i}, \beta_{i}) \\
    \beta_i &= \eta + \frac{d - 1 - i}{2}
\end{align}
Thus, if we choose the LKJ prior, the covariance $\Sigma_\varepsilon$ can be replaced by $d$ Inverse-Gamma variates $\sigma^2$ and $d(d-1)/$ Beta variates $p$ without loss of generality. Calculating expectations of log priors and entropies of these variables is then straightforward.

Moreover, we can note that for all canonical partial correlations $r = 2x - 1$ for $x \sim \mathrm{Beta}(\beta, \beta)$, we have $E[\rho] = 0$. And since every entry in $\Omega \sim LKJ(\eta)$, a full correlation, is constructed from sums of products of random variables (stemming from the recursion in (\ref{rho_recursion})), each of which contains at least one partial correlation with mean 0, we have $\mathbb{E}[\Omega] = \mathbf{1}$, the identity correlation matrix.

Likewise, we will want to make use of the expectation of the log determinant of $\Omega$, which we can calculate from the above as
\begin{equation}
    \mathbb{E}[\log |\Omega|] = \sum_{i < j} \mathbb{E}[\log (1 - r_{ij}^2)]
\end{equation}
From the above, we can rewrite this in terms of the beta-distributed variables $x_{ij} \sim \mathrm{Beta}(\beta_i, \beta_i)$ as
\begin{align}
    \label{Elogdet_Omega}
    \mathbb{E}[\log |\Omega|] &= \sum_{i < j} \mathbb{E}[\log (4x_{ij}(1 - x_{ij}))] \\
    &= d(d - 1) \log 2 + 2\sum_{i} (d - i)[\psi(\beta_i) - \psi(2\beta_i)]
\end{align}
That is, the expectation of the log determinant is simply proportional to the sum of expectations of the logs of the rescaled partial correlations $x$.

\subsection{Inference on log firing rate}
If we write the evidence lower bound as
\begin{equation}
    \mathcal{L} = \mathbb{E}_q \left[ \frac{\log p}{\log q}\right] =
    \mathbb{E}_q[\log p] + \mathcal{H}[q]
\end{equation}
Then the only non-conjugate terms, as noted above, are related to $\eta$:
\begin{align}
    \mathcal{L}_\eta &=
    \sum_{tu} \mathbb{E}_q[N_{tu} \eta_{tu} - e^{\eta_{tu}}]
    - \frac{1}{2} \sum_{t} \mathbb{E}_q \left[
    (\eta_t - \overline{\eta}_t)^\top \Sigma_\varepsilon^{-1}(\eta_t - \overline{\eta}_t)
    + \log |\Sigma_\varepsilon| \right]\\
    &= \sum_{tu} \left[ N_{tu} \mu_{\eta tu} -
    \exp\left(\mu_{\eta tu} + \frac{1}{2} \Sigma_{\eta tuu} \right)\right] \\
    \label{eta_cov}
    &- \frac{1}{2} \sum_{t} \mathrm{Tr}\left(
    \mathbb{E}_q[\Sigma_\varepsilon^{-1}]
    \left[ \Sigma_{\eta t} + (\mu_{\eta t} - \mathbb{E}_q[\overline{\eta}_{t}])
    (\mu_{\eta t} - \mathbb{E}_q[\overline{\eta}_{t}])^\top
    + \mathrm{Cov}[\overline{\eta}]_{t}\right] \right) \\
    &- \frac{T}{2} \mathbb{E}_q[\log |\Sigma_\varepsilon|] + const
\end{align}
with $\overline{\eta} \equiv a + bx + cz$ and
\begin{align}
    \mathrm{Cov}[\overline{\eta}] &= \mathrm{Cov}[a] + \mathrm{Cov}[bx] + \mathrm{Cov}[cz] \\
    \mathrm{Cov}[a]_{tuu'} &= \delta_{uu'}(\sigma^a_u)^2 \\
    \mathrm{Cov}[bx]_{tuu'} &= \delta_{uu'}\mathrm{Tr}\left((x_t\cdot x_t^\top) \Sigma^b_{u}\right) \\
    \mathrm{Cov}[cz]_{tuu'} &= \mathbb{E}_q\left[\sum_{k}\left( c_{ku} z_{tk} - \mu^c_{ku}\xi_{tk} \right)\sum_{k'}\left( c_{k'u'} z_{tk'} - \mu^c_{k'u'}\xi_{tk'} \right)\right] \\
    &= \sum_{kk'}\left[\left(\delta_{uu'}\Sigma^c_{kk'} + \mu^c_{ku} \mu^c_{k'u'} \right)\left(\delta_{kk'}\xi_{tk}(1 - \xi_{tk}) + \xi_{tk}\xi_{tk'}\right) - \mu^c_{ku}\mu^c_{k'u'}\xi_{tk}\xi_{tk'} \right] \\
    &= \delta_{uu'}\left(\sum_k \Sigma^c_{kku} \xi_{tk} (1 - \xi_{tk}) + \xi_t^\top \Sigma_u^c \xi_t\right) + \sum_k \mu^c_{ku}\mu^c_{ku'}\xi_{tk}(1 - \xi_{tk})
\end{align}
In the case of the LKJ prior on $\Sigma_\varepsilon$ discussed above, this can be further simplified, since $\mathbb{E}_q[\Sigma_\varepsilon^{-1}] = \mathbb{E}_q[\mathrm{diag}(\boldsymbol{\sigma}^{-2})] \equiv \boldsymbol{\tau}$. Since this matrix is diagonal, (\ref{eta_cov}) simplifies to
\begin{align}
    & -\frac{1}{2} \sum_{tu}\tau_u \left[ \Sigma^\eta_{tuu} +
    \left(\mu^\eta_{tu} - \mu^a_{u} - \sum_r \mu^b_{ru} x_{tr} - \sum_k \mu^c_{ku} \xi_{tk}\right)^2
    \right. \\
    & \left.
    + (\sigma^a_u)^2 + x^\top_t \Sigma^b_u\, x_t +
    \xi^\top_t \Sigma^c_u\, \xi_t +
    \sum_k (\Sigma^c_{kku} + (\mu^c_{ku})^2)\xi_{tk}(1 - \xi_{tk})
    \right]
\end{align}

We can also simplify the $\log |\Sigma_\varepsilon|$ term by noting that
\begin{align}
    -\frac{T}{2}\mathbb{E}_q[\log |\Sigma_\varepsilon|] &=
    -\frac{T}{2}\mathbb{E}_q[\log S + \log |\Omega|] \\
    &= -\frac{T}{2}\mathbb{E}_q[\sum_u \log \sigma^2_u + \log |\Omega|] \\
    &= \frac{T}{2}\sum_u \mathbb{E}_q[-\log \sigma_u^2] - \frac{T}{2}\mathbb{E}_q[\log |\Omega|] \\
    &= \frac{T}{2}\sum_u \overline{-\log \sigma_u^2}  - T\sum_{u} (U - u)[\psi(\beta_u) - \psi(2\beta_i)]
\end{align}
where the last calculation follows from (\ref{Elogdet_Omega}) and if $\sigma^2 \sim \text{Inv-Ga}(a, b)$, $1/\sigma^2 \sim \mathrm{Ga}(a, b)$ and $\overline{-\log \sigma_u^2} = \psi(a) - \log b$.

We can then ask what happens when we look for local minima of this objective with respect to $\mu_\eta$ and $\Sigma_\eta$:
\begin{align}
    \frac{\partial \mathcal{L}}{\partial \mu_{tu}} &=
    N_{tu} - \exp\left(\mu_{tu} + \frac{1}{2} \Sigma_{\eta tuu} \right)
    - \mathbb{E}_q[\Sigma_\varepsilon^{-1}](\mu_{tu} - \mathbb{E}_q[\overline{\eta}_{tu}]) \\
    \frac{\partial \mathcal{L}}{\partial \Sigma_{\eta tu \neq v}} &=
    \frac{1}{2} \left(\Sigma^{-1}_\eta \right)_{tuv}
    -\frac{1}{2}\mathbb{E}_q[\Sigma_\varepsilon^{-1}]_{uv}\\
    \frac{\partial \mathcal{L}}{\partial \Sigma_{\eta tuu}} &=
    -\frac{1}{2} \exp\left(\mu_{tu} + \frac{1}{2} \Sigma_{\eta tuu} \right)
    + \frac{1}{2} \left(\Sigma^{-1}_\eta \right)_{tuu}
    -\frac{1}{2}\mathbb{E}_q[\Sigma_\varepsilon^{-1}]_{uu}
\end{align}
and asking $\nabla \mathcal{L} = 0$ then immediately gives $\Sigma^{-1}_{\eta tuv} = \mathbb{E}_q[\Sigma_\varepsilon^{-1}]$ for $u\neq v$ --- that is, the precision matrix $\Sigma^{-1}_{\eta t}$ is equal to the expected value of the noise matrix off-diagonal.

\subsection{Inference on latents}
Here as above, inference for the latents $z_{tk}$ can be done via the forward-backward algorithm. However, for that algorithm, we also need the log evidence of the observations conditioned on latent state assignments:
\begin{equation}
    \log \psi_{tk} = \log p(N|z_{tk})
\end{equation}

The expression in the bottleneck model is different than that for $\nu_{tk}$ and $F$ calculated in the model without $\eta$ above. In this case, it results from the term in $\mathcal{L}$
\begin{equation}
    - \frac{1}{2} \sum_{t} \mathbb{E}_q \left[
    (\eta_t - \overline{\eta}_t)^\top \Sigma_\varepsilon^{-1}(\eta_t - \overline{\eta}_t) \right]
\end{equation}
which (momentarily ignoring expectations) has the following terms involving $z$:
\begin{equation}
    -\frac{1}{2}\sum_{uu'k}\left(\Sigma_\varepsilon\right)^{-1}_{uu'} \left[-2(\eta_{tu'} - a_{u'} - \sum_r b_{ru'} x_{tr})c_{ku} + \sum_{k'}c_{ku}c_{k'u'}z_{tk'}\right]z_{tk}
\end{equation}
for which the expectation over variables other than $z_{tk}$ reduces to:
\begin{multline}
    -\frac{1}{2}\sum_{uu'k}\mathbb{E}_q\left[\left(\Sigma_\varepsilon\right)^{-1}_{uu'}\right]
    \left[-2\left((\mu_\eta)_{tu'} - (\mu_a)_{u'} - \sum_r (\mu_b)_{ru'} x_{tr}\right)
    (\mu_c)_{ku} + \right. \\
    \left.
    \sum_{k'}((\mu_c)_{ku}(\mu_c)_{k'u'} + (\Sigma_c)_{ukk'}\delta_{uu'})(\xi_{tk'} + (1 - \xi_{tk'})\delta_{kk'})\right]z_{tk}
\end{multline}
where we have made use of the fact that $z^2 = z$ in deriving th term proportional to $\delta_{kk'}$.\footnote{Note that while it appears that the term in brackets depends on $\xi_{tk} = \mathbb{E}[z_{tk}]$, a simple calculation shows that it only depends on terms with $k' \neq k$.}

This further simplifies when we make use of the decomposition of $\Sigma_\varepsilon$ in terms of scale and correlation matrices (\ref{sigeps_def}), for which we have $\mathbb{E}[\Omega] = \mathbf{1}$. In fact, we also have $\mathbb{E}[\Omega^{-1}] = \mathbf{1}$. This can be argued straightforwardly from the permutation and rotational symmetries of the LKJ likelihood.

Thus we can write
\begin{equation}
    \mathbb{E}_q\left[\Sigma_\varepsilon^{-1}\right] =
    \mathbb{E}_\sigma \left[ S^{-\frac{1}{2}} \mathbf{1} S^{-\frac{1}{2}}\right]
    = \mathbb{E}_\sigma \left[\mathrm{diag}(\boldsymbol{\sigma}^{-2}) \right]
\end{equation}
yielding
\begin{multline}
    \log \psi_{tk} = \sum_u \tau_u \left[
    \left((\mu_\eta)_{tu} - (\mu_a)_{u} - \sum_r (\mu_b)_{ru} x_{tr}
    \right)
    (\mu_c)_{ku} \right .\\
    \left.
     -\frac{1}{2} \sum_{k'}((\mu_c)_{ku}(\mu_c)_{k'u} + (\Sigma_c)_{ukk'})(\xi_{tk'} + (1 - \xi_{tk'})\delta_{kk'})\right]
\end{multline}
where $\tau_u \equiv \mathbb{E}[\sigma_u^{-2}]$.
Note that here, we have taken $\psi$ here as $T \times K$ rather than $T \times 2 \times K$, with one entry for each state. This is because only \emph{relative} log probabilities are needed for the forward-backward algorithm, so for each chain $k$, we can take $\log \psi_t$ as a two-column matrix with the first column all 0s.

\subsubsection{DEPRECATED: Solving for $\mu$}
The above transcendental equation for $\mu$ can be solved by using the Lambert W function, which satisfiex $z = W(z)e^{W(z)}$. That is, $y = W(z)$ is the solution to $ye^y = z$. Moreover, the equation
\begin{equation}
    p^{ax + b} = cx + d
\end{equation}
has the solution
\begin{equation}
    x = \frac{-W\left(-\frac{a \log p}{c}
    p^{b - \frac{ad}{c}} \right)}{a \log p} - \frac{d}{c}
\end{equation}
Thus, we can substitute
\begin{align}
    p &= e \\
    a &= 1 \\
    b &= \frac{1}{2} \sigma^2 \\
    c &= -\tau \\
    d &= N + 1 + \tau \mathbb{E}_q[\overline{\eta}]
\end{align}
into the above to yield
\begin{equation}
    \mu = N\tau^{-1} + \mathbb{E}_q[\overline{\eta}] -
    W\left(\tau^{-1} \exp \left(
    \frac{1}{2}\sigma^2 + N\tau^{-1} + \mathbb{E}_q[\overline{\eta}]
    \right)\right)
\end{equation}
Moreover, we note that when $x \gg 1$, $W(x) \approx \log x - \log \log x + o(1)$.

\subsubsection{DEPRECATED: Solving for $\sigma^2$}
For $\sigma^2$, we have the condition
\begin{equation}
    \sigma^2 = \frac{1}{\tau + \exp\left(\mu + \frac{1}{2}\sigma^2 \right)}
\end{equation}
Solving this is similar to solving the fixed point equation $x = f(x)$ with $f$ the ($x$-reversed) logistic function. This equation can potentially solved by iteration, but one must be careful of the constants involved (so the approximation converges).

\end{document}
