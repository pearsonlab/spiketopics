\documentclass[11pt]{article}
\usepackage{amssymb,amsmath}

\begin{document}
\title{The Log-Normal Model}
\author{John Pearson}
\maketitle

\section{Observation Model}
Assume an experiment with $M$ observations (one per unit, time combination). Each unit is responsive to $R$ regressors and $K$ latent states. Stimuli are indexed by a time variable with $T$ unique values. Spike counts are governed by a Poisson process given by
\begin{equation}
    N_{m} \sim \mathrm{Poiss}(e^{\eta_{m}})
\end{equation}
where $N_{m}$ is the count from observation $m$ and the effective rate $\eta_{m}$ is given by
\begin{equation}
    \label{loglambda}
    \eta_m = \lambda_{0u(m)} + \sum_k z_{t(m)k} b_{ku(m)} + \sum_r x_{mr} \beta_{ru(m)} + \epsilon_{m}
\end{equation}
Here, the coefficients $b_{ku}$ index responses of each unit to the (binary) latent states $z_{mk}$ while the $\beta_{ru}$ index responses to (non-latent) regressors $x_{mr}$. The final ($\epsilon$) term represents moment-to-moment fluctuations of each unit. We have also assumed functions $u(m)$ and $t(m)$ mapping each observation to its appropriate unit and time, respectively.

\section{Latent feature model}
We will assume a Hidden Markov Model (HMM) for the latent states $z$. We will likewise assume that these latent features are present in the stimulus, so that they are indexed by the stimulus time $t(m)$. When the meaning is clear, we will drop the dependency of stimulus time on observation number and simply write $t$.

To describe the transition model for the HMM, we take
\begin{align}
    A_{ij} &\equiv p(z_{t+1} = i|z_t = j) \\
    \pi_i &\equiv p(z_0 = i)
\end{align}
(Note that this means that the \emph{columns} of $A$ sum to 1, which is the opposite of the usual convention. In other words, in matrix notation, $z_{t+1} = A \cdot z_t$.)

Given this notation, it is straightforward to write the probability of a sequence of hidden states, conditioned on the chain parameters
\begin{equation}
    \log p(z|A, \pi) = \sum_t \log A_{z_{t+1} z_t} + \log \pi_{z_0}
\end{equation}

\section{Priors}
In addition to the model (\ref{loglambda}) above and the HMM ansatz, we posit the following hierarchical generative model for the parameters:
\begin{align}
    \lambda_{0u} &\sim \mathcal{N}(m_0, v^2_0) \\
    b_{ku} &\sim \mathcal{N}\left(0, (v^2_b)_{k}\right) \\
    \beta_{ku} &\sim \mathcal{N}\left(0, (v^2_\beta)_{k}\right) \\
    \epsilon_m &\sim \mathcal{N}\left(0, (v^2_\epsilon)_{u(m)}\right) \\
    (v^2_b)_{k} &\sim \text{Inv-Ga}\left(s_b, r_b \right) \\
    (v^2_\beta)_{r} &\sim \text{Inv-Ga}\left(s_\beta, r_\beta \right) \\
    (v^2_\epsilon)_{u} &\sim \text{Inv-Ga}\left(s_\epsilon, r_\epsilon \right) \\
    \pi_k &\sim \mathrm{Beta}\left((a_\pi)_k, (b_\pi)_k \right) \\
    \left(A_k \right)_{1i} &\sim \mathrm{Beta}\left((a_A)_{ki}, (b_A)_{ki} \right)
    \end{align}
where $A_{1i}$ is the probability of the transition $i \rightarrow 1$ (i.e., the transition \emph{into} the $z = 1$ state.)

Note also that, by putting sparse priors on the $v^2$ terms, we can implement automatic relevance determination (ARD) on the regression coefficients.

\section{Variational ansatz}
We would like to approximate the joint posterior density
\begin{multline}
    p(\lambda_0, b, \beta, A, \pi, z, \epsilon|N) \propto p(N|\lambda_0, z, b, \beta, \epsilon) p(z|A, \pi) \\
    \times p(\lambda_0) p(b) p(\beta) p(A) p(\pi) p(\epsilon) p(v^2_b) p(v^2_\beta) p(v^2_\eta) p(v^2_\epsilon)
\end{multline}
with a structured mean field form that factorizes over units and chains:
\begin{multline}
    q(\lambda_0, b, \beta, A, \pi, z, \epsilon) = \prod_m q(\epsilon_m) \prod_{ku} q(\lambda_{0u})
    q(b_{ku}) q(\beta_{ku}) q(z_k) q(A_k) q(\pi_k) \\
    \times q(v^2_{bk}) q(v^2_{\beta k}) q(v^2_{\eta u})
\end{multline}
For this, we will use the posterior variational ansatz
\begin{align}
    \lambda_{0u} &\sim \mathcal{N}\left(\mu_{0u}, \sigma^2_{0u}\right) \\
    b_{ku} &\sim \mathcal{N}\left((\mu_b)_{ku}, (\sigma^2_b)_{ku}\right) \\
    \beta_{ku} &\sim \mathcal{N}\left((\mu_\beta)_{ku}, (\sigma^2_\beta)_{ku}\right) \\
    \epsilon_m &\sim \mathcal{N}\left(\mu_{\epsilon m}, \sigma^2_{\epsilon m}\right) \\
    (v^2_b)_{k} &\sim \text{Inv-Ga}\left((\varsigma_b)_{k}, (\rho_b)_{k} \right) \\
    (v^2_\beta)_{r} &\sim \text{Inv-Ga}\left((\varsigma_\beta)_r, (\rho_\beta)_r \right) \\
    (v^2_\epsilon)_{u} &\sim \text{Inv-Ga}\left((\varsigma_\epsilon)_u, (\rho_\epsilon)_u \right) \\
    \pi_k &\sim \mathrm{Beta}\left((\gamma_\pi)_k, (\delta_\pi)_k \right) \\
    \left(A_k \right)_{1i} &\sim \mathrm{Beta}\left((\gamma_A)_{ki}, (\delta_A)_{ki} \right)
\end{align}

\section{HMM inference}
Given the parameters of our observation model $\theta = (\lambda_0, b, \beta, A, \pi)$, the well-known Forward-Backward Algorithm returns the following posteriors
\begin{align}
    \xi_t \equiv p(z_t|N, \theta) &\qquad \text{posterior marginals} \\
    \Xi_{t, ij} \equiv p(z_{t+1} = j, z_t = i|N, \theta) &\qquad \text{two-slice marginals} \\
    \log Z_t = \log p(N_{t+1 \bullet}|N_{t\bullet}, \theta) &\qquad \text{partition function}
\end{align}
The first two allow us to calculate expressions with respect to $q(z)$, while the last gives the normalization for the joint posterior over all $z$: $\log Z = \sum_t \log Z_t = \log p(N_{1:T}|\theta)$

\section{Evidence Lower Bound (ELBo)}
We would like to maximize a lower bound on the log evidence given by
\begin{equation}
    \mathcal{L} = \mathbb{E}_q \left[ \log \frac{p}{q} \right]
\end{equation}
Thanks to factorization in the priors and posterior ansatz, this can easily be broken down in pieces, one per variable type:

\subsection{$\lambda_0$}
We want
\begin{multline}
    \mathbb{E}_{q(\lambda_0)}\left[ \log \frac{p(\lambda_0)}{q(\lambda_0)}\right] = \sum_u \mathbb{E}_q\left[ -\frac{1}{2v_0^2} (\lambda_{0u} - m_0)^2
    - \frac{1}{2}\log 2\pi v_0^2
    + \frac{1}{2\sigma_{0u}^2} (\lambda_{0u} - \mu_{0u})^2
    + \frac{1}{2}\log 2\pi \sigma^2_{0u}
    \right] \\
    = \sum_u \left[
    -\frac{1}{2v_0^2} \left(\sigma^2_{0u} + (\mu_{0u} - m_0)^2 \right)
    + \log \frac{\sigma_{0u}}{v_0} + \frac{1}{2}
    \right]
\end{multline}
where we have used
\begin{equation}
    \mathbb{E}[X^2] = \mathrm{var}[X] + \mathbb{E}[X]^2
\end{equation}

\subsection{$b$}
As with the $\lambda$ case, we have
\begin{multline}
    \mathbb{E}_{q(b)}\left[ \log \frac{p(b)}{q(b)}\right] = \sum_{ku} \mathbb{E}_q\left[ -\frac{1}{2v_{bk}^2} b_{ku}^2
    - \frac{1}{2}\log 2\pi v_{bk}^2
    + \frac{1}{2\sigma_{bku}^2} (b_{ku} - \mu_{bku})^2
    + \frac{1}{2}\log 2\pi \sigma^2_{bku}
    \right] \\
    = \sum_{ku} \left[
    -\frac{1}{2}\frac{\varsigma_{bk}}{\rho_{bk}}\left(\sigma^2_{bku} + \mu^2_{bku} \right)
    + \frac{1}{2}(\psi(\varsigma_{bk}) - \log \rho_{bk}) - \log \sqrt{2\pi} + \frac{1}{2} \log 2\pi e \sigma^2_{bku}
    \right]
\end{multline}
Where we have used $v^2 \sim \text{Inv-Ga}(\varsigma, \rho)$ to define $\tau = v^{-2} \sim \mathrm{Ga}(\varsigma, \rho)$ and
\begin{align}
    \mathbb{E}[\tau] &= \frac{\varsigma}{\rho} \\
    \mathbb{E}[\log \tau] &= \psi(\varsigma) - \log \rho
\end{align}
with $\psi(x)$ the digamma function.

\subsection{$v^2_b$}
Here again, we will define $\tau = v^{-2} \sim \mathrm{Ga}(\varsigma, \rho)$ so that we can write
\begin{multline}
    \mathbb{E}_{q(\tau)}\left[\log \frac{p(\tau)}{q(\tau)}\right] =
    \sum_k \mathbb{E}_q \left[
    (s_{b} - 1) \log \tau_{bk} - r_{b} \tau_{bk} \right]
    + H_g(\varsigma_{bk}, \rho_{bk})
    + \mathrm{const} \\
    = \sum_k \left[
    (s_{b} - 1) (\psi(\varsigma_{bk}) - \log \rho_{bk})
    - r_{b} \frac{\varsigma_{bk}}{\rho_{bk}} + H_g(\varsigma_{bk}, \rho_{bk})
    \right]
\end{multline}
where we have discarded constants that do not depend on the variational parameters $\varsigma$ and $\rho$ and $H_g$ is the differential entropy of the gamma distribution:
\begin{equation}
    H_g(a, b) = a - \log b + \log \Gamma(a) + (1 - a)\psi(a)
\end{equation}

\subsection{$\beta$, $\epsilon$}
Formulas in this case are the same as those given above for $b$, with only trivial substitutions required.

\subsection{$\pi$}
First, we have
\begin{equation}
    \mathbb{E}_{q(\pi)} \left[\log \frac{p(\pi)}{q(\pi)} \right] = \sum_k \left[(a_{\pi k} - 1)\overline{\log \pi_{k1}} + (b_{\pi k} - 1) \overline{\log \pi_{k0}} - \log B(a_{\pi k}, b_{\pi k}) + H(\pi_k) \right]
\end{equation}
with
\begin{align}
    \overline{\log \pi_{k1}} &= \psi(\gamma_{\pi k}) - \psi(\gamma_{\pi k} + \delta_{\pi k}) \\
    \overline{\log \pi_{k0}} &= \psi(\delta_{\pi k}) - \psi(\gamma_{\pi k} + \delta_{\pi k})
\end{align}
with $\psi$ the digamma function and
\begin{equation}
    H(\pi_k) = H_b(\gamma_{\pi k}, \delta_{\pi k})
\end{equation}
with $H_b$ the entropy of the beta distribution:
\begin{equation}
    H_b(\alpha, \beta) = \log B(\alpha, \beta) - (\alpha - 1) \psi(\alpha) - (\beta - 1) \psi(\beta) + (\alpha + \beta - 2)\psi(\alpha + \beta)
\end{equation}

\subsection{$A$}
Similarly,
\begin{equation}
    \mathbb{E}_{q(A)} \left[\log \frac{p(A)}{q(A)} \right] =
\sum_{ik} \left[ (a_{Aki} - 1) \overline{\log A_{k1i}} + (b_{Aki} - 1) \overline{\log A_{k0i}} - \log B(a_{Aki}, b_{Aki}) + H(A_{k1i}) \right]
\end{equation}
where
\begin{align}
    \overline{\log A_{k1i}} &= \psi(\gamma_{Aki}) - \psi(\gamma_{Aki} + \delta_{Aki}) \\
    \overline{\log A_{k0i}} &= \psi(\delta_{Aki}) - \psi(\gamma_{Aki} + \delta_{Aki})
\end{align}
and again
\begin{equation}
    H(A_{k1i}) = H_b(\gamma_{Aki}, \delta_{Aki})
\end{equation}

\subsection{Observation model}
Finally, we would like to calculate the piece of the evidence lower bound arising from the Poisson observation model:
\begin{multline}
    \label{obsmodel}
    \mathbb{E}_q \left[ \log \frac{p(N, z|\theta)}{q(z)} \right] =
    \sum_{m} \left[
    N_{m} \mathbb{E}_q [\eta_m]
    - \mathbb{E}_q[e^{\eta_m}] \right]
    + \sum_{kt} \left[\mathrm{tr}\left(\Xi_{kt} \overline{\log A_k^T}\right) + \xi_{0k}^T \overline{\log \pi_k} \right] \\
    - \sum_{kt} \left[ \xi_{tk}^T \nu_{tk} + \mathrm{tr}\left(\Xi_{kt} \tilde{A}_k^T\right) + \xi_{0k}^T \tilde{\pi}_k - \log Z_{kt} \right]
\end{multline}
with $\xi$ and $\Xi$ defined as the expected value of $z$ and the two-slice marginals, as above, and $(\nu, \tilde{\pi}, \tilde{A})$ the variational parameters in the posterior $q(z)$. Roughly, the three terms above correspond to $\log p(N|z, \theta)$, $\log p(z|A, \pi)$, and $\log q(z)$.

The first two terms involving expectations of $\lambda_{mu}$ can be calculated from (\ref{loglambda}) above and the variational ansatz:
\begin{align}
    \mathbb{E}_q [\eta_m] &= \overline{\eta_m} =
    \mu_{0u(m)} + \sum_{k} \xi_{t(m) k} \mu_{bku(m)} + \sum_r x_{mr} \mu_{\beta r u(m)} + \mu_{\epsilon m} \\
    \mathbb{E}_q [e^{\eta_m}] &= \overline{e^{\eta_m}} = \exp \left(
    \overline{\eta_{m \setminus b}}
    + \frac{1}{2} \left[ \sigma^2_{0u(m)}
    + \sum_r x^2_{m r} \sigma^2_{\beta ru(m)} + \sigma^2_{\epsilon u(m)}
    \right]
    \right) \\ \nonumber
    &\times \left[
    \prod_k \left(
    1 - \xi_{t(m) k} + \xi_{t(m) k} e^{\mu_{bku(m)} + \sigma^2_{bku(m)} / 2}
    \right)\right]
\end{align}
with $\overline{\eta_{m \setminus b}}$ the same as $\overline{\eta_m}$ with $zb$ terms removed. We have also used the expression for the moment generating function of a normally distributed variable:
\begin{equation}
    \mathbb{E}[e^{tX}] = e^{t\mu + \frac{1}{2} t^2\sigma^2}
\end{equation}
and the fact that $z^2 = z$ since the latents are assumed to be binary.


\section{Variational Updates}

\subsection{$z$}
Technically, $\xi$ is not a variational parameter, but depends on the variational parameters in $q(z)$: $\xi = \xi(\nu, \tilde{A}, \tilde{\pi})$, and similarly for $\Xi$. For the actual variational parameters, the updates are straightforward:
\begin{align}
    \nu_{tk} &\leftarrow \sum_{t(m) = t}
    \begin{pmatrix}
        - F_{mk} \\
        N_{m} \mu_{bku(m)} -
        e^{\mu_{bku(m)} + \sigma^2_{bku(m)} / 2} F_{mk}
    \end{pmatrix} \\
    \tilde{A}_{k} &\leftarrow \overline{\log A_k} \\
    \tilde{\pi}_k &\leftarrow \overline{\log \pi_k}
\end{align}
Here, we sum over all observations at stimulus time $t$, and $\eta$ the local evidence for $z$ is calculated by setting the relevant $z$ to 0 or 1. For simplicity, we also define
\begin{equation}
    F_{mk} = \frac{\overline{e^{\eta_m}}}{1 - \xi_{t(m) k} + \xi_{t(m) k} e^{\mu_{bku(m)} + \sigma^2_{bku(m)} / 2}}
\end{equation}

We note, after Beal, that as a result of these updates, $\tilde{A}$ and $\tilde{\pi}$ are subadditive (i.e., they do not sum to 1), but that the forward-backward algorithm nonetheless returns a correctly normalized posterior.

\subsection{$\pi$, $A$}
Given conjugacy, these are trivial to write down:
\begin{align}
    \gamma_{\pi k} &\leftarrow a_{\pi k} + \xi_{0k} \\
    \delta_{\pi k} &\leftarrow b_{\pi k} + 1 - \xi_{0k} \\
    \gamma_{A ki} &\leftarrow a_{A ki} + \sum_t \Xi_{kt, 1i} \\
    \delta_{A ki} &\leftarrow b_{A ki} + \sum_t \Xi_{kt, 0i}
\end{align}

\subsection{$v^2$}
Here again, conjugacy makes the updates trivial for the $v^2$ terms. For both $b$ and $\beta$
\begin{align}
    \varsigma_k &\leftarrow s + \frac{U}{2} \\
    \rho_k &\leftarrow r + \frac{1}{2}\sum_u (\sigma^2_{ku} + \mu^2_{ku}) \\
\end{align}
while for $\epsilon$
\begin{align}
    \varsigma_u &\leftarrow s + \frac{M_u}{2} \\
    \rho_u &\leftarrow r + \frac{1}{2} \sum_m (\sigma^2_{mu} + \mu^2_{mu})
\end{align}

\subsection{$\lambda_0$ $b$, $\beta$, $\epsilon$}
Here, the nonlinearity in $\overline{e^{\eta_m}}$ makes updates less straightforward. In each case, we have something like
\begin{equation}
    \frac{\partial\mathcal{L}}{\partial \mu} = \frac{\partial\mathcal{E}}{\partial \mu} + \sum_{m} \left[N_{m} \frac{\partial\overline{\eta_{m}}}{\partial \mu} - \frac{\partial\overline{e^{\eta_{m}}}}{\partial \mu}\right] = 0
\end{equation}
which works out in the various cases as
\begin{align}
    \frac{\partial\mathcal{L}}{\partial \mu_{0u}} &= -(\mu_{0u} - m_0)\tau_0 + \sum_{u(m) = u} [N_{m} - \overline{e^{\eta_{m}}}] \\
    \frac{\partial\mathcal{L}}{\partial \mu_{bku}} &=
    - \mu_{bku} \overline{\tau_{bk}} + \sum_{u(m) = u} [N_{m} - e^{\mu_{bku(m)} + \sigma^2_{bku(m)} / 2} F_{mk} ] \xi_{t(m) k} \\
    \frac{\partial\mathcal{L}}{\partial \mu_{\beta ru}} &=
    - \mu_{\beta ru} \overline{\tau_{\beta r}} + \sum_{u(m) = u} [N_{m} - \overline{e^{\eta_{m}}}] x_{mr}  \\
    \frac{\partial\mathcal{L}}{\partial \mu_{\epsilon m}} &=
    - \mu_{\epsilon m} \overline{\tau_{\epsilon u(m)}} + N_{m} -
    \overline{e^{\eta_m}}
\end{align}
Unfortunately, $\overline{e^{\eta_m}}$ depends on the $\mu$ variables in question, requiring us to solve a transcendental equation in each case.

The case of variation with respect to $\sigma^2$ is similar:
\begin{align}
    \frac{\partial\mathcal{L}}{\partial \sigma^2_{0u}} &= -\frac{1}{2}\tau_0 + \frac{1}{2\sigma^2_{0u}} - \frac{1}{2} \sum_{u(m) = u} \overline{e^{\eta_m}} \\
    \frac{\partial\mathcal{L}}{\partial \sigma^2_{bku}} &=
    - \frac{1}{2} \overline{\tau_{bk}} + \frac{1}{2\sigma^2_{bku}} - \frac{1}{2}\sum_{u(m) = u} \xi_{t(m)k} e^{\mu_{bku(m)} + \sigma^2_{bku(m)} / 2} F_{mk}  \\
    \frac{\partial\mathcal{L}}{\partial \sigma^2_{\beta ru}} &=
    - \frac{1}{2} \overline{\tau_{\beta r}} + \frac{1}{2\sigma^2_{\beta ru}} - \frac{1}{2} \sum_{u(m) = u} x^2_{mr} \overline{e^{\eta_m}} \\
    \frac{\partial\mathcal{L}}{\partial \sigma^2_{\epsilon m}} &=
    -\frac{1}{2} \overline{\tau_{\epsilon u(m)}} + \frac{1}{2\sigma^2_{\epsilon m}} - \frac{1}{2} \overline{e^{\eta_m}}
\end{align}
where $\overline{\tau} = \mathbb{E}[1/v^2] = \varsigma / \rho$.

Note, however, that for unconstrained minimization, we need to shift parameterization to $\sigma^2 = e^\kappa$, with the result that

\begin{align}
    \frac{\partial\mathcal{L}}{\partial \kappa_{0u}} &= -\frac{1}{2}\tau_0 e^{\kappa_{0u}} + \frac{1}{2} - \frac{1}{2} \sum_{u(m) = u} e^{\kappa_{0u}} \overline{e^{\eta_m}} \\
    \frac{\partial\mathcal{L}}{\partial \kappa_{bku}} &=
    - \frac{1}{2} \overline{\tau_{bk}} e^{\kappa_{bku}} + \frac{1}{2} - \frac{1}{2}\sum_{u(m) = u} \xi_{t(m)k} e^{\mu_{bku(m)} + \frac{1}{2} e^{\kappa_{bku}} + \kappa_{bku}} F_{mk}  \\
    \frac{\partial\mathcal{L}}{\partial \kappa_{\beta ru}} &=
    - \frac{1}{2} \overline{\tau_{\beta r}} e^{\kappa_{\beta bru}} +
    \frac{1}{2} - \frac{1}{2} \sum_{u(m) = u} x^2_{mr} e^{\kappa_{\beta ru}} \overline{e^{\eta_m}} \\
    \frac{\partial\mathcal{L}}{\partial \kappa_{\epsilon m}} &=
    -\frac{1}{2} \overline{\tau_{\epsilon u(m)}} e^{\kappa_{\epsilon m}} + \frac{1}{2} - \frac{1}{2} e^{\kappa_{\epsilon m}} \overline{e^{\eta_m}}
\end{align}

\subsubsection{Hessians}
Because the structure of the observation model is a sum over (independent)observations and each observation corresponds to a particular $(t, u)$ the Hessian matrix takes on a particularly simple form within each sub-block:
\begin{align}
    \frac{\partial^2\mathcal{L}}{\partial \mu_{0u}\partial \mu_{0u'}} &=
    -\left[\tau_0 + \sum_{u(m)=u} \overline{e^{\eta_m}}\right]\delta_{uu'} \\
    \frac{\partial^2\mathcal{L}}{\partial \mu_{bku}\partial \mu_{bk'u'}} &=
    -\left[\overline{\tau_{bk}} + \xi_{t(m) k}\sum_{u(m) = u} e^{\mu_{bku(m)} + \sigma^2_{bku(m)} / 2} F_{mk} \right]\delta_{uu'} \delta_{k k'} \\
    \frac{\partial^2\mathcal{L}}{\partial \mu_{\beta ru}\partial \mu_{\beta r'u'}} &=
    -\left[\overline{\tau_{\beta r}} + \sum_{u(m) = u} x^2_{mr} \overline{e^{\eta_m}} \right]\delta_{uu'} \delta_{r r'} \\
\end{align}
to be continued...

\section{Bottleneck model}
Consider a log-linear Poisson model with a matrix of count observations of $U$ units at $T$ times.
\begin{equation}
    N_{tu} \sim \mathrm{Poisson}(e^{\eta_{tu}})
\end{equation}
Furthermore, assume that the variable $\eta$ is Gaussian:
\begin{equation}
    \eta_{t\cdot} \sim \mathcal{N}\left(a_{\cdot} + \sum_{r=1}^R x_{tr} b_{r\cdot} +
    \sum_{k=1}^K z_{tk} c_{k\cdot}, \Sigma_\varepsilon\right)
\end{equation}
where $z \in \{0, 1\}$ and the variables $x$ are known regressors. We can then write the observation model as
\begin{equation}
    p(N) = p(N|\eta) p(\eta|a, b, c, z, \Sigma_\varepsilon) p(a) p(b) p(c) p(z) p(\Sigma_\varepsilon)
\end{equation}
We will assume the following priors for the parameters:
\begin{align}
    a_u &\sim \mathcal{N}(m_a, s_a^2) \\
    b_{\cdot u} &\sim \mathcal{N}(m_b, S_b) \\
    c_{\cdot u} &\sim \mathcal{N}(m_c, S_c) \\
    z_{\cdot k} &\sim \mathrm{HMM}(A_k, \pi_k) \\
    \Sigma_\varepsilon &\sim \text{Inv-Wishart}
\end{align}
That is, the coefficients $a$, $b$, and $c$ for each unit are drawn from population normal distributions (perhaps with nontrivial covariance), while the hidden binary variables $z$ are drawn from separate hidden Markov models.

Now, the variational posterior $q$ can be chosen as a product of the following:
\begin{align}
    \eta_{t\cdot} &\sim \mathcal{N}(\mu^t_\eta, \Sigma^t_\eta) \\
    a_u &\sim \mathcal{N}(\mu^u_a, (\sigma^u_a)^2) \\
    b_{\cdot u} &\sim \mathcal{N}(\mu^u_b, \Sigma^u_b) \\
    c_{\cdot u} &\sim \mathcal{N}(\mu^u_c, \Sigma^u_c) \\
    z_{\cdot k} &\sim \mathrm{HMM}(\tilde{A}_k, \tilde{\pi}_k) \\
    \Sigma_\varepsilon &\sim \text{Inv-Wishart}
\end{align}

Some notes:
\begin{itemize}
    \item Because of the linear-Gaussian form of the model for $\eta$, the Gaussian variational posteriors of $a$, $b$, and $c$ can make use of conjugacy for rapid, exact coordinate ascent.
    \item For the priors, we specified a single population prior for each unit, but for the posteriors, we clearly need a posterior over \emph{each} unit. The same holds for the posteriors for $\eta$ at distinct times.\footnote{In this case, we might want a posterior that incorporates autocorrelation in time. File under future work.}
    \item The HMM inference can be performed exactly by the forward-backward algorithm. Our only approximation is to assume that the posterior $q(z)$ takes the form of a product over \emph{independent} chains. This is only approximate because these chains are coupled through the observations (and intermediately through $\eta$) in the true posterior.
    \item The only non-conjugacy in the model is in $\eta$, since we assume a Gaussian posterior and a Poisson observation model for the counts.
\end{itemize}

\subsection{Inference on log firing rate}
If we write the evidence lower bound as
\begin{equation}
    \mathcal{L} = \mathbb{E}_q \left[ \frac{\log p}{\log q}\right] =
    \mathbb{E}_q[\log p] + \mathcal{H}[q]
\end{equation}
Then the only non-conjugate terms, as noted above, are related to $\eta$\footnote{Recall that $p(N|\lambda)p(\lambda) d\lambda = p(N|e^\eta)\lvert\frac{d\lambda}{d\eta}\rvert p(\eta) d\eta$, so that we need an extra factor $e^\eta$ implying $N \rightarrow N + 1$ in the sufficient statistics. This is equivalent, of course, to noting that $\lambda = e^\eta$ should be log-normally distributed, in which case the extra factor is just the extra measure factor in the log-normal.}:
\begin{align}
    \mathcal{L}_\eta &=
    \sum_{tu} \mathbb{E}_q[(N_{tu} + 1) \eta_{tu} - e^{\eta_{tu}}]
    - \frac{1}{2} \sum_{tu} \mathbb{E}_q \left[
    (\eta - \overline{\eta})^\top \Sigma_\varepsilon^{-1}(\eta - \overline{\eta}) \right]
    - \frac{1}{2} \log |\Sigma_\eta| \\
    &= \sum_{tu} \left[ (N_{tu} + 1) \mu_{\eta tu} -
    \exp\left(\mu_{\eta tu} + \frac{1}{2} \Sigma_{\eta tuu} \right)\right] \\
    &- \frac{1}{2} \sum_{t} \mathrm{Tr}\left(
    \mathbb{E}_q[\Sigma_\varepsilon^{-1}]
    \left[ \Sigma_{\eta t} + (\mu_{\eta t} - \mathbb{E}_q[\overline{\eta}_{t}])
    (\mu_{\eta t} - \mathbb{E}_q[\overline{\eta}_{t}])^\top
    + \mathrm{Cov}[\overline{\eta}]_{t}\right] \right) \\
    &- \frac{1}{2} \log |\Sigma_\eta| + const
\end{align}
with $\overline{\eta} \equiv a + bx + cz$ and
\begin{align}
    \mathrm{Cov}[\overline{\eta}] &= \mathrm{Cov}[a] + \mathrm{Cov}[bx] + \mathrm{Cov}[cz] \\
    \mathrm{Cov}[a]_{tuu'} &= \delta_{uu'}(\sigma^a_u)^2 \\
    \mathrm{Cov}[bx]_{tuu'} &= \mathrm{Tr}\left((x_t\cdot x_t^\top) \Sigma^b_{uu'}\right) \\
    \mathrm{Cov}[cz]_{tuu'} &= \mathbb{E}_q\left[\sum_{k}\left( c_{ku} z_{tk} - \mu^c_{ku}\xi_{tk} \right)\sum_{k'}\left( c_{k'u'} z_{tk'} - \mu^c_{k'u'}\xi_{tk'} \right)\right] \\
    &= \sum_{kk'}\left[\left(\delta_{uu'}\Sigma^c_{kk'} + \mu^c_{ku} \mu^c_{k'u'} \right)\left(\delta_{kk'}\xi_{tk}(1 - \xi_{tk}) + \xi_{tk}\xi_{tk'}\right) - \mu^c_{ku}\mu^c_{k'u'}\xi_{tk}\xi_{tk'} \right] \\
    &= \delta_{uu'}\left(\sum_k \Sigma^c_{kk} \xi_{tk} (1 - \xi_{tk}) + \xi_t^\top \Sigma^c \xi_t\right) + \sum_k \mu^c_{ku}\mu^c_{ku'}\xi_{tk}(1 - \xi_{tk})\\
\end{align}

We can then ask what happens when we look for local minima of this objective with respect to $\mu_\eta$ and $\Sigma_\eta$:
\begin{align}
    \frac{\partial \mathcal{L}}{\partial \mu_{tu}} &=
    N_{tu} + 1 - \exp\left(\mu_{tu} + \frac{1}{2} \Sigma_{\eta tuu} \right)
    - \mathbb{E}_q[\Sigma_\varepsilon^{-1}](\mu_{tu} - \mathbb{E}_q[\overline{\eta}_{tu}]) \\
    \frac{\partial \mathcal{L}}{\partial \Sigma_{\eta tu \neq v}} &=
    \frac{1}{2} \left(\Sigma^{-1}_\eta \right)_{tuv}
    -\frac{1}{2}\mathbb{E}_q[\Sigma_\varepsilon^{-1}]_{uv}\\
    \frac{\partial \mathcal{L}}{\partial \Sigma_{\eta tuu}} &=
    -\frac{1}{2} \exp\left(\mu_{tu} + \frac{1}{2} \Sigma_{\eta tuu} \right)
    + \frac{1}{2} \left(\Sigma^{-1}_\eta \right)_{tuu}
    -\frac{1}{2}\mathbb{E}_q[\Sigma_\varepsilon^{-1}]_{uu} \\
\end{align}
and asking $\nabla \mathcal{L} = 0$ then immediately gives $\Sigma^{-1}_{\eta tuv} = \mathbb{E}_q[\Sigma_\varepsilon^{-1}]$ for $u\neq v$ --- that is, the precision matrix $\Sigma^{-1}_{\eta t}$ is equal to the expected value of the noise matrix off-diagonal.

\subsubsection{DEPRECATED: Solving for $\mu$}
The above transcendental equation for $\mu$ can be solved by using the Lambert W function, which satisfiex $z = W(z)e^{W(z)}$. That is, $y = W(z)$ is the solution to $ye^y = z$. Moreover, the equation
\begin{equation}
    p^{ax + b} = cx + d
\end{equation}
has the solution
\begin{equation}
    x = \frac{-W\left(-\frac{a \log p}{c}
    p^{b - \frac{ad}{c}} \right)}{a \log p} - \frac{d}{c}
\end{equation}
Thus, we can substitute
\begin{align}
    p &= e \\
    a &= 1 \\
    b &= \frac{1}{2} \sigma^2 \\
    c &= -\tau \\
    d &= N + 1 + \tau \mathbb{E}_q[\overline{\eta}]
\end{align}
into the above to yield
\begin{equation}
    \mu = N\tau^{-1} + \mathbb{E}_q[\overline{\eta}] -
    W\left(\tau^{-1} \exp \left(
    \frac{1}{2}\sigma^2 + N\tau^{-1} + \mathbb{E}_q[\overline{\eta}]
    \right)\right)
\end{equation}
Moreover, we note that when $x \gg 1$, $W(x) \approx \log x - \log \log x + o(1)$.

\subsubsection{DEPRECATED: Solving for $\sigma^2$}
For $\sigma^2$, we have the condition
\begin{equation}
    \sigma^2 = \frac{1}{\tau + \exp\left(\mu + \frac{1}{2}\sigma^2 \right)}
\end{equation}
Solving this is similar to solving the fixed point equation $x = f(x)$ with $f$ the ($x$-reversed) logistic function. This equation can potentially solved by iteration, but one must be careful of the constants involved (so the approximation converges).

\end{document}
