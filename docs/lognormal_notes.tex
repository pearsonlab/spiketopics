\documentclass[11pt]{article}
\usepackage{amssymb,amsmath}

\begin{document}
\title{The Log-Normal Model}
\author{John Pearson}
\maketitle

\section{Observation Model}
Assume an experiment with $M$ observations (one per unit, time combination). Each unit is responsive to $R$ regressors and $K$ latent states. Stimuli are indexed by a time variable with $T$ unique values. Spike counts are governed by a Poisson process given by
\begin{equation}
    N_{m} \sim \mathrm{Poiss}(e^{\eta_{m}}) 
\end{equation}
where $N_{m}$ is the count from observation $m$ and the effective rate $\eta_{m}$ is given by 
\begin{equation}
    \label{loglambda}
    \eta_m = \lambda_{0u(m)} + \sum_k z_{t(m)k} b_{ku(m)} + \sum_r x_{mr} \beta_{ru(m)} + \epsilon_{m}
\end{equation}
Here, the coefficients $b_{ku}$ index responses of each unit to the (binary) latent states $z_{mk}$ while the $\beta_{ru}$ index responses to (non-latent) regressors $x_{mr}$. The final ($\epsilon$) term represents moment-to-moment fluctuations of each unit. (This is of course equivalent to $\eta \sim \mathcal{N}(\overline{\eta}, v^2_\eta)$.) We have also assumed functions $u(m)$ and $t(m)$ mapping each observation to its appropriate unit and time, respectively.

\section{Latent feature model}
We will assume a Hidden Markov Model (HMM) for the latent states $z$. We will likewise assume that these latent features are present in the stimulus, so that they are indexed by the stimulus time $t(m)$. When the meaning is clear, we will drop the dependency of stimulus time on observation number and simply write $t$.

To describe the transition model for the HMM, we take
\begin{align}
    A_{ij} &\equiv p(z_{t+1} = i|z_t = j) \\
    \pi_i &\equiv p(z_0 = i)
\end{align}
(Note that this means that the \emph{columns} of $A$ sum to 1, which is the opposite of the usual convention. In other words, in matrix notation, $z_{t+1} = A \cdot z_t$.)

Given this notation, it is straightforward to write the probability of a sequence of hidden states, conditioned on the chain parameters
\begin{equation}
    \log p(z|A, \pi) = \sum_t \log A_{z_{t+1} z_t} + \log \pi_{z_0}
\end{equation}

\section{Priors}
In addition to the model (\ref{loglambda}) above and the HMM ansatz, we posit the following hierarchical generative model for the parameters:
\begin{align}
    \eta_m &\sim \mathcal{N}(\overline{\eta}_{u(m)}, v^2_{\eta u(m)}) \\
    \lambda_{0u} &\sim \mathcal{N}(m_0, v^2_0) \\
    b_{ku} &\sim \mathcal{N}\left(0, (v^2_b)_{k}\right) \\
    \beta_{ku} &\sim \mathcal{N}\left(0, (v^2_\beta)_{k}\right) \\
    (v^2_b)_{k} &\sim \text{Inv-Ga}\left(s_b, r_b \right) \\
    (v^2_\beta)_{r} &\sim \text{Inv-Ga}\left(s_\beta, r_\beta \right) \\
    (v^2_\eta)_{u} &\sim \text{Inv-Ga}\left(s_\eta, r_\eta \right) \\
    \pi_k &\sim \mathrm{Beta}\left((a_\pi)_k, (b_\pi)_k \right) \\ 
    \left(A_k \right)_{1i} &\sim \mathrm{Beta}\left((a_A)_{ki}, (b_A)_{ki} \right)  
    \end{align}
where $A_{1i}$ is the probability of the transition $i \rightarrow 1$ (i.e., the transition \emph{into} the $z = 1$ state.)

Note also that, by putting sparse priors on the $v^2$ terms, we can implement automatic relevance determination (ARD) on the regression coefficients.

\section{Variational ansatz}
We would like to approximate the joint posterior density 
\begin{multline}
    p(\eta, \lambda_0, b, \beta, A, \pi, z|N) \propto p(N|\eta) p(\eta|\lambda_0, z, b, \beta) p(z|A, \pi) \\
    \times p(\lambda_0) p(b) p(\beta) p(A) p(\pi) p(v^2_b) p(v^2_\beta) p(v^2_\eta)
\end{multline}
with a structured mean field form that factorizes over units and chains:
\begin{multline}
    q(\eta, \lambda_0, b, \beta, A, \pi, z) = \prod_m q(\eta_m) \prod_{ku} q(\lambda_{0u}) 
    q(b_{ku}) q(\beta_{ku}) q(z_k) q(A_k) q(\pi_k) \\
    \times q(v^2_{bk}) q(v^2_{\beta k}) q(v^2_{\eta u})
\end{multline}
For this, we will use the posterior variational ansatz
\begin{align}
    \eta_m &\sim \mathcal{N}\left(\mu_{\eta m}, \sigma^2_{\eta m}\right) \\
    \lambda_{0u} &\sim \mathcal{N}\left(\mu_{0u}, \sigma^2_{0u}\right) \\
    b_{ku} &\sim \mathcal{N}\left((\mu_b)_{ku}, (\sigma^2_b)_{ku}\right) \\
    \beta_{ku} &\sim \mathcal{N}\left((\mu_\beta)_{ku}, (\sigma^2_\beta)_{ku}\right) \\
    (v^2_b)_{k} &\sim \text{Inv-Ga}\left((\varsigma_b)_{k}, (\rho_b)_{k} \right) \\
    (v^2_\beta)_{r} &\sim \text{Inv-Ga}\left((\varsigma_\beta)_r, (\rho_\beta)_r \right) \\
    (v^2_\eta)_{u} &\sim \text{Inv-Ga}\left((\varsigma_\eta)_u, (\rho_\eta)_u \right) \\
    \pi_k &\sim \mathrm{Beta}\left((\gamma_\pi)_k, (\delta_\pi)_k \right) \\ 
    \left(A_k \right)_{1i} &\sim \mathrm{Beta}\left((\gamma_A)_{ki}, (\delta_A)_{ki} \right)  
\end{align}

\section{HMM inference}
Given the parameters of our observation model $\theta = (\eta, b, \beta, A, \pi)$, the well-known Forward-Backward Algorithm returns the following posteriors
\begin{align}
    \xi_t \equiv p(z_t|N, \theta) &\qquad \text{posterior marginals} \\
    \Xi_{t, ij} \equiv p(z_{t+1} = j, z_t = i|N, \theta) &\qquad \text{two-slice marginals} \\
    \log Z_t = \log p(N_{t+1 \bullet}|N_{t\bullet}, \theta) &\qquad \text{partition function}
\end{align}
The first two allow us to calculate expressions with respect to $q(z)$, while the last gives the normalization for the joint posterior over all $z$: $\log Z = \sum_t \log Z_t = \log p(N_{1:T}|\theta)$

\section{Evidence Lower Bound (ELBo)}
We would like to maximize a lower bound on the log evidence given by
\begin{equation}
    \mathcal{L} = \mathbb{E}_q \left[ \log \frac{p}{q} \right]
\end{equation}
Thanks to factorization in the priors and posterior ansatz, this can easily be broken down in pieces, one per variable type:

\subsection{$\lambda_0$}
We want 
\begin{multline}
    \mathbb{E}_{q(\lambda_0)}\left[ \log \frac{p(\lambda_0)}{q(\lambda_0)}\right] = \sum_u \mathbb{E}_q\left[ -\frac{1}{2v_0^2} (\lambda_{0u} - m_0)^2
    - \frac{1}{2}\log 2\pi v_0^2
    + \frac{1}{2\sigma_{0u}^2} (\lambda_{0u} - \mu_{0u})^2
    + \frac{1}{2}\log 2\pi \sigma^2_{0u}
    \right] \\
    = \sum_u \left[
    -\frac{1}{2v_0^2} \left(\sigma^2_{0u} + (\mu_{0u} - m_0)^2 \right)
    + \log \frac{\sigma_{0u}}{v_0} + \frac{1}{2}
    \right] 
\end{multline}
where we have used 
\begin{equation}
    \mathbb{E}[X^2] = \mathrm{var}[X] + \mathbb{E}[X]^2
\end{equation}

\subsection{$b$}
As with the $\lambda$ case, we have 
\begin{multline}
    \mathbb{E}_{q(b)}\left[ \log \frac{p(b)}{q(b)}\right] = \sum_{ku} \mathbb{E}_q\left[ -\frac{1}{2v_{bk}^2} b_{ku}^2
    - \frac{1}{2}\log 2\pi v_{bk}^2
    + \frac{1}{2\sigma_{bku}^2} (b_{ku} - \mu_{bku})^2
    + \frac{1}{2}\log 2\pi \sigma^2_{bku} 
    \right] \\
    = \sum_{ku} \left[
    -\frac{1}{2}\frac{\varsigma_{bk}}{\rho_{bk}}\left(\sigma^2_{bku} + \mu^2_{bku} \right)
    + \frac{1}{2}(\psi(\varsigma_{bk}) - \log \rho_{bk}) - \log \sqrt{2\pi} + \frac{1}{2} \log 2\pi e \sigma^2_{bku}
    \right]
\end{multline}
Where we have used $v^2 \sim \text{Inv-Ga}(\varsigma, \rho)$ to define $\tau = v^{-2} \sim \mathrm{Ga}(\varsigma, \rho)$ and 
\begin{align}
    \mathbb{E}[\tau] &= \frac{\varsigma}{\rho} \\
    \mathbb{E}[\log \tau] &= \psi(\varsigma) - \log \rho
\end{align}
with $\psi(x)$ the digamma function.

\subsection{$\beta$}
Formulas in this case are the same as those given above for $b$, with only trivial substitutions required.

\subsection{$\eta$}
First, from (\ref{loglambda}), we know that $p(\eta|\lambda_0, b, z, \beta)$ is normal, such that
\begin{multline}
    \mathbb{E}_{q}\left[ \log \frac{p(\eta|\lambda_0, b, z, \beta)}{q(\eta)}\right] = 
    \sum_m \mathbb{E}_q \Bigg[
    -\frac{1}{2 v^2_{\eta u(m)}} 
        \left(
        \eta_m - \lambda_{0u} - \sum_k z_{t(m) k} b_{k u(m)} - 
        \sum_r x_{mr} \beta_{r u(m)} 
        \right)^2  \\
    -\frac{1}{2} \log 2\pi v^2_{\eta u(m)}
    + \frac{1}{2\sigma^2_{\eta m}} (\eta_m - \mu_{\eta m})^2 + 
    \frac{1}{2} \log 2\pi \sigma^2_{\eta m}
    \Bigg] \\
    = \sum_m \Bigg[ -\frac{1}{2}\frac{\varsigma_{\eta u(m)}}{\rho_{\eta u(m)}} 
    \Bigg(
        \sigma^2_{\eta m} + \sigma^2_{0 u(m)} + 
        \sum_k \left[ \xi_{t(m) k} \sigma^2_{b k u(m)} +
        \xi_{t(m) k} (1 - \xi_{t(m) k}) \mu^2_{bk u(m)}\right] + \\
        \sum_r x^2_{r u(m)} \sigma^2_{\beta r u(m)}
    +
    \Big(
    \mu_{\eta m} - \mu_{0 u(m)} - \sum_k \xi_{t(m) k} \mu_{b k u(m)} 
    - \sum_r x_{mr} \mu_{\beta r u(m)}
    \Big)^2 \Bigg)  \\
    + \frac{1}{2}(\psi(\varsigma_{\eta u(m)}) - \log \rho_{\eta u(m)}) - \log \sqrt{2\pi}
    + \frac{1}{2} \log 2\pi e \sigma^2_{\eta m}
    \Bigg]
\end{multline}

\subsection{$v^2_b$}
Here again, we will define $\tau = v^{-2} \sim \mathrm{Ga}(\varsigma, \rho)$ so that we can write
\begin{multline}
    \mathbb{E}_{q(\tau)}\left[\log \frac{p(\tau)}{q(\tau)}\right] =
    \sum_k \mathbb{E}_q \left[
    (s_{b} - 1) \log \tau_{bk} - r_{b} \tau_{bk} \right]  
    + H_g(\varsigma_{bk}, \rho_{bk})    
    + \mathrm{const} \\
    = \sum_k \left[ 
    (s_{b} - 1) (\psi(\varsigma_{bk}) - \log \rho_{bk}) 
    - r_{b} \frac{\varsigma_{bk}}{\rho_{bk}} + H_g(\varsigma_{bk}, \rho_{bk})
    \right]
\end{multline}
where we have discarded constants that do not depend on the variational parameters $\varsigma$ and $\rho$ and $H_g$ is the differential entropy of the gamma distribution:
\begin{equation}
    H_g(a, b) = a - \log b + \log \Gamma(a) + (1 - a)\psi(a)
\end{equation}

\subsection{$\pi$}
First, we have
\begin{equation}
    \mathbb{E}_{q(\pi)} \left[\log \frac{p(\pi)}{q(\pi)} \right] = \sum_k \left[(a_{\pi k} - 1)\overline{\log \pi_{k1}} + (b_{\pi k} - 1) \overline{\log \pi_{k0}} - \log B(a_{\pi k}, b_{\pi k}) + H(\pi_k) \right]
\end{equation}
with
\begin{align}
    \overline{\log \pi_{k1}} &= \psi(\gamma_{\pi k}) - \psi(\gamma_{\pi k} + \delta_{\pi k}) \\
    \overline{\log \pi_{k0}} &= \psi(\delta_{\pi k}) - \psi(\gamma_{\pi k} + \delta_{\pi k})
\end{align}
with $\psi$ the digamma function and 
\begin{equation}
    H(\pi_k) = H_b(\gamma_{\pi k}, \delta_{\pi k})
\end{equation}
with $H_b$ the entropy of the beta distribution:
\begin{equation}
    H_b(\alpha, \beta) = \log B(\alpha, \beta) - (\alpha - 1) \psi(\alpha) - (\beta - 1) \psi(\beta) + (\alpha + \beta - 2)\psi(\alpha + \beta)
\end{equation}

\subsection{$A$} 
Similarly,
\begin{equation}
    \mathbb{E}_{q(A)} \left[\log \frac{p(A)}{q(A)} \right] = 
\sum_{ik} \left[ (a_{Aki} - 1) \overline{\log A_{k1i}} + (b_{Aki} - 1) \overline{\log A_{k0i}} - \log B(a_{Aki}, b_{Aki}) + H(A_{k1i}) \right]
\end{equation}
where 
\begin{align}
    \overline{\log A_{k1i}} &= \psi(\gamma_{Aki}) - \psi(\gamma_{Aki} + \delta_{Aki}) \\
    \overline{\log A_{k0i}} &= \psi(\delta_{Aki}) - \psi(\gamma_{Aki} + \delta_{Aki})
\end{align}
and again
\begin{equation}
    H(A_{k1i}) = H_b(\gamma_{Aki}, \delta_{Aki})
\end{equation}

\subsection{$z$}
For the Hidden Markov Model, we would like to calculate the probability of the sequence of hidden states in the generative model and the posterior over hidden states:
\begin{multline}
    \label{obsmodel}
    \mathbb{E}_q \left[ \log \frac{p(z|A, \pi)}{q(z)} \right] =
    \sum_{kt}  \left[\mathrm{tr}\left(\Xi_{kt} \overline{\log A_k^T}\right) + \xi_{0k}^T \overline{\log \pi_k} \right] \\
    - \sum_{kt} \left[ \xi_{tk}^T \nu_{tk} + \mathrm{tr}\left(\Xi_{kt} \tilde{A}_k^T\right) + \xi_{0k}^T \tilde{\pi}_k - \log Z_{kt} \right]
\end{multline}
with $\xi$ and $\Xi$ defined as the expected value of $z$ and the two-slice marginals, as above, and $(\nu, \tilde{\pi}, \tilde{A})$ the variational parameters in the posterior $q(z)$. Roughly, the three terms above correspond to $\log p(z|A, \pi)$ and $\log q(z)$.

\subsection{Observation model}
This piece of the model is trivial:
\begin{multline}
    \mathbb{E}_q \left[ \log p(N|\eta) \right] = 
    \sum_m \mathbb{E}_q \left[ N_m \eta_m - e^{\eta_m} \right] = 
    \sum_m \left[N_m \mu_{\eta m} - e^{\mu_{\eta m} + \frac{1}{2} \sigma^2_{\eta m}} \right]
\end{multline}
where we have used the expression for the moment generating function of a normally distributed variable:
\begin{equation}
    \mathbb{E}[e^{tX}] = e^{t\mu + \frac{1}{2} t^2\sigma^2}
\end{equation}
and ignored constants $\log N_m!$.


\section{Variational Updates}

\subsection{$z$}
Technically, $\xi$ is not a variational parameter, but depends on the variational parameters in $q(z)$: $\xi = \xi(\nu, \tilde{A}, \tilde{\pi})$, and similarly for $\Xi$. For the actual variational parameters, the updates are straightforward:
\begin{align}
    \nu_{tk} &\leftarrow \sum_{t(m) = t} 
    \begin{pmatrix}
        0 \\
        \frac{\varsigma_{\eta u}}{\rho_{\eta u}} \left(
        \mu_{\eta m}\mu_{bku} - \frac{1}{2} \mu_{bku}^2 
        - \sigma^2_{bku} \right)
    \end{pmatrix} \\
    \tilde{A}_{k} &\leftarrow \overline{\log A_k} \\
    \tilde{\pi}_k &\leftarrow \overline{\log \pi_k}
\end{align}
Here, we sum over all observations $m$ corresponding to stimulus time $t$, implicitly using $u = u(m)$. To determine $\nu$, the local evidence for $z$, we set $z = 0$ or $z = 1$ in $p(\eta|z, \lambda_0, b, \beta)$ but also make use of the fact that this term need only be defined up to an overall constant (i.e., the probability need not be normalized) to remove terms not dependent on $z$. 

We also note, after Beal, that as a result of these updates, $\tilde{A}$ and $\tilde{\pi}$ are subadditive (i.e., they do not sum to 1), but that the forward-backward algorithm nonetheless returns a correctly normalized posterior.

\subsection{$\pi$, $A$}
Given conjugacy, these are trivial to write down:
\begin{align}
    \gamma_{\pi k} &\leftarrow a_{\pi k} + \xi_{0k} \\
    \delta_{\pi k} &\leftarrow b_{\pi k} + 1 - \xi_{0k} \\
    \gamma_{A ki} &\leftarrow a_{A ki} + \sum_t \Xi_{kt, 1i} \\
    \delta_{A ki} &\leftarrow b_{A ki} + \sum_t \Xi_{kt, 0i} 
\end{align}

\subsection{$v^2$}
Here again, conjugacy makes the updates trivial for the $v^2$ terms. For both $b$ and $\beta$
\begin{align}
    \varsigma_k &\leftarrow s + \frac{U}{2} \\
    \rho_k &\leftarrow r + \frac{1}{2}\sum_u (\sigma^2_{ku} + \mu^2_{ku}) \\
\end{align}
while for $\eta$
\begin{align}
    \varsigma_u &\leftarrow s + \frac{M_u}{2} \\
    \rho_u &\leftarrow r + \frac{1}{2} \sum_m (\sigma^2_{mu} + \mu^2_{mu})
\end{align}
where $M_u$ is the number of observations of unit $u$.

\subsection{$\lambda_0$}
Here, we calculate
\begin{multline}
    \frac{\partial\mathcal{L}}{\partial \mu_{0u}} = -\frac{1}{v_0^2} (\mu_{0u} - m_0) + \sum_{u(m) = u} \frac{\varsigma_{\eta u}}{\rho_{\eta u}} 
    \left( 
    \mu_{\eta m} - \mu_{0u} - \sum_k \xi_{t(m) k} \mu_{bku} + 
    \sum_r x_{mr} \mu_{\beta ru}
    \right) = 0 \\
    \Rightarrow \quad \mu_{0u} \leftarrow 
    \left(\frac{m_0}{v_0^2} + \sum_{u(m) = u} \frac{\varsigma_{\eta u}}{\rho_{\eta u}}  \right)^{-1} 
    \sum_{u(m) = u} \frac{\varsigma_{\eta u}}{\rho_{\eta u}} 
    \left[
    \mu_{\eta m} - \sum_k \xi_{t(m) k} \mu_{bku} + 
    \sum_r x_{mr} \mu_{\beta ru}
    \right]
\end{multline}
and similarly
\begin{align}
    &\frac{\partial\mathcal{L}}{\partial \sigma^2_{0u}} = -\frac{1}{2v_0^2} + 
    \frac{1}{2\sigma^2_{0u}} - \frac{1}{2} \sum_{u(m) = u} \frac{\varsigma_{\eta u}}{\rho_{\eta u}} = 0 \\
    \Rightarrow \quad &\tau_{0u} = \frac{1}{\sigma^2_{0u}} \leftarrow 
    \frac{1}{v^2_0} + \sum_{u(m) = u} \frac{\varsigma_{\eta u}}{\rho_{\eta u}}
\end{align}

\subsection{$b$}
Again, requiring 0 gradient gives
\begin{multline}
    &\frac{\partial\mathcal{L}}{\partial \mu_{bku}} = 
    -\frac{\varsigma_{bk}}{\rho_{bk}} \mu_{bku} + \\
    \sum_{u(m) = u} \frac{\varsigma_{\eta u}}{\rho_{\eta u}} 
    \Bigg[ \xi_{t(m) k}
    \left( 
    \mu_{\eta m} - \mu_{0u} - \sum_j \xi_{t(m) j} \mu_{bju} + 
    \sum_r x_{mr} \mu_{\beta ru}
    \right)  
    - \xi_{t(m) k} (1 - \xi_{t(m)k}) \mu_{bku} \Bigg] = 0 \\
\end{multline}
which yields the update equation
\begin{align}
    &\mu_{b u} \leftarrow \Lambda^{-1} \cdot \sum_{u(m) = u}
    \frac{\varsigma_{\eta u}}{\rho_{\eta u}} \xi_{t(m) } 
    \left(
    \mu_{\eta m} - \mu_{0u} + \sum_r x_{mr} \mu_{\beta r u}
    \right) \\
    &(\Lambda)_{kj} \equiv \frac{\varsigma_{bk}}{\rho_{bk}} \delta_{kj}
    + \sum_{u(m) = u} \left(\xi_{t(m)k} \xi_{t(m) j} 
    + \xi_{t(m) k} (1 - \xi_{t(m)k}) \delta_{kj} \right)
\end{align}
where we have written the update in matrix form, suppressing the latent 
state index $k$, and the update requires solving a $K x K$ system of linear equations.

Similarly,
\begin{align}
    &\frac{\partial\mathcal{L}}{\partial \sigma^2_{bku}} = 
    -\frac{1}{2}\frac{\varsigma_{bk}}{\rho_{bk}} + \frac{1}{2\sigma^2_{bku}}
    - \frac{1}{2}\sum_{u(m) = u} \frac{\varsigma_{\eta u}}{\rho_{\eta u}} \xi_{t(m) k} = 0\\
    \Rightarrow \quad &\tau_{bku} = \frac{1}{\sigma^2_{bku}} \leftarrow 
    \frac{\varsigma_{bk}}{\rho_{bk}} + \sum_{u(m) = u} \frac{\varsigma_{\eta u}}{\rho_{\eta u}} \xi_{t(m) k}
\end{align}



\subsection{$\lambda_0$ $b$, $\beta$, $\eta$}
Here, the nonlinearity in $\overline{\lambda_{mu}}$ makes updates less straightforward. In each case, we have something like
\begin{equation}
    \frac{\partial\mathcal{L}}{\partial \mu} = \frac{\partial\mathcal{E}}{\partial \mu} + \sum_{mu} \left[N_{mu} \frac{\partial\overline{\log \lambda_{mu}}}{\partial \mu} - \frac{\partial\overline{\lambda_{mu}}}{\partial \mu}\right] = 0
\end{equation}
which works out in the various cases as
\begin{align}
    \frac{\partial\mathcal{L}}{\partial \mu_{0u}} &= -\frac{\mu_{0u} - m_0}{v_0^2} + \sum_m [N_{mu} - \overline{\lambda_{mu}}] \\
    \frac{\partial\mathcal{L}}{\partial \mu_{bku}} &= 
    - \mu_{bku} \overline{\tau_{bk}} + \sum_m [N_{mu} - e^{\mu_{bku} + \sigma^2_{bku} / 2} F_{mku} ] \xi_{t(m) k} \\
    \frac{\partial\mathcal{L}}{\partial \mu_{\beta ju}} &= 
    - \mu_{\beta ju} \overline{\tau_{\beta j}} + \sum_m [N_{mu} - \overline{\lambda_{mu}}] x_{mj}  \\
    \frac{\partial\mathcal{L}}{\partial \mu_{\eta mu}} &= 
    - \mu_{\eta mu} \overline{\tau_{\eta u}} + N_{mu} - 
    \overline{\lambda_{mu}}
\end{align}
Unfortunately, $\overline{\lambda_{mu}}$ depends on the $\mu$ variables in question, requiring us to solve a transcendental equation in each case.

The case of variation with respect to $\sigma^2$ is similar:
\begin{align}
    \frac{\partial\mathcal{L}}{\partial \sigma^2_{0u}} &= -\frac{1}{2v_0^2} + \frac{1}{2\sigma^2_{0u}} - \frac{1}{2} \sum_m \overline{\lambda_{mu}} \\
    \frac{\partial\mathcal{L}}{\partial \sigma^2_{bku}} &= 
    - \frac{1}{2} \overline{\tau_{bk}} + \frac{1}{2\sigma^2_{bku}} - \frac{1}{2}\sum_m \xi_{t(m)k} e^{\mu_{bku} + \sigma^2_{bku} / 2} F_{mku}  \\
    \frac{\partial\mathcal{L}}{\partial \sigma^2_{\beta ju}} &= 
    - \frac{1}{2} \overline{\tau_{\beta j}} + \frac{1}{2\sigma^2_{\beta ju}} - \frac{1}{2} \sum_m x_{mj} \overline{\lambda_{mu}} \\
    \frac{\partial\mathcal{L}}{\partial \sigma^2_{\eta mu}} &= 
    -\frac{1}{2} \overline{\tau_{\eta u}} + \frac{1}{2\sigma^2_{\eta mu}} - \frac{1}{2} \overline{\lambda_{mu}}
\end{align}

\section{Khan et al. dual formulation}
Here, we use a dual convex formulation by Khan et al. (2014) to reduce the number of parameters and, in principle, allow for full posterior covariances. For the moment, however, we will continue to assume that firing rates for individual units are independent from one another given population hyperparameters and effects for different latent states are also independent.

As before, we let $m$ index observations (of which there are $M$ total), each observation corresponding to a single spike count at a given moment for a particular unit. That is, for each $m$, $u(m)$ is uniquely specified. As a result, we will replace $u$ indices with $m$ indices for simplicity. We also denote the log firing rate by $\eta$:
\begin{equation}
    \eta_m = \log \lambda_{mu} = \lambda_{0m} + \sum_k z_{mk} b_{k m} + \sum_r x_{mr} \beta_{r m} + \eta_m
\end{equation}
Now, we will introduce new variables $h_m \equiv \mathbb{E}_q[\eta_m]$, $\rho_m \equiv \mathrm{var}_q[\eta_m]$ with the constraints
\begin{align}
    h_m &= \mu_{0m} + \sum_k \xi_{mk} \mu_{bkm} + \sum_r x_{mr} \mu_{\beta rm} \\
    \rho_m &= \sigma^2_{0m} 
\end{align}



\end{document}