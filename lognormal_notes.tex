\documentclass[11pt]{article}
\usepackage{amssymb,amsmath}

\begin{document}
\title{The Gamma-Poisson Model}
\author{John Pearson}
\maketitle

\section{Observation Model}
Assume an experiment with $M$ observations of $U$ units exposed to a stimulus indexed by $T$. Each unit is responsive to $J$ regressors and $K$ latent states, and is described by a Poisson process given by 
\begin{equation}
    N_{mu} \sim \mathrm{Poiss}(\lambda_{mu}) 
\end{equation}
where $N_{mu}$ is the count observed from unit $u$ on observation $m$ and the effective rate $\lambda_{mu}$ is given by 
\begin{equation}
    \label{loglambda}
    \log \lambda_{mu} = \lambda_{0u} + \sum_k z_{mk} b_{ku} + \sum_j x_{mj} \beta_{ju} + \epsilon_{mu}
\end{equation}
Here, the coefficients $b_{ku}$ index responses of each unit to the (binary) latent states $z_{mk}$ while the $\beta_{ju}$ index responses to (non-latent) regressors $x_{mj}$. The final ($\epsilon$) term represents moment-to-moment fluctuations of each unit.

\section{Latent feature model}
We will assume a Hidden Markov Model (HMM) for the latent states $z$. We will likewise assume that these latent features are present in the stimulus, so that they are indexed by the stimulus time $t(m)$. When the meaning is clear, we will drop the dependency of stimulus time on observation number and simply write $t$.

To describe the transition model for the HMM, we take
\begin{align}
    A_{ij} &\equiv p(z_{t+1} = i|z_t = j) \\
    \pi_i &\equiv p(z_0 = i)
\end{align}
(Note that this means that the \emph{columns} of $A$ sum to 1, which is the opposite of the usual convention. In other words, in matrix notation, $z_{t+1} = A \cdot z_t$.)

Given this notation, it is straightforward to write the probability of a sequence of hidden states, conditioned on the chain parameters
\begin{equation}
    \log p(z|A, \pi) = \sum_t \log A_{z_{t+1} z_t} + \log \pi_{z_0}
\end{equation}

\section{Priors}
In addition to the model (\ref{loglambda}) above and the HMM ansatz, we posit the following hierarchical generative model for the parameters:
\begin{align}
    \lambda_{0u} &\sim \mathcal{N}(m_0, v^2_0) \\
    b_{ku} &\sim \mathcal{N}\left(0, (v^2_b)_{k}\right) \\
    (v^2_b)_{k} &\sim \text{Inv-Ga}\left((s_b)_k, (r_b)_k \right) \\
    \beta_{ku} &\sim \mathcal{N}\left(0, (v^2_\beta)_{k}\right) \\
    (v^2_\beta)_{k} &\sim \text{Inv-Ga}\left((s_\beta)_k, (r_\beta)_k \right) \\
    \epsilon_{mu} &\sim \mathcal{N}(0, (v^2_\epsilon)_u) \\
    (v^2_\epsilon)_{u} &\sim \text{Inv-Ga}\left((s_\epsilon)_u, (r_\epsilon)_u \right) \\
    \pi_k &\sim \mathrm{Beta}\left((a_\pi)_k, (b_\pi)_k \right) \\ 
    \left(A_k \right)_{1i} &\sim \mathrm{Beta}\left((a_A)_{ki}, (b_A)_{ki} \right)  
    \end{align}
where $A_{1i}$ is the probability of the transition $i \rightarrow 1$ (i.e., the transition \emph{into} the $z = 1$ state.)

Note also that, by putting sparse priors on the $v^2$ terms, we can implement automatic relevance determination (ARD) on the regression coefficients.

\section{Variational ansatz}
We would like to approximate the joint posterior density 
\begin{equation}
    p(\lambda_0, b, \beta, \epsilon, A, \pi, z|N) \propto p(N, z|b, \beta, \epsilon, A, \pi) p(b) p(\beta) p(\epsilon) p(A) p(\pi) p(v^2_b) p(v^2_\beta) p(v^2_\epsilon)
\end{equation}
with a structured mean field form that factorizes over units and chains:
\begin{multline}
    q(\lambda_0, b, \beta, \epsilon, A, \pi, z) = \prod_{ku} q(\lambda_{0u}) 
    q(b_{ku}) q(\beta_{ku}) q(\epsilon_u) q(z_k) q(A_k) q(\pi_k) \\
    \times q(v^2_{bk}) q(v^2_{\beta k}) q(v^2_{\epsilon u})
\end{multline}
For this, we will use the posterior variational ansatz
\begin{align}
    \lambda_{0u} &\sim \mathcal{N}\left(\mu_{0u}, \sigma^2_{0u}\right) \\
    b_{ku} &\sim \mathcal{N}\left((\mu_b)_{ku}, (\sigma^2_b)_{ku}\right) \\
    (v^2_b)_{k} &\sim \text{Inv-Ga}\left((\varsigma_b)_{k}, (\rho_b)_{k} \right) \\
    \beta_{ku} &\sim \mathcal{N}\left((\mu_\beta)_{ku}, (\sigma^2_\beta)_{ku}\right) \\
    (v^2_\beta)_{k} &\sim \text{Inv-Ga}\left((\varsigma_\beta)_k, (\rho_\beta)_k \right) \\
    \epsilon_{mu} &\sim \mathcal{N}((\mu_\epsilon)_{mu}, (\sigma^2_\epsilon)_{mu}) \\
    (v^2_\epsilon)_{u} &\sim \text{Inv-Ga}\left((\varsigma_\epsilon)_u, (\rho_\epsilon)_u \right) \\
    \pi_k &\sim \mathrm{Beta}\left((\gamma_\pi)_k, (\delta_\pi)_k \right) \\ 
    \left(A_k \right)_{1i} &\sim \mathrm{Beta}\left((\gamma_A)_{ki}, (\delta_A)_{ki} \right)  
\end{align}

\section{HMM inference}
Given the parameters of our observation model $\theta = (b, \beta, \epsilon, A, \pi)$, the well-known Forward-Backward Algorithm returns the following posteriors
\begin{align}
    \xi_t \equiv p(z_t|N, \theta) &\qquad \text{posterior marginals} \\
    \Xi_{t, ij} \equiv p(z_{t+1} = j, z_t = i|N, \theta) &\qquad \text{two-slice marginals} \\
    \log Z_t = \log p(N_{t+1 \bullet}|N_{t\bullet}, \theta) &\qquad \text{partition function}
\end{align}
The first two allow us to calculate expressions with respect to $q(z)$, while the last gives the normalization for the joint posterior over all $z$: $\log Z = \sum_t \log Z_t = \log p(N_{1:T}|\theta)$

\end{document}