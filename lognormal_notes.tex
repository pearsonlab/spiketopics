\documentclass[11pt]{article}
\usepackage{amssymb,amsmath}

\begin{document}
\title{The Gamma-Poisson Model}
\author{John Pearson}
\maketitle

\section{Observation Model}
Assume an experiment with $M$ observations of $U$ units exposed to a stimulus indexed by $T$. Each unit is responsive to $J$ regressors and $K$ latent states, and is described by a Poisson process given by 
\begin{equation}
    N_{mu} \sim \mathrm{Poiss}(\lambda_{mu}) 
\end{equation}
where $N_{mu}$ is the count observed from unit $u$ on observation $m$ and the effective rate $\lambda_{mu}$ is given by 
\begin{equation}
    \label{loglambda}
    \log \lambda_{mu} = \lambda_{0u} + \sum_k z_{mk} b_{ku} + \sum_j x_{mj} \beta_{ju} + \epsilon_{mu}
\end{equation}
Here, the coefficients $b_{ku}$ index responses of each unit to the (binary) latent states $z_{mk}$ while the $\beta_{ju}$ index responses to (non-latent) regressors $x_{mj}$. The final ($\epsilon$) term represents moment-to-moment fluctuations of each unit.

\section{Latent feature model}
We will assume a Hidden Markov Model (HMM) for the latent states $z$. We will likewise assume that these latent features are present in the stimulus, so that they are indexed by the stimulus time $t(m)$. When the meaning is clear, we will drop the dependency of stimulus time on observation number and simply write $t$.

To describe the transition model for the HMM, we take
\begin{align}
    A_{ij} &\equiv p(z_{t+1} = i|z_t = j) \\
    \pi_i &\equiv p(z_0 = i)
\end{align}
(Note that this means that the \emph{columns} of $A$ sum to 1, which is the opposite of the usual convention. In other words, in matrix notation, $z_{t+1} = A \cdot z_t$.)

Given this notation, it is straightforward to write the probability of a sequence of hidden states, conditioned on the chain parameters
\begin{equation}
    \log p(z|A, \pi) = \sum_t \log A_{z_{t+1} z_t} + \log \pi_{z_0}
\end{equation}

\section{Priors}
In addition to the model (\ref{loglambda}) above and the HMM ansatz, we posit the following hierarchical generative model for the parameters:
\begin{align}
    \lambda_{0u} &\sim \mathcal{N}(m_0, v^2_0) \\
    b_{ku} &\sim \mathcal{N}\left(0, (v^2_b)_{k}\right) \\
    (v^2_b)_{k} &\sim \text{Inv-Ga}\left((s_b)_k, (r_b)_k \right) \\
    \beta_{ku} &\sim \mathcal{N}\left(0, (v^2_\beta)_{k}\right) \\
    (v^2_\beta)_{k} &\sim \text{Inv-Ga}\left((s_\beta)_k, (r_\beta)_k \right) \\
    \epsilon_{mu} &\sim \mathcal{N}(0, (v^2_\epsilon)_u) \\
    (v^2_\epsilon)_{u} &\sim \text{Inv-Ga}\left((s_\epsilon)_u, (r_\epsilon)_u \right) \\
    \pi_k &\sim \mathrm{Beta}\left((a_\pi)_k, (b_\pi)_k \right) \\ 
    \left(A_k \right)_{1i} &\sim \mathrm{Beta}\left((a_A)_{ki}, (b_A)_{ki} \right)  
    \end{align}
where $A_{1i}$ is the probability of the transition $i \rightarrow 1$ (i.e., the transition \emph{into} the $z = 1$ state.)

Note also that, by putting sparse priors on the $v^2$ terms, we can implement automatic relevance determination (ARD) on the regression coefficients.

\section{Variational ansatz}
We would like to approximate the joint posterior density 
\begin{equation}
    p(\lambda_0, b, \beta, \epsilon, A, \pi, z|N) \propto p(N, z|b, \beta, \epsilon, A, \pi) p(b) p(\beta) p(\epsilon) p(A) p(\pi) p(v^2_b) p(v^2_\beta) p(v^2_\epsilon)
\end{equation}
with a structured mean field form that factorizes over units and chains:
\begin{multline}
    q(\lambda_0, b, \beta, \epsilon, A, \pi, z) = \prod_{ku} q(\lambda_{0u}) 
    q(b_{ku}) q(\beta_{ku}) q(\epsilon_u) q(z_k) q(A_k) q(\pi_k) \\
    \times q(v^2_{bk}) q(v^2_{\beta k}) q(v^2_{\epsilon u})
\end{multline}
For this, we will use the posterior variational ansatz
\begin{align}
    \lambda_{0u} &\sim \mathcal{N}\left(\mu_{0u}, \sigma^2_{0u}\right) \\
    b_{ku} &\sim \mathcal{N}\left((\mu_b)_{ku}, (\sigma^2_b)_{ku}\right) \\
    (v^2_b)_{k} &\sim \text{Inv-Ga}\left((\varsigma_b)_{k}, (\rho_b)_{k} \right) \\
    \beta_{ku} &\sim \mathcal{N}\left((\mu_\beta)_{ku}, (\sigma^2_\beta)_{ku}\right) \\
    (v^2_\beta)_{k} &\sim \text{Inv-Ga}\left((\varsigma_\beta)_k, (\rho_\beta)_k \right) \\
    \epsilon_{mu} &\sim \mathcal{N}((\mu_\epsilon)_{mu}, (\sigma^2_\epsilon)_{mu}) \\
    (v^2_\epsilon)_{u} &\sim \text{Inv-Ga}\left((\varsigma_\epsilon)_u, (\rho_\epsilon)_u \right) \\
    \pi_k &\sim \mathrm{Beta}\left((\gamma_\pi)_k, (\delta_\pi)_k \right) \\ 
    \left(A_k \right)_{1i} &\sim \mathrm{Beta}\left((\gamma_A)_{ki}, (\delta_A)_{ki} \right)  
\end{align}

\section{HMM inference}
Given the parameters of our observation model $\theta = (b, \beta, \epsilon, A, \pi)$, the well-known Forward-Backward Algorithm returns the following posteriors
\begin{align}
    \xi_t \equiv p(z_t|N, \theta) &\qquad \text{posterior marginals} \\
    \Xi_{t, ij} \equiv p(z_{t+1} = j, z_t = i|N, \theta) &\qquad \text{two-slice marginals} \\
    \log Z_t = \log p(N_{t+1 \bullet}|N_{t\bullet}, \theta) &\qquad \text{partition function}
\end{align}
The first two allow us to calculate expressions with respect to $q(z)$, while the last gives the normalization for the joint posterior over all $z$: $\log Z = \sum_t \log Z_t = \log p(N_{1:T}|\theta)$

\section{Evidence Lower Bound (ELBo)}
We would like to maximize a lower bound on the log evidence given by
\begin{equation}
    \mathcal{L} = \mathbb{E}_q \left[ \log \frac{p}{q} \right]
\end{equation}
Thanks to factorization in the priors and posterior ansatz, this can easily be broken down in pieces, one per variable type:

\subsection{$\lambda_0$}
We want 
\begin{multline}
    \mathbb{E}_{q(\lambda_0)}\left[ \log \frac{p(\lambda_0)}{q(\lambda_0)}\right] = \sum_u \mathbb{E}_q\left[ -\frac{1}{2v_0^2} (\lambda_{0u} - m_0)^2
    - \frac{1}{2}\log 2\pi v_0^2
    + \frac{1}{2\sigma_{0u}^2} (\lambda_{0u} - \mu_{0u})^2
    + \frac{1}{2}\log 2\pi \sigma^2_{0u}
    \right] \\
    = \sum_u \left[
    -\frac{1}{2v_0^2} \left(\sigma^2_{0u} + (\mu_{0u} - m_0)^2 \right)
    + \log \frac{\sigma_{0u}}{v_0} + \frac{1}{2}
    \right] 
\end{multline}
where we have used 
\begin{equation}
    \mathbb{E}[X^2] = \mathrm{var}[X] + \mathbb{E}[X]^2
\end{equation}

\subsection{$b$}
As with the $\lambda$ case, we have 
\begin{multline}
    \mathbb{E}_{q(b)}\left[ \log \frac{p(b)}{q(b)}\right] = \sum_{ku} \mathbb{E}_q\left[ -\frac{1}{2v_{bk}^2} b_{ku}^2
    - \frac{1}{2}\log 2\pi v_{bk}^2
    + \frac{1}{2\sigma_{bku}^2} (b_{ku} - \mu_{bku})^2
    + \frac{1}{2}\log 2\pi \sigma^2_{bku} 
    \right] \\
    = \sum_{ku} \left[
    -\frac{1}{2}\frac{\varsigma_{bk}}{\rho_{bk}}\left(\sigma^2_{bku} + \mu^2_{bku} \right)
    + \psi(\varsigma_{bk}) - \log \rho_{bk} - \log \sqrt{2\pi} + \frac{1}{2} \log 2\pi e \sigma^2_{bku}
    \right]
\end{multline}
Where we have used $v^2 \sim \text{Inv-Ga}(\varsigma, \rho)$ to define $\tau = v^{-2} \sim \mathrm{Ga}(\varsigma, \rho)$ and 
\begin{align}
    \mathbb{E}[\tau] &= \frac{\varsigma}{\rho} \\
    \mathbb{E}[\log \tau] &= \psi(\varsigma) - \log \rho
\end{align}
with $\psi(x)$ the digamma function.

\subsection{$v^2_b$}
Here again, we will define $\tau = v^{-2} \sim \mathrm{Ga}(\varsigma, \rho)$ so that we can write
\begin{multline}
    \mathbb{E}_{q(\tau)}\left[\log \frac{p(\tau)}{q(\tau)}\right] =
    \sum_k \mathbb{E}_q \left[
    (s_{bk} - 1) \log \tau_{bk} - r_{bk} \tau_{bk} \right]  
    + H_g(\varsigma_{bk}, \rho_{bk})    
    + \mathrm{const} \\
    = \sum_k \left[ 
    (s_{bk} - 1) (\psi(\varsigma_{bk}) - \log \rho_{bk}) 
    - r_{bk} \frac{\varsigma_{bk}}{\rho_{bk}} + H_g(\varsigma_{bk}, \rho_{bk})
    \right]
\end{multline}
where we have discarded constants that do not depend on the variational parameters $\varsigma$ and $\rho$ and $H_g$ is the differential entropy of the gamma distribution:
\begin{equation}
    H_g(a, b) = a - \log b + \log \Gamma(a) + (1 - a)\psi(a)
\end{equation}

\subsection{$\beta$, $\epsilon$}
Formulas in these cases are the same as those given above for $b$ and $v^2_b$, with only trivial substitutions required.

\subsection{$\pi$}
First, we have
\begin{equation}
    \mathbb{E}_{q(\pi)} \left[\log \frac{p(\pi)}{q(\pi)} \right] = \sum_k \left[(a_{\pi k} - 1)\overline{\log \pi_{k1}} + (b_{\pi k} - 1) \overline{\log \pi_{k0}} - \log B(a_{\pi k}, b_{\pi k}) + H(\pi_k) \right]
\end{equation}
with
\begin{align}
    \overline{\log \pi_{k1}} &= \psi(\gamma_{\pi k}) - \psi(\gamma_{\pi k} + \delta_{\pi k}) \\
    \overline{\log \pi_{k0}} &= \psi(\delta_{\pi k}) - \psi(\gamma_{\pi k} + \delta_{\pi k})
\end{align}
with $\psi$ the digamma function and 
\begin{equation}
    H(\pi_k) = H_b(\gamma_{\pi k}, \delta_{\pi k})
\end{equation}
with $H_b$ the entropy of the beta distribution:
\begin{equation}
    H_b(\alpha, \beta) = \log B(\alpha, \beta) - (\alpha - 1) \psi(\alpha) - (\beta - 1) \psi(\beta) + (\alpha + \beta - 2)\psi(\alpha + \beta)
\end{equation}

\subsection{$A$} 
Similarly,
\begin{equation}
    \mathbb{E}_{q(A)} \left[\log \frac{p(A)}{q(A)} \right] = 
\sum_{ik} \left[ (a_{Aki} - 1) \overline{\log A_{k1i}} + (b_{Aki} - 1) \overline{\log A_{k0i}} - \log B(a_{Aki}, b_{Aki}) + H(A_{k1i}) \right]
\end{equation}
where 
\begin{align}
    \overline{\log A_{k1i}} &= \psi(\gamma_{Aki}) - \psi(\gamma_{Aki} + \delta_{Aki}) \\
    \overline{\log A_{k0i}} &= \psi(\delta_{Aki}) - \psi(\gamma_{Aki} + \delta_{Aki})
\end{align}
and again
\begin{equation}
    H(A_{k1i}) = H_b(\gamma_{Aki}, \delta_{Aki})
\end{equation}




\end{document}