{
 "metadata": {
  "name": "",
  "signature": "sha256:614cb20c1fc12d2bb60dddfe09aaf9c9065c77c0ff70df56d3f728c011423ffc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "An LDA-inspired model for neural populations"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model 0:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assume\n",
      "1. A single neuron\n",
      "1. A Dirichlet (not Dirichlet process) prior on topics\n",
      "1. Topics are independent between successive times"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More specifically, let\n",
      "$$\n",
      "\\boldsymbol{\\theta}_t \\sim \\text{Dir}(\\boldsymbol{\\alpha}) \\quad \\text{distribution over topics for time $t$} \\\\\n",
      "\\lambda_j \\sim \\text{Ga}(a_j, b_j) \\quad \\text{for each topic} \\\\\n",
      "N_t \\sim \\text{Pois}(\\boldsymbol{\\theta}^T\\boldsymbol{\\lambda}) \\quad \\text{spike count at time $t$}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Calculating the joint distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, for each time $t$, we can calculate the joint distribution\n",
      "$$\n",
      "p(N, \\theta, \\lambda) \\propto p(N|\\lambda, \\theta)p(\\lambda)p(\\theta) \\\\\n",
      "\\propto \\frac{(\\theta^T\\lambda)^N}{N!} e^{-\\theta^T\\lambda} \\left(\\prod_j \\lambda_j^{a_j - 1} e^{-b_j\\lambda_j} \\right)\\left(\\prod_k \\theta_k^{\\alpha_k - 1} \\right) \\delta\\left(\\sum\\theta - 1\\right) \\\\\n",
      "\\propto (\\theta^T\\lambda)^N e^{-(\\theta + b)^T\\lambda} \\left(\\prod_k \\lambda_k^{a_k - 1}\\theta_k^{\\alpha_k - 1} \\right) \\delta\\left(\\sum\\theta - 1\\right) \\\\\n",
      "= \\left[\\sum_{|\\mathbf{n}|_1=N} \\binom{N}{\\mathbf{n}} \\prod_j \\left(\\theta_j \\lambda_j\\right)^{n_j} \\right] e^{-(\\theta + b)^T\\lambda} \\left(\\prod_k \\lambda_k^{a_k - 1}\\theta_k^{\\alpha_k - 1} \\right) \\delta\\left(\\sum\\theta - 1\\right) \\\\ \n",
      "= \\sum_{|\\mathbf{n}|_1=N} \\binom{N}{\\mathbf{n}} \\left[e^{-(\\theta + b)^T\\lambda} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} \\right)  \\right]\\delta\\left(\\sum\\theta - 1\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus $p(\\theta|N, \\lambda)$ is a mixture of Gamma distributions, as is $p(\\lambda|N, \\theta)$. However, these are constrained by $\\sum \\theta = 1$ and $\\sum n = N$, so the various Gammas are not indepedent."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Simplifying the joint distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, this suggests that we can simplify the joint distribution by adding the vector $\\mathbf{n}$ as a latent parameter, with\n",
      "$$\n",
      "\\mathbf{n} \\sim \\text{Multinomial}(N, \\mathbf{p}) \\\\\n",
      "p_i \\equiv \\frac{\\theta_i\\lambda_i}{\\theta^T\\lambda}\n",
      "$$\n",
      "that is, $p$ is the normalized elementwise product of $\\theta$ and $\\lambda$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this new variable, the joint distribution can then be written\n",
      "$$\n",
      "p(N, n, \\theta, \\lambda) \\propto \\binom{N}{n} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} e^{-(\\theta_k + b_k)\\lambda_k} \\right) \\delta\\left(\\sum\\theta - 1\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sampling Inference for Model 0"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given the expression above, it seems natural to implement a Gibbs sampling scheme. However, we have the problem that, while the $\\lambda_k$ are each independent (given $\\theta$), the $\\theta_k$ are not, due to the Dirichlet restriction.\n",
      "\n",
      "However, in this case, we can imagine rejection sampling scheme in which we sample $\\theta \\sim \\text{Dir}(n + \\alpha)$ and accept with probability $e^{-\\theta^T \\lambda}$. (Note that this may not be particularly efficient, given the likely values of $\\lambda$.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This suggests the following sampling scheme:\n",
      "\n",
      "$\\alpha$, $a$, $b$, $N$ known.\n",
      "\n",
      "Repeat:\n",
      "\n",
      "1. Draw $\\lambda_k \\sim \\text{Ga}(n_k + a_k, \\theta_k + b_k)$.\n",
      "1. Using either Metropolis-Hastings or rejection sampling, draw $\\theta$.\n",
      "1. Calculate $p = \\theta \\odot \\lambda / \\theta^T \\lambda$. Draw $n \\sim \\text{Mult}(N, p)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variational Inference for Model 0"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We begin with an approximating hypothesis for $p(\\lambda,\\theta|N)$:\n",
      "$$\n",
      "p(\\lambda, \\theta|N) \\approx q(\\lambda, \\theta) = q(\\theta)\\prod_k q(\\lambda_k)\\\\\n",
      "q(\\theta) \\sim \\text{Dir}(\\gamma) \\\\\n",
      "q(\\lambda_k) \\sim \\text{Ga}(c_k, d_k)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now following Blei, Ng, and Jordan, we can write the marginal distribution (suppressing hyperparameters) as\n",
      "\n",
      "$$\n",
      "\\log p(N) = \\log \\int\\! d\\theta d\\lambda \\; p(N, \\lambda,\\theta) = \\log \\int\\! d\\theta d\\lambda \\; \\frac{p(N, \\lambda,\\theta) q(\\lambda, \\theta)}{q(\\lambda, \\theta)} \\\\\n",
      "\\geq \\log \\int\\! d\\theta d\\lambda \\; q(\\lambda, \\theta) \\left[\\log p(N, \\lambda,\\theta) - \\log q(\\lambda, \\theta) \\right] \\\\\n",
      "= \\mathbb{E}[\\log p(N, \\lambda,\\theta)] + H[q] = \\mathcal{L}\n",
      "$$\n",
      "where the inequality follows from Jensen's inequality."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, it can be shown that maximizing this lower bound to the fixed expression on the left-hand side is equivalent to minimizing the KL divergence between $q(\\lambda, \\theta)$ and $p(\\lambda, \\theta|N)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now wish to calculate the terms in $\\mathcal{L}$.\n",
      "\n",
      "First of all, we can simply write down that\n",
      "$$\n",
      "H[q(\\lambda_k)] = c_k - \\log d_k + \\log\\Gamma(c_k) + (1-c_k)\\psi(c_k)\n",
      "$$\n",
      "with $(c_k, d_k)$ the shape and rate parameters of $q$, $\\Gamma$ the gamma function and $\\psi$ the digamma function. \n",
      "\n",
      "Likewise\n",
      "$$\n",
      "H[q(\\theta)] = \\log B(\\gamma) + (\\sum \\gamma - K)\\psi(\\sum \\gamma) - \\sum_k (\\gamma_k - 1) \\psi(\\gamma_k)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the expectations of $p$ with respect to $q$, we have\n",
      "$$\n",
      "\\mathbb{E}_q[\\log p(N, \\lambda, \\theta)] = \\mathbb{E}_q[\\log p(N|\\lambda, \\theta)] + \\mathbb{E}_q[\\log p(\\lambda)] + \\mathbb{E}_q[\\log p(\\theta)]\n",
      "$$\n",
      "\n",
      "Again, using standard results for the Gamma family:\n",
      "$$\n",
      "\\mathbb{E}_q[\\log p(\\lambda_k)] = \\mathbb{E}_q[(a_k - 1)\\log \\lambda_k - b_k\\lambda_k -a_k\\log b_k -\\log\\Gamma(a_k)] = (a_k - 1)\\left[\\psi(c_k) - \\log d_k\\right] - b_k \\frac{c_k}{d_k} + f(a_k, b_k)\n",
      "$$\n",
      "where where the last term is a constant in the variational parameters $(c_k, d_k)$.\n",
      "\n",
      "Just as easily:\n",
      "$$\n",
      "\\mathbb{E}_q[\\log p(\\theta)] = \\mathbb{E}_q[\\sum_k (\\alpha_k - 1)\\log \\theta_k] + g(\\alpha) \\\\\n",
      "= \\sum_k (\\alpha_k - 1) \\left[ \\psi(\\gamma_k) - \\psi(\\sum \\gamma)\\right] \\\\\n",
      "= \\sum_k (\\alpha_k - 1)\\psi(\\gamma_k) - \\psi(\\sum\\gamma)\\sum_k (\\alpha_k - 1)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we wish to calculate \n",
      "$$\n",
      "\\mathbb{E}_q[\\log p(N|\\lambda, \\theta)] = \\mathbb{E}_q[N \\log \\theta^T\\lambda - \\theta^T\\lambda] \\\\\n",
      "= N \\mathbb{E}_q\\left[ \\log \\sum_k \\theta_k\\lambda_k \\right] - \\sum_k \\frac{\\gamma_k}{\\sum\\gamma}\\frac{c_k}{d_k}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the first term, we will use Jensen's inequality to form a lower bound:\n",
      "$$\n",
      "\\mathbb{E}_q\\left[ \\log \\sum_k \\theta_k\\lambda_k \\right] \n",
      "\\geq \n",
      "\\mathbb{E}_q\\left[\\sum_k \\theta_k \\log \\lambda_k \\right] = \\\\\n",
      "\\sum_k \\mathbb{E}_q\\left[\\theta_k \\log\\lambda_k \\right] = \\sum_k \\frac{\\gamma_k}{\\sum \\gamma} \\left[ \\psi(c_k) - \\log d_k \\right]\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now write the full Lagrangian as\n",
      "$$\n",
      "\\mathcal{L} = \\sum_k \\left[ \\frac{N}{\\sum \\gamma} \\gamma_k \\left[\\psi(c_k) - \\log d_k \\right] - \\frac{\\gamma_k}{\\sum\\gamma}\\frac{c_k}{d_k} \\\\\n",
      "+ (a_k - 1)\\left[\\psi(c_k) - \\log d_k\\right] - b_k \\frac{c_k}{d_k} \\\\\n",
      "+ (\\alpha_k - 1)\\left[\\psi(\\gamma_k) - \\psi(\\sum\\gamma)\\right] \\\\\n",
      "+ c_k - \\log d_k + \\log\\Gamma(c_k) + (1-c_k)\\psi(c_k) \\\\\n",
      "- (\\gamma_k - 1)\\psi(\\gamma_k)\n",
      "\\right] + \\log B(\\gamma) + (\\sum \\gamma - K)\\psi(\\sum \\gamma)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model 0'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In contrast to the above, we could imagine that each time point possesses an **unnormalized** set of topic scores, $\\theta$, with\n",
      "$$\n",
      "\\theta_i \\sim \\text{Ga}(\\alpha_i, \\beta_i)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this model, we can repeat the above derivation to arrive at the joint distribution\n",
      "$$\n",
      "p(N, n, \\theta, \\lambda) \\propto \\binom{N}{n} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} e^{-(\\theta_k\\lambda_k + b_k \\lambda_k + \\beta_k\\theta_k)} \\right) \\\\\n",
      "\\propto \\binom{N}{n} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} e^{-(\\theta_k + b_k)(\\lambda_k + \\beta_k)} \\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, simple Gibbs sampling will do, since the conditionals for $\\theta_k$ are now independent and of Gamma form."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note**: In order to have nonzero probability of having some $\\theta_i \\neq 0$, we would need to use an exponential prior on $\\theta$. That is, we would need to set $\\alpha_i = 0$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sampling Inference for Model 0'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Due to the conjugate properties of Model 0', it is straightforward to implement the following Gibbs sampling algorithm:\n",
      "\n",
      "$\\alpha$, $a$, $b$, $N$ known.\n",
      "\n",
      "Repeat:\n",
      "\n",
      "1. Draw $\\lambda_k \\sim \\text{Ga}(n_k + a_k, \\theta_k + b_k)$.\n",
      "1. Draw $\\theta_k \\sim \\text{Ga}(n_k + \\alpha_k, \\lambda_k + \\beta_k)$.\n",
      "1. Calculate $p = \\theta \\odot \\lambda / \\theta^T \\lambda$. Draw $n \\sim \\text{Mult}(N, p)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variational Inference for Model 0'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We begin with an approximating hypothesis for $p(\\lambda,\\theta|N)$:\n",
      "$$\n",
      "p(\\lambda, \\theta|N) \\approx q(\\lambda, \\theta) = \\prod_k q(\\theta_k)q(\\lambda_k)\\\\\n",
      "q(\\theta_k) \\sim \\text{Ga}(\\gamma_k, \\delta_k) \\\\\n",
      "q(\\lambda_k) \\sim \\text{Ga}(c_k, d_k)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now wish to calculate the terms in $\\mathcal{L}$.\n",
      "\n",
      "First of all, we can simply write down that\n",
      "$$\n",
      "H[q(\\theta_k)] = \\gamma_k - \\log\\delta_k + \\log\\Gamma(\\gamma_k) + (1-\\gamma_k)\\psi(\\gamma_k)\n",
      "$$\n",
      "with $(\\gamma_k, \\delta_k)$ the shape and rate parameters of $q$, $\\Gamma$ the gamma function and $\\psi$ the digamma function. \n",
      "\n",
      "As a result, we can easily write the entropy term in $\\mathcal{L}$ as\n",
      "$$\n",
      "H[q] = \\sum_k \\left[ \\gamma_k - \\log\\delta_k + \\log\\Gamma(\\gamma_k) + (1-\\gamma_k)\\psi(\\gamma_k) + c_k - \\log d_k + \\log\\Gamma(c_k) + (1-c_k)\\psi(c_k)\\right]\n",
      "$$\n",
      "with one term for each $\\theta_k$ and $\\lambda_k$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Likewise, for the expectations of $p$ with respect to $q$, we have\n",
      "$$\n",
      "\\mathbb{E}_q[\\log p(N, \\lambda, \\theta)] = \\mathbb{E}_q[\\log p(N|\\lambda, \\theta)] + \\mathbb{E}_q[\\log p(\\lambda)] + \\mathbb{E}_q[\\log p(\\theta)]\n",
      "$$\n",
      "\n",
      "Again, using standard results for the Gamma family:\n",
      "$$\n",
      "\\mathbb{E}_q[\\log p(\\theta_k)] = \\mathbb{E}_q[(\\alpha_k - 1)\\log \\theta_k - \\beta_k\\theta_k -\\alpha_k\\log\\beta_k -\\log\\Gamma(\\alpha_k)] = (\\alpha_k - 1)\\left[\\psi(\\gamma_k) - \\log\\delta_k\\right] - \\beta_k \\frac{\\gamma_k}{\\delta_k} + f(\\alpha_k, \\beta_k)\n",
      "$$\n",
      "where where the last term is a constant in the variational parameters $(\\gamma_k, \\delta_k)$.\n",
      "\n",
      "Thus we can write the last two terms in the KL divergence as\n",
      "$$\n",
      "\\mathbb{E}_q[\\log p(\\lambda)] = \\sum_k \\left[(a_k - 1)\\left[\\psi(c_k) - \\log d_k\\right] - b_k \\frac{c_k}{d_k}\\right] \\\\\n",
      "\\mathbb{E}_q[\\log p(\\theta)] = \\sum_k \\left[ (\\alpha_k - 1)\\left[\\psi(\\gamma_k) - \\log\\delta_k\\right] - \\beta_k \\frac{\\gamma_k}{\\delta_k} \\right]\n",
      "$$\n",
      "up to irrelevant constants."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we wish to calculate \n",
      "$$\n",
      "\\mathbb{E}_q[\\log p(N|\\lambda, \\theta)] = \\mathbb{E}_q[N \\log \\theta^T\\lambda - \\theta^T\\lambda] \\\\\n",
      "= N \\mathbb{E}_q\\left[ \\log \\sum_k \\theta_k\\lambda_k \\right] - \\sum_k \\frac{\\gamma_k}{\\delta_k}\\frac{c_k}{d_k}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the first term, we will use Jensen's inequality to form a lower bound:\n",
      "$$\n",
      "\\mathbb{E}_q\\left[ \\log \\sum_k \\theta_k\\lambda_k \\right] = \\mathbb{E}_q\\left[ \\log \\sum_k \\frac{\\delta_k\\theta_k}{\\delta^T\\theta}(\\delta^T\\theta)\\frac{\\lambda_k}{\\delta_k}) \\right] \\geq \\mathbb{E}_q\\left[\\sum_k \\frac{\\delta_k\\theta_k}{\\delta^T\\theta} \\log (\\delta^T\\theta)\\frac{\\lambda_k}{\\delta_k} \\right] = \\\\\n",
      "\\sum_k \\mathbb{E}_q\\left[\\frac{\\delta_k\\theta_k}{\\delta^T\\theta} \\log \\lambda_k \\right] - \\mathbb{E}_q\\left[\\frac{\\delta_k\\theta_k}{\\delta^T\\theta} \\log \\delta_k \\right] + \\sum_k \\mathbb{E}_q\\left[\\frac{\\delta_k\\theta_k}{\\delta^T\\theta} \\log \\delta^T\\theta \\right]\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now it follows from $\\theta_k \\sim \\text{Ga}(\\gamma_k, \\delta_k)$ that \n",
      "$$\n",
      "\\delta_k\\theta_k \\sim \\text{Ga}(\\gamma_k, 1) \\\\ \n",
      "\\eta \\equiv \\delta \\odot \\theta / \\delta^T\\theta \\sim \\text{Dir}(\\gamma) \\\\ \n",
      "\\tau \\equiv \\delta^T\\theta \\sim \\text{Ga}(\\sum\\gamma, 1)\n",
      "$$\n",
      "We can thus rewrite the two terms above as \n",
      "$$\n",
      "\\mathbb{E}_q\\left[ \\log \\sum_k \\theta_k\\lambda_k \\right] \\geq \\sum_k \\mathbb{E}_\\eta[\\eta_k]\\mathbb{E}_\\lambda [\\log \\lambda_k] - \\sum_k \\mathbb{E}_\\eta[\\eta_k]\\log \\delta_k + \\sum_k \\mathbb{E}_\\eta[\\eta_k] \\mathbb{E}_\\tau [\\log \\tau] \\\\\n",
      "\\sum_k \\frac{\\gamma_k}{\\sum \\gamma}\\left[ \\psi(c_k) - \\log d_k - \\log \\delta_k + \\psi(\\sum\\gamma)\\right]\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now write the full Lagrangian as\n",
      "$$\n",
      "\\mathcal{L} = \\sum_k \\left[ \\frac{N}{\\sum \\gamma} \\gamma_k \\left[\\psi(c_k) - \\log d_k - \\log \\delta_k + \\psi(\\sum\\gamma)\\right] - \\frac{\\gamma_k}{\\delta_k}\\frac{c_k}{d_k} \\\\\n",
      "+ (a_k - 1)\\left[\\psi(c_k) - \\log d_k\\right] - b_k \\frac{c_k}{d_k} \\\\\n",
      "+ (\\alpha_k - 1)\\left[\\psi(\\gamma_k) - \\log\\delta_k\\right] - \\beta_k \\frac{\\gamma_k}{\\delta_k} \\\\\n",
      "+ \\gamma_k - \\log\\delta_k + \\log\\Gamma(\\gamma_k) + (1-\\gamma_k)\\psi(\\gamma_k) \\\\\n",
      "+ c_k - \\log d_k + \\log\\Gamma(c_k) + (1-c_k)\\psi(c_k)\\right]\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}