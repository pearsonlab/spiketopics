{
 "metadata": {
  "name": "",
  "signature": "sha256:5c4283d4776ab03cb26132ac0919ecabf75dadeeaa7100e9c7d43556e20104e9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "An LDA-inspired model for neural populations"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model 0:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assume\n",
      "1. A single neuron\n",
      "1. A Dirichlet (not Dirichlet process) prior on topics\n",
      "1. Topics are independent between successive times"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More specifically, let\n",
      "$$\n",
      "\\boldsymbol{\\theta}_t \\sim \\text{Dir}(\\boldsymbol{\\alpha}) \\quad \\text{distribution over topics for time $t$} \\\\\n",
      "\\lambda_j \\sim \\text{Ga}(a_j, b_j) \\quad \\text{for each topic} \\\\\n",
      "N_t \\sim \\text{Pois}(\\boldsymbol{\\theta}^T\\boldsymbol{\\lambda}) \\quad \\text{spike count at time $t$}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Calculating the joint distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, for each time $t$, we can calculate the joint distribution\n",
      "$$\n",
      "p(N, \\theta, \\lambda) \\propto p(N|\\lambda, \\theta)p(\\lambda)p(\\theta) \\\\\n",
      "\\propto \\frac{(\\theta^T\\lambda)^N}{N!} e^{-\\theta^T\\lambda} \\left(\\prod_j \\lambda_j^{a_j - 1} e^{-b_j\\lambda_j} \\right)\\left(\\prod_k \\theta_k^{\\alpha_k - 1} \\right) \\delta\\left(\\sum\\theta - 1\\right) \\\\\n",
      "\\propto (\\theta^T\\lambda)^N e^{-(\\theta + b)^T\\lambda} \\left(\\prod_k \\lambda_k^{a_k - 1}\\theta_k^{\\alpha_k - 1} \\right) \\delta\\left(\\sum\\theta - 1\\right) \\\\\n",
      "= \\left[\\sum_{|\\mathbf{n}|_1=N} \\binom{N}{\\mathbf{n}} \\prod_j \\left(\\theta_j \\lambda_j\\right)^{n_j} \\right] e^{-(\\theta + b)^T\\lambda} \\left(\\prod_k \\lambda_k^{a_k - 1}\\theta_k^{\\alpha_k - 1} \\right) \\delta\\left(\\sum\\theta - 1\\right) \\\\ \n",
      "= \\sum_{|\\mathbf{n}|_1=N} \\binom{N}{\\mathbf{n}} \\left[e^{-(\\theta + b)^T\\lambda} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} \\right)  \\right]\\delta\\left(\\sum\\theta - 1\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus $p(\\theta|N, \\lambda)$ is a mixture of Gamma distributions, as is $p(\\lambda|N, \\theta)$. However, these are constrained by $\\sum \\theta = 1$ and $\\sum n = N$, so the various Gammas are not indepedent."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Simplifying the joint distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, this suggests that we can simplify the joint distribution by adding the vector $\\mathbf{n}$ as a latent parameter, with\n",
      "$$\n",
      "\\mathbf{n} \\sim \\text{Multinomial}(N, \\mathbf{p}) \\\\\n",
      "p_i \\equiv \\frac{\\theta_i\\lambda_i}{\\theta^T\\lambda}\n",
      "$$\n",
      "that is, $p$ is the normalized elementwise product of $\\theta$ and $\\lambda$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this new variable, the joint distribution can then be written\n",
      "$$\n",
      "p(N, n, \\theta, \\lambda) \\propto \\binom{N}{n} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} e^{-(\\theta_k + b_k)\\lambda_k} \\right) \\delta\\left(\\sum\\theta - 1\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sampling Inference for Model 0"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given the expression above, it seems natural to implement a Gibbs sampling scheme. However, we have the problem that, while the $\\lambda_k$ are each independent (given $\\theta$), the $\\theta_k$ are not, due to the Dirichlet restriction.\n",
      "\n",
      "However, in this case, we can imagine rejection sampling scheme in which we sample $\\theta \\sim \\text{Dir}(n + \\alpha)$ and accept with probability $e^{-\\theta^T \\lambda}$. (Note that this may not be particularly efficient, given the likely values of $\\lambda$.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This suggests the following sampling scheme:\n",
      "\n",
      "$\\alpha$, $a$, $b$, $N$ known.\n",
      "\n",
      "Repeat:\n",
      "\n",
      "1. Draw $\\lambda_k \\sim \\text{Ga}(n_k + a_k, \\theta_k + b_k)$.\n",
      "1. Using either Metropolis-Hastings or rejection sampling, draw $\\theta$.\n",
      "1. Calculate $p = \\theta \\odot \\lambda / \\theta^T \\lambda$. Draw $n \\sim \\text{Mult}(N, p)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variational Inference"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Variational Inference: basic setup"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following Blei, Ng, and Jordan, we can write the marginal distribution (suppressing hyperparameters) as\n",
      "\n",
      "$$\n",
      "\\log p(N) = \\log \\int\\! d\\theta d\\lambda \\; p(N, \\lambda,\\theta) = \\log \\int\\! d\\theta d\\lambda \\; \\frac{p(N, \\lambda,\\theta) q(\\lambda, \\theta)}{q(\\lambda, \\theta)} \\\\\n",
      "\\geq \\log \\int\\! d\\theta d\\lambda \\; q(\\lambda, \\theta) \\left[\\log p(N, \\lambda,\\theta) - \\log q(\\lambda, \\theta) \\right] \\\\\n",
      "= \\mathbb{E}[\\log p(N, \\lambda,\\theta)] + H[q] = \\mathcal{L}\n",
      "$$\n",
      "where the inequality follows from Jensen's inequality."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, it can be shown that maximizing this lower bound to the fixed expression on the left-hand side is equivalent to minimizing the KL divergence between $q(\\lambda, \\theta)$ and $p(\\lambda, \\theta|N)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Variational Inference: mean field for exponential families:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we follow the derivation in Appendix A of [Blei and Jordan (2006)](http://projecteuclid.org/euclid.ba/1340371077)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us assume that the variational ansatz $q_\\nu(\\theta)$ for the model parameters $\\theta$ is of the fully-factorized mean-field form:\n",
      "$$\n",
      "q_\\nu(\\theta) = \\prod_j q_{\\nu_j}(\\theta_j)\n",
      "$$\n",
      "and that each factor is a member of the exponential family:\n",
      "$$\n",
      "q_{\\nu_j}(\\theta_j) = h(\\theta_j)e^{\\nu_j^T\\theta_j - a(\\nu_j)}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Under these circumstances, it can then be shown that maximize $\\mathcal{L}$ is equivalent to solving\n",
      "$$\n",
      "\\nu_j = \\frac{1}{a''(\\nu_j)}\\left[\\frac{\\partial}{\\partial \\nu_j}\\mathbb{E}_q[p(\\theta_j|\\theta_{-j}, x)] - \\frac{\\partial}{\\partial \\nu_j}\\mathbb{E}_q[h(\\theta_j)] \\right]\n",
      "$$\n",
      "where $p(\\theta_j|\\theta_{-j}, x)$ is the conditional distribution of $\\theta_j$ given the other model parameters and the data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, there is a further model simplification that occurs when the conditional distribution itself takes an exponential form. When we can write\n",
      "$$\n",
      "p(\\theta_j|\\theta_{-j}, x) = h(\\theta_j)\\exp\\left[g_j(\\theta_{-j}, x)^T \\theta_j - a(g_j(\\theta_{-j}, x))\\right]\n",
      "$$\n",
      "the equation for the optimal $\\nu_j$ reduces to\n",
      "$$\n",
      "\\nu_j = \\mathbb{E}_q[g(\\theta_{-j}, x)]\n",
      "$$\n",
      "That is, the natural parameter in the variational distribution is the expectation over the variational distribution of the natural parameter of the conditional distribution.\n",
      "\n",
      "Thus, in what follows, we will use this formula rather than derive the Lagrangian from scratch."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variational Inference for Model 0"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We begin with the variational ansatz:\n",
      "$$\n",
      "p(\\lambda, \\theta|N) \\approx q(\\lambda, \\theta) = q(\\theta)\\prod_k q(\\lambda_k)\\\\\n",
      "q(\\theta) \\sim \\text{Dir}(\\gamma) \\\\\n",
      "q(\\lambda_k) \\sim \\text{Ga}(c_k, d_k)\n",
      "$$\n",
      "Most importantly, this form is fully factorized, and all distributions are in the exponential form."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, $p(N, \\theta, \\lambda)$ does not factorize, being a sum of terms, and so we cannot easily apply the above formulae. To do so, we will once again add the auxiliary variable $n$ to our variational ansatz. (We can think of this $n$ as something like the number of spikes resulting from each category.)\n",
      "$$\n",
      "q(n) \\sim \\text{Mult}(N, \\pi)\n",
      "$$\n",
      "giving us a revised approximation\n",
      "$$\n",
      "p(\\lambda, \\theta, n|N) \\approx q(\\lambda, \\theta, n) = q(\\theta)q(n)\\prod_k q(\\lambda_k)\\\\\n",
      "q(\\theta) \\sim \\text{Dir}(\\gamma) \\\\\n",
      "q(\\lambda_k) \\sim \\text{Ga}(c_k, d_k) \\\\\n",
      "q(n) \\sim \\text{Mult}(N, \\pi)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a result, we can now easily see that the conditional posterior densities for each variable take the following form:\n",
      "$$\n",
      "p(\\lambda_k|\\lambda_{-k}, \\theta, n, N) \\sim \\text{Ga}(n_k + a_k, \\theta_k + b_k) \\\\\n",
      "p(n|\\lambda, \\theta, N) \\sim \\text{Mult}(N, p)\n",
      "$$\n",
      "where $p_j \\equiv \\theta_j\\lambda_j/\\theta^T\\lambda$.\n",
      "\n",
      "Unfortunately $p(\\theta|\\lambda, n, N)$, while in the exponential family, does not have a simple, closed-form integral. As a result, the simple update formulae will not apply. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model 0'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In contrast to the above, we could imagine that each time point possesses an **unnormalized** set of topic scores, $\\theta$, with\n",
      "$$\n",
      "\\theta_i \\sim \\text{Ga}(\\alpha_i, \\beta_i)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this model, we can repeat the above derivation to arrive at the joint distribution\n",
      "$$\n",
      "p(N, n, \\theta, \\lambda) \\propto \\binom{N}{n} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} e^{-(\\theta_k\\lambda_k + b_k \\lambda_k + \\beta_k\\theta_k)} \\right) \\\\\n",
      "\\propto \\binom{N}{n} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} e^{-(\\theta_k + b_k)(\\lambda_k + \\beta_k)} \\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, simple Gibbs sampling will do, since the conditionals for $\\theta_k$ are now independent and of Gamma form."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note**: In order to have nonzero probability of having some $\\theta_i \\neq 0$, we would need to use an exponential prior on $\\theta$. That is, we would need to set $\\alpha_i = 0$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sampling Inference for Model 0'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Due to the conjugate properties of Model 0', it is straightforward to implement the following Gibbs sampling algorithm:\n",
      "\n",
      "$\\alpha$, $a$, $b$, $N$ known.\n",
      "\n",
      "Repeat:\n",
      "\n",
      "1. Draw $\\lambda_k \\sim \\text{Ga}(n_k + a_k, \\theta_k + b_k)$.\n",
      "1. Draw $\\theta_k \\sim \\text{Ga}(n_k + \\alpha_k, \\lambda_k + \\beta_k)$.\n",
      "1. Calculate $p = \\theta \\odot \\lambda / \\theta^T \\lambda$. Draw $n \\sim \\text{Mult}(N, p)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variational Inference for Model 0'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As above, we begin with an approximating hypothesis for $p(\\lambda,\\theta, n|N)$:\n",
      "$$\n",
      "p(\\lambda, \\theta|N) \\approx q(\\lambda, \\theta) = q(n)\\prod_k q(\\theta_k)q(\\lambda_k)\\\\\n",
      "q(\\theta_k) \\sim \\text{Ga}(\\gamma_k, \\delta_k) \\\\\n",
      "q(\\lambda_k) \\sim \\text{Ga}(c_k, d_k) \\\\\n",
      "q(n) \\sim \\text{Mult}(N, \\pi)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, as opposed to Model 0, all the conditionals take simplified exponential family forms:\n",
      "$$\n",
      "p(\\lambda_k|\\lambda_{-k}, \\theta, n, N) \\sim \\text{Ga}(n_k + a_k, \\theta_k + b_k) \\\\\n",
      "p(\\theta_k|\\lambda, \\theta_{-k}, n, N) \\sim \\text{Ga}(n_k + \\alpha_k, \\lambda_k + \\beta_k) \\\\\n",
      "p(n|\\lambda, \\theta, N) \\sim \\text{Mult}(N, p)\n",
      "$$\n",
      "where $p_j \\equiv \\theta_j\\lambda_j/\\theta^T\\lambda$. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, using the formula from Blei and Jordan above, we can write down the update equations for the natural parameters for each variable:\n",
      "$$\n",
      "\\eta_k^\\lambda = \\begin{bmatrix} c_k - 1 \\\\ -d_k\\end{bmatrix} \\leftarrow \n",
      "\\mathbb{E}_q \\begin{bmatrix} n_k + a_k - 1 \\\\ -\\theta_k -b_k\\end{bmatrix} = \\begin{bmatrix} \\pi_k + a_k - 1 \\\\ -\\frac{\\gamma_k}{\\delta_k} -b_k\\end{bmatrix} \\\\\n",
      "\\eta_k^\\theta = \\begin{bmatrix} \\gamma_k - 1 \\\\ -\\delta_k\\end{bmatrix} \\leftarrow \n",
      "\\mathbb{E}_q \\begin{bmatrix} n_k + \\alpha_k - 1 \\\\ -\\lambda_k -\\beta_k\\end{bmatrix} = \\begin{bmatrix} \\pi_k + \\alpha_k - 1 \\\\ -\\frac{c_k}{d_k} -\\beta_k\\end{bmatrix} \\\\\n",
      "\\eta_k^n = \\log\\pi_k \\leftarrow \\mathbb{E}_q\\left[\\log \\frac{\\theta_k\\lambda_k}{\\theta^T\\lambda}\\right] = \\psi(\\gamma_k) -\\log \\delta_k + \\psi(c_k) -\\log d_k - \\mathbb{E}_q[\\log \\theta^T\\lambda]\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now there is no simple expression for the last expectation above, but we can replace it with the tightest first-order lower bound (cf. [Paisley 2010](http://www.columbia.edu/~jwp2128/Teaching/E6892/papers/twobounds.pdf)):\n",
      "$$\n",
      "-\\mathbb{E}_q[\\log \\sum_k \\theta_k\\lambda_k] \\geq -\\log \\sum_k \\mathbb{E}_q[\\theta_k\\lambda_k] =\n",
      "-\\log \\sum_k \\frac{\\gamma_k c_k}{\\delta_k d_k}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result of all this is a simple set of algebraic updates:\n",
      "$$\n",
      "c_k \\leftarrow \\pi_k + a_k \\\\\n",
      "d_k \\leftarrow b_k + \\frac{\\gamma_k}{\\delta_k} \\\\\n",
      "\\gamma_k \\leftarrow \\pi_k + \\alpha_k \\\\\n",
      "\\delta_k \\leftarrow \\beta_k + \\frac{c_k}{d_k} \\\\\n",
      "C \\leftarrow \\sum_k \\frac{\\gamma_k c_k}{\\delta_k d_k} \\\\\n",
      "\\log \\pi_k \\leftarrow \\psi(\\gamma_k) -\\log \\delta_k + \\psi(c_k) -\\log d_k - \\log C\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Multiple responding units"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now extend Model 0' to multiple response units, each of which is described by a copy of the above. More specifically, let there be $U$ units and $T$ times, such that the counts are given by $N_{ut}$. Then the distribution of response counts is given by\n",
      "$$\n",
      "N_{tu} \\sim \\text{Pois}((\\Theta\\Lambda)_{tu}) = \\text{Pois}\\left(\\sum_k \\Theta_{tk}\\Lambda_{ku}\\right)\n",
      "$$\n",
      "Thus, we have replaced the vectors $\\theta$ and $\\lambda$ with matrices, where $\\Theta$ is a $T \\times K$ matrix of frame categories with one row per frame and $\\Lambda$ is a $K \\times U$ matrix of neural responses with one column per unit."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In what follows, we will once again introduce the auxiliary variable $n$, which this time forms a $K \\times T \\times U$ 3-tensor satisfying $\\sum_k n_{ktu} = N_{tu}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following the above, we write the total joint distribution of the data as\n",
      "$$\n",
      "p(n, \\Lambda, \\Theta) = p(n|\\Lambda, \\Theta) p(\\Lambda) p(\\Theta) \\\\\n",
      "p(n | \\Lambda, \\Theta) \\propto \\left(\\prod_{ktu} [n_{ktu}!]^{-1} \\right)\\exp\\left(\\sum_{ktu} \\left[ -\\theta_{tk} \\lambda_{ku} + n_{ktu} (\\log \\theta_{tk} + \\log \\lambda_{ku})\\right] \\right) \\\\\n",
      "p(\\Lambda) \\propto \\exp\\left(\\sum_{ku} \\left[(a_{ku} - 1)\\log \\lambda_{ku} - b_{ku}\\lambda_{ku} \\right] \\right) \\\\\n",
      "p(\\Theta) \\propto \\exp\\left(\\sum_{tk} \\left[(\\alpha_{tk} - 1)\\log \\theta_{tk} - \\beta_{tk}\\theta_{tk} \\right] \\right) \\\\\n",
      "\\Rightarrow p(n, \\Lambda, \\Theta) \\propto \\left(\\prod_{ktu} [n_{ktu}!]^{-1} \\right) \\exp\n",
      "\\left(\n",
      "\\sum_{ktu} \\left[ -\\theta_{tk} \\lambda_{ku} + n_{ktu} (\\log \\theta_{tk} + \\log \\lambda_{ku}) \\right] \\\\\n",
      "+ \\sum_{ku} \\left[ (a_{ku} - 1)\\log \\lambda_{ku} - b_{ku}\\lambda_{ku} \\right] \\\\\n",
      "+ \\sum_{tk} \\left[ (\\alpha_{tk} - 1)\\log \\theta_{tk} - \\beta_{tk}\\theta_{tk} \\right] \n",
      "\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clearly, this is in the exponential form we require, and a straightforward generalization of the derivation above gives\n",
      "$$\n",
      "c_{ku} \\leftarrow \\sum_t \\pi_{ktu} + a_{ku} \\\\\n",
      "d_{ku} \\leftarrow \\sum_t \\frac{\\gamma_{tk}}{\\delta_{tk}} + b_{ku} \\\\\n",
      "\\gamma_{tk} \\leftarrow \\sum_u \\pi_{ktu} + \\alpha_{tk} \\\\\n",
      "\\delta_{tk} \\leftarrow \\sum_u \\frac{c_{ku}}{d_{ku}} + \\beta_{tk} \\\\\n",
      "C_{tu} \\leftarrow \\sum_k \\frac{\\gamma_{tk}}{\\delta_{tk}}\\frac{c_{ku}}{d_{ku}} \\\\\n",
      "\\log \\pi_{ktu} \\leftarrow \\psi(\\gamma_{tk}) - \\log \\delta_{tk} + \\psi(c_{ku}) - \\log d_{ku} - \\log C_{tu}\n",
      "$$"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}