{
 "metadata": {
  "name": "",
  "signature": "sha256:6299a32b96fb183599585968783767760449975640c76a4152965229bb76c6c5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "An LDA-inspired model for neural populations"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model 0:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assume\n",
      "1. A single neuron\n",
      "1. A Dirichlet (not Dirichlet process) prior on topics\n",
      "1. Topics are independent between successive times"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More specifically, let\n",
      "$$\n",
      "\\boldsymbol{\\theta}_t \\sim \\text{Dir}(\\boldsymbol{\\alpha}) \\quad \\text{distribution over topics for time $t$} \\\\\n",
      "\\lambda_j \\sim \\text{Ga}(a_j, b_j) \\quad \\text{for each topic} \\\\\n",
      "N_t \\sim \\text{Pois}(\\boldsymbol{\\theta}^T\\boldsymbol{\\lambda}) \\quad \\text{spike count at time $t$}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Calculating the joint distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, for each time $t$, we can calculate the joint distribution\n",
      "$$\n",
      "p(N, \\theta, \\lambda) \\propto p(N|\\lambda, \\theta)p(\\lambda)p(\\theta) \\\\\n",
      "\\propto \\frac{(\\theta^T\\lambda)^N}{N!} e^{-\\theta^T\\lambda} \\left(\\prod_j \\lambda_j^{a_j - 1} e^{-b_j\\lambda_j} \\right)\\left(\\prod_k \\theta_k^{\\alpha_k - 1} \\right) \\delta\\left(\\sum\\theta - 1\\right) \\\\\n",
      "\\propto (\\theta^T\\lambda)^N e^{-(\\theta + b)^T\\lambda} \\left(\\prod_k \\lambda_k^{a_k - 1}\\theta_k^{\\alpha_k - 1} \\right) \\delta\\left(\\sum\\theta - 1\\right) \\\\\n",
      "= \\left[\\sum_{|\\mathbf{n}|_1=N} \\binom{N}{\\mathbf{n}} \\prod_j \\left(\\theta_j \\lambda_j\\right)^{n_j} \\right] e^{-(\\theta + b)^T\\lambda} \\left(\\prod_k \\lambda_k^{a_k - 1}\\theta_k^{\\alpha_k - 1} \\right) \\delta\\left(\\sum\\theta - 1\\right) \\\\ \n",
      "= \\sum_{|\\mathbf{n}|_1=N} \\binom{N}{\\mathbf{n}} \\left[e^{-(\\theta + b)^T\\lambda} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} \\right)  \\right]\\delta\\left(\\sum\\theta - 1\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus $p(\\theta|N, \\lambda)$ is a mixture of Gamma distributions, as is $p(\\lambda|N, \\theta)$. However, these are constrained by $\\sum \\theta = 1$ and $\\sum n = N$, so the various Gammas are not indepedent."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Simplifying the joint distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, this suggests that we can simplify the joint distribution by adding the vector $\\mathbf{n}$ as a latent parameter, with\n",
      "$$\n",
      "\\mathbf{n} \\sim \\text{Multinomial}(N, \\mathbf{p}) \\\\\n",
      "p_i \\equiv \\frac{\\theta_i\\lambda_i}{\\theta^T\\lambda}\n",
      "$$\n",
      "that is, $p$ is the normalized elementwise product of $\\theta$ and $\\lambda$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this new variable, the joint distribution can then be written\n",
      "$$\n",
      "p(N, n, \\theta, \\lambda) \\propto \\binom{N}{n} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} e^{-(\\theta_k + b_k)\\lambda_k} \\right) \\delta\\left(\\sum\\theta - 1\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sampling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given the expression above, it seems natural to implement a Gibbs sampling scheme. However, we have the problem that, while the $\\lambda_k$ are each independent (given $\\theta$), the $\\theta_k$ are not, due to the Dirichlet restriction.\n",
      "\n",
      "However, in this case, we can imagine rejection sampling scheme in which we sample $\\theta \\sim \\text{Dir}(n + \\alpha)$ and accept with probability $e^{-\\theta^T \\lambda}$. (Note that this may not be particularly efficient, given the likely values of $\\lambda$.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This suggests the following sampling scheme:\n",
      "\n",
      "$\\alpha$, $a$, $b$, $N$ known.\n",
      "\n",
      "Repeat:\n",
      "\n",
      "1. Draw $\\lambda_k \\sim \\text{Ga}(n_k + a_k, \\theta_k + b_k)$.\n",
      "1. Using either Metropolis-Hastings or rejection sampling, draw $\\theta$.\n",
      "1. Calculate $p = \\theta \\odot \\lambda / \\theta^T \\lambda$. Draw $n \\sim \\text{Mult}(N, p)$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}