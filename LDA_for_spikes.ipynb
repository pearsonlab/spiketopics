{
 "metadata": {
  "name": "",
  "signature": "sha256:abf7df6ebfd2ee75eeec34782107d08b42003e66763b03aa45922cf4baac11ea"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "An LDA-inspired model for neural populations"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model 0:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assume\n",
      "1. A single neuron\n",
      "1. A Dirichlet (not Dirichlet process) prior on topics\n",
      "1. Topics are independent between successive times"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More specifically, let\n",
      "$$\n",
      "\\boldsymbol{\\theta}_t \\sim \\text{Dir}(\\boldsymbol{\\alpha}) \\quad \\text{distribution over topics for time $t$} \\\\\n",
      "\\lambda_j \\sim \\text{Ga}(a_j, b_j) \\quad \\text{for each topic} \\\\\n",
      "N_t \\sim \\text{Pois}(\\boldsymbol{\\theta}^T\\boldsymbol{\\lambda}) \\quad \\text{spike count at time $t$}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Calculating the joint distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, for each time $t$, we can calculate the joint distribution\n",
      "$$\n",
      "p(N, \\theta, \\lambda) \\propto p(N|\\lambda, \\theta)p(\\lambda)p(\\theta) \\\\\n",
      "\\propto \\frac{(\\theta^T\\lambda)^N}{N!} e^{-\\theta^T\\lambda} \\left(\\prod_j \\lambda_j^{a_j - 1} e^{-b_j\\lambda_j} \\right)\\left(\\prod_k \\theta_k^{\\alpha_k - 1} \\right) \\delta\\left(\\sum\\theta - 1\\right) \\\\\n",
      "\\propto (\\theta^T\\lambda)^N e^{-(\\theta + b)^T\\lambda} \\left(\\prod_k \\lambda_k^{a_k - 1}\\theta_k^{\\alpha_k - 1} \\right) \\delta\\left(\\sum\\theta - 1\\right) \\\\\n",
      "= \\left[\\sum_{|\\mathbf{n}|_1=N} \\binom{N}{\\mathbf{n}} \\prod_j \\left(\\theta_j \\lambda_j\\right)^{n_j} \\right] e^{-(\\theta + b)^T\\lambda} \\left(\\prod_k \\lambda_k^{a_k - 1}\\theta_k^{\\alpha_k - 1} \\right) \\delta\\left(\\sum\\theta - 1\\right) \\\\ \n",
      "= \\sum_{|\\mathbf{n}|_1=N} \\binom{N}{\\mathbf{n}} \\left[e^{-(\\theta + b)^T\\lambda} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} \\right)  \\right]\\delta\\left(\\sum\\theta - 1\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus $p(\\theta|N, \\lambda)$ is a mixture of Gamma distributions, as is $p(\\lambda|N, \\theta)$. However, these are constrained by $\\sum \\theta = 1$ and $\\sum n = N$, so the various Gammas are not indepedent."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Simplifying the joint distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, this suggests that we can simplify the joint distribution by adding the vector $\\mathbf{n}$ as a latent parameter, with\n",
      "$$\n",
      "\\mathbf{n} \\sim \\text{Multinomial}(N, \\mathbf{p}) \\\\\n",
      "p_i \\equiv \\frac{\\theta_i\\lambda_i}{\\theta^T\\lambda}\n",
      "$$\n",
      "that is, $p$ is the normalized elementwise product of $\\theta$ and $\\lambda$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this new variable, the joint distribution can then be written\n",
      "$$\n",
      "p(N, n, \\theta, \\lambda) \\propto \\binom{N}{n} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} e^{-(\\theta_k + b_k)\\lambda_k} \\right) \\delta\\left(\\sum\\theta - 1\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sampling Inference for Model 0"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given the expression above, it seems natural to implement a Gibbs sampling scheme. However, we have the problem that, while the $\\lambda_k$ are each independent (given $\\theta$), the $\\theta_k$ are not, due to the Dirichlet restriction.\n",
      "\n",
      "However, in this case, we can imagine rejection sampling scheme in which we sample $\\theta \\sim \\text{Dir}(n + \\alpha)$ and accept with probability $e^{-\\theta^T \\lambda}$. (Note that this may not be particularly efficient, given the likely values of $\\lambda$.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This suggests the following sampling scheme:\n",
      "\n",
      "$\\alpha$, $a$, $b$, $N$ known.\n",
      "\n",
      "Repeat:\n",
      "\n",
      "1. Draw $\\lambda_k \\sim \\text{Ga}(n_k + a_k, \\theta_k + b_k)$.\n",
      "1. Using either Metropolis-Hastings or rejection sampling, draw $\\theta$.\n",
      "1. Calculate $p = \\theta \\odot \\lambda / \\theta^T \\lambda$. Draw $n \\sim \\text{Mult}(N, p)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variational Inference"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Variational Inference: basic setup"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following Blei, Ng, and Jordan, we can write the marginal distribution (suppressing hyperparameters) as\n",
      "\n",
      "$$\n",
      "\\log p(N) = \\log \\int\\! d\\theta d\\lambda \\; p(N, \\lambda,\\theta) = \\log \\int\\! d\\theta d\\lambda \\; \\frac{p(N, \\lambda,\\theta) q(\\lambda, \\theta)}{q(\\lambda, \\theta)} \\\\\n",
      "\\geq \\log \\int\\! d\\theta d\\lambda \\; q(\\lambda, \\theta) \\left[\\log p(N, \\lambda,\\theta) - \\log q(\\lambda, \\theta) \\right] \\\\\n",
      "= \\mathbb{E}_q[\\log p(N, \\lambda,\\theta)] + H[q] = \\mathcal{L}\n",
      "$$\n",
      "where the inequality follows from Jensen's inequality."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, it can be shown that maximizing this lower bound to the fixed expression on the left-hand side is equivalent to minimizing the KL divergence between $q(\\lambda, \\theta)$ and $p(\\lambda, \\theta|N)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Variational Inference: mean field for exponential families:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we follow the derivation in Appendix A of [Blei and Jordan (2006)](http://projecteuclid.org/euclid.ba/1340371077)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us assume that the variational ansatz $q_\\nu(\\theta)$ for the model parameters $\\theta$ is of the fully-factorized mean-field form:\n",
      "$$\n",
      "q_\\nu(\\theta) = \\prod_j q_{\\nu_j}(\\theta_j)\n",
      "$$\n",
      "and that each factor is a member of the exponential family:\n",
      "$$\n",
      "q_{\\nu_j}(\\theta_j) = h(\\theta_j)e^{\\nu_j^T\\theta_j - a(\\nu_j)}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Under these circumstances, it can then be shown that maximize $\\mathcal{L}$ is equivalent to solving\n",
      "$$\n",
      "\\nu_j = \\frac{1}{a''(\\nu_j)}\\left[\\frac{\\partial}{\\partial \\nu_j}\\mathbb{E}_q[p(\\theta_j|\\theta_{-j}, x)] - \\frac{\\partial}{\\partial \\nu_j}\\mathbb{E}_q[h(\\theta_j)] \\right]\n",
      "$$\n",
      "where $p(\\theta_j|\\theta_{-j}, x)$ is the conditional distribution of $\\theta_j$ given the other model parameters and the data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, there is a further model simplification that occurs when the conditional distribution itself takes an exponential form. When we can write\n",
      "$$\n",
      "p(\\theta_j|\\theta_{-j}, x) = h(\\theta_j)\\exp\\left[g_j(\\theta_{-j}, x)^T \\theta_j - a(g_j(\\theta_{-j}, x))\\right]\n",
      "$$\n",
      "the equation for the optimal $\\nu_j$ reduces to\n",
      "$$\n",
      "\\nu_j = \\mathbb{E}_q[g(\\theta_{-j}, x)]\n",
      "$$\n",
      "That is, the natural parameter in the variational distribution is the expectation over the variational distribution of the natural parameter of the conditional distribution.\n",
      "\n",
      "Thus, in what follows, we will use this formula rather than derive the Lagrangian from scratch."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variational Inference for Model 0"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We begin with the variational ansatz:\n",
      "$$\n",
      "p(\\lambda, \\theta|N) \\approx q(\\lambda, \\theta) = q(\\theta)\\prod_k q(\\lambda_k)\\\\\n",
      "q(\\theta) \\sim \\text{Dir}(\\gamma) \\\\\n",
      "q(\\lambda_k) \\sim \\text{Ga}(c_k, d_k)\n",
      "$$\n",
      "Most importantly, this form is fully factorized, and all distributions are in the exponential form."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, $p(N, \\theta, \\lambda)$ does not factorize, being a sum of terms, and so we cannot easily apply the above formulae. To do so, we will once again add the auxiliary variable $n$ to our variational ansatz. (We can think of this $n$ as something like the number of spikes resulting from each category.)\n",
      "$$\n",
      "q(n) \\sim \\text{Mult}(N, \\pi)\n",
      "$$\n",
      "giving us a revised approximation\n",
      "$$\n",
      "p(\\lambda, \\theta, n|N) \\approx q(\\lambda, \\theta, n) = q(\\theta)q(n)\\prod_k q(\\lambda_k)\\\\\n",
      "q(\\theta) \\sim \\text{Dir}(\\gamma) \\\\\n",
      "q(\\lambda_k) \\sim \\text{Ga}(c_k, d_k) \\\\\n",
      "q(n) \\sim \\text{Mult}(N, \\pi)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a result, we can now easily see that the conditional posterior densities for each variable take the following form:\n",
      "$$\n",
      "p(\\lambda_k|\\lambda_{-k}, \\theta, n, N) \\sim \\text{Ga}(n_k + a_k, \\theta_k + b_k) \\\\\n",
      "p(n|\\lambda, \\theta, N) \\sim \\text{Mult}(N, p)\n",
      "$$\n",
      "where $p_j \\equiv \\theta_j\\lambda_j/\\theta^T\\lambda$.\n",
      "\n",
      "Unfortunately $p(\\theta|\\lambda, n, N)$, while in the exponential family, does not have a simple, closed-form integral. As a result, the simple update formulae will not apply. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model 0'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In contrast to the above, we could imagine that each time point possesses an **unnormalized** set of topic scores, $\\theta$, with\n",
      "$$\n",
      "\\theta_i \\sim \\text{Ga}(\\alpha_i, \\beta_i)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this model, we can repeat the above derivation to arrive at the joint distribution\n",
      "$$\n",
      "p(N, n, \\theta, \\lambda) \\propto \\binom{N}{n} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} e^{-(\\theta_k\\lambda_k + b_k \\lambda_k + \\beta_k\\theta_k)} \\right) \\\\\n",
      "\\propto \\binom{N}{n} \\left(\\prod_k \\lambda_k^{n_k + a_k - 1}\\theta_k^{n_k + \\alpha_k - 1} e^{-(\\theta_k + b_k)(\\lambda_k + \\beta_k)} \\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, simple Gibbs sampling will do, since the conditionals for $\\theta_k$ are now independent and of Gamma form."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note**: In order to have nonzero probability of having some $\\theta_i \\neq 0$, we would need to use an exponential prior on $\\theta$. That is, we would need to set $\\alpha_i = 0$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sampling Inference for Model 0'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Due to the conjugate properties of Model 0', it is straightforward to implement the following Gibbs sampling algorithm:\n",
      "\n",
      "$\\alpha$, $a$, $b$, $N$ known.\n",
      "\n",
      "Repeat:\n",
      "\n",
      "1. Draw $\\lambda_k \\sim \\text{Ga}(n_k + a_k, \\theta_k + b_k)$.\n",
      "1. Draw $\\theta_k \\sim \\text{Ga}(n_k + \\alpha_k, \\lambda_k + \\beta_k)$.\n",
      "1. Calculate $p = \\theta \\odot \\lambda / \\theta^T \\lambda$. Draw $n \\sim \\text{Mult}(N, p)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variational Inference for Model 0'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As above, we begin with an approximating hypothesis for $p(\\lambda,\\theta, n|N)$:\n",
      "$$\n",
      "p(\\lambda, \\theta|N) \\approx q(\\lambda, \\theta) = q(n)\\prod_k q(\\theta_k)q(\\lambda_k)\\\\\n",
      "q(\\theta_k) \\sim \\text{Ga}(\\gamma_k, \\delta_k) \\\\\n",
      "q(\\lambda_k) \\sim \\text{Ga}(c_k, d_k) \\\\\n",
      "q(n) \\sim \\text{Mult}(N, \\pi)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, as opposed to Model 0, all the conditionals take simplified exponential family forms:\n",
      "$$\n",
      "p(\\lambda_k|\\lambda_{-k}, \\theta, n, N) \\sim \\text{Ga}(n_k + a_k, \\theta_k + b_k) \\\\\n",
      "p(\\theta_k|\\lambda, \\theta_{-k}, n, N) \\sim \\text{Ga}(n_k + \\alpha_k, \\lambda_k + \\beta_k) \\\\\n",
      "p(n|\\lambda, \\theta, N) \\sim \\text{Mult}(N, p)\n",
      "$$\n",
      "where $p_j \\equiv \\theta_j\\lambda_j/\\theta^T\\lambda$. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, using the formula from Blei and Jordan above, we can write down the update equations for the natural parameters for each variable:\n",
      "$$\n",
      "\\eta_k^\\lambda = \\begin{bmatrix} c_k - 1 \\\\ -d_k\\end{bmatrix} \\leftarrow \n",
      "\\mathbb{E}_q \\begin{bmatrix} n_k + a_k - 1 \\\\ -\\theta_k -b_k\\end{bmatrix} = \\begin{bmatrix} N\\pi_k + a_k - 1 \\\\ -\\frac{\\gamma_k}{\\delta_k} -b_k\\end{bmatrix} \\\\\n",
      "\\eta_k^\\theta = \\begin{bmatrix} \\gamma_k - 1 \\\\ -\\delta_k\\end{bmatrix} \\leftarrow \n",
      "\\mathbb{E}_q \\begin{bmatrix} n_k + \\alpha_k - 1 \\\\ -\\lambda_k -\\beta_k\\end{bmatrix} = \\begin{bmatrix} N\\pi_k + \\alpha_k - 1 \\\\ -\\frac{c_k}{d_k} -\\beta_k\\end{bmatrix} \\\\\n",
      "\\eta_k^n = \\log\\pi_k \\leftarrow \\mathbb{E}_q\\left[\\log \\frac{\\theta_k\\lambda_k}{\\theta^T\\lambda}\\right] = \\psi(\\gamma_k) -\\log \\delta_k + \\psi(c_k) -\\log d_k - \\mathbb{E}_q[\\log \\theta^T\\lambda]\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now there is no simple expression for the last expectation above, but we can replace it with the tightest first-order lower bound (cf. [Paisley 2010](http://www.columbia.edu/~jwp2128/Teaching/E6892/papers/twobounds.pdf)):\n",
      "$$\n",
      "-\\mathbb{E}_q[\\log \\sum_k \\theta_k\\lambda_k] \\geq -\\log \\sum_k \\mathbb{E}_q[\\theta_k\\lambda_k] =\n",
      "-\\log \\sum_k \\frac{\\gamma_k c_k}{\\delta_k d_k}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result of all this is a simple set of algebraic updates:\n",
      "$$\n",
      "c_k \\leftarrow N\\pi_k + a_k \\\\\n",
      "d_k \\leftarrow b_k + \\frac{\\gamma_k}{\\delta_k} \\\\\n",
      "\\gamma_k \\leftarrow N\\pi_k + \\alpha_k \\\\\n",
      "\\delta_k \\leftarrow \\beta_k + \\frac{c_k}{d_k} \\\\\n",
      "C \\leftarrow \\sum_k \\frac{\\gamma_k c_k}{\\delta_k d_k} \\\\\n",
      "\\log \\pi_k \\leftarrow \\psi(\\gamma_k) -\\log \\delta_k + \\psi(c_k) -\\log d_k - \\log C\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Multiple responding units"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now extend Model 0' to multiple response units, each of which is described by a copy of the above. More specifically, let there be $U$ units and $T$ times, such that the counts are given by $N_{ut}$. Then the distribution of response counts is given by\n",
      "$$\n",
      "N_{tu} \\sim \\text{Pois}((\\Theta\\Lambda)_{tu}) = \\text{Pois}\\left(\\sum_k \\Theta_{tk}\\Lambda_{ku}\\right)\n",
      "$$\n",
      "Thus, we have replaced the vectors $\\theta$ and $\\lambda$ with matrices, where $\\Theta$ is a $T \\times K$ matrix of frame categories with one row per frame and $\\Lambda$ is a $K \\times U$ matrix of neural responses with one column per unit."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In what follows, we will once again introduce the auxiliary variable $n$, which this time forms a $K \\times T \\times U$ 3-tensor satisfying $\\sum_k n_{ktu} = N_{tu}$. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following the above, we write the total joint distribution of the data as\n",
      "$$\n",
      "p(n, \\Lambda, \\Theta) = p(n|\\Lambda, \\Theta) p(\\Lambda) p(\\Theta) \\\\\n",
      "p(n | \\Lambda, \\Theta) \\propto \\left(\\prod_{ktu} [n_{ktu}!]^{-1} \\right)\\exp\\left(\\sum_{ktu} \\left[ -\\theta_{tk} \\lambda_{ku} + n_{ktu} (\\log \\theta_{tk} + \\log \\lambda_{ku})\\right] \\right) \\delta\\left(\\sum_k n_{ktu} - N_{tu}\\right)\\\\\n",
      "p(\\Lambda) \\propto \\exp\\left(\\sum_{ku} \\left[(a_{ku} - 1)\\log \\lambda_{ku} - b_{ku}\\lambda_{ku} \\right] \\right) \\\\\n",
      "p(\\Theta) \\propto \\exp\\left(\\sum_{tk} \\left[(\\alpha_{tk} - 1)\\log \\theta_{tk} - \\beta_{tk}\\theta_{tk} \\right] \\right) \\\\\n",
      "\\Rightarrow p(n, \\Lambda, \\Theta) \\propto \\left(\\prod_{ktu} [n_{ktu}!]^{-1} \\right) \\exp\n",
      "\\left(\n",
      "\\sum_{ktu} \\left[ -\\theta_{tk} \\lambda_{ku} + n_{ktu} (\\log \\theta_{tk} + \\log \\lambda_{ku}) \\right] \\\\\n",
      "+ \\sum_{ku} \\left[ (a_{ku} - 1)\\log \\lambda_{ku} - b_{ku}\\lambda_{ku} \\right] \\\\\n",
      "+ \\sum_{tk} \\left[ (\\alpha_{tk} - 1)\\log \\theta_{tk} - \\beta_{tk}\\theta_{tk} \\right] \n",
      "\\right)\\delta\\left(\\sum_k n_{ktu} - N_{tu}\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clearly, this is in the exponential form we require, and a straightforward generalization of the derivation above gives\n",
      "$$\n",
      "c_{ku} \\leftarrow \\sum_t N_{tu} \\pi_{ktu} + a_{ku} \\\\\n",
      "d_{ku} \\leftarrow \\sum_t \\frac{\\gamma_{tk}}{\\delta_{tk}} + b_{ku} \\\\\n",
      "\\gamma_{tk} \\leftarrow \\sum_u N_{tu} \\pi_{ktu} + \\alpha_{tk} \\\\\n",
      "\\delta_{tk} \\leftarrow \\sum_u \\frac{c_{ku}}{d_{ku}} + \\beta_{tk} \\\\\n",
      "C_{tu} \\leftarrow \\sum_k \\frac{\\gamma_{tk}}{\\delta_{tk}}\\frac{c_{ku}}{d_{ku}} \\\\\n",
      "\\log \\pi_{ktu} \\leftarrow \\psi(\\gamma_{tk}) - \\log \\delta_{tk} + \\psi(c_{ku}) - \\log d_{ku} - \\log C_{tu} \n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For Model 1, we alter the above such that, instead of having a Gamma- or Dirichlet-distributed category loading $\\theta$, we use a binary presence/absence tag $z$. This is related to Beta Process/IBP models in which we truncate to finite $K$ for performing variational inference (cf [Doshi-Velez et al.](http://eprints.pascal-network.org/archive/00006738/)). However, in our case, we let $z$ evolve in time as a Markov model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More explicitly, for $T$ times, $U$ units, and $K$ topics, let\n",
      "$$\n",
      "\\lambda_{ku} \\sim \\text{Ga}(a_{ku}, b_{ku}) \\quad \\text{firing rate for each (topic, unit)} \\\\\n",
      "p(z_{tk}|z_{(t-1)k}) = A^{(k)}_{z_{t}, z_{t-1}} \\quad \\text{a column-stochastic Markov matrix} \\\\\n",
      "z_{0k} \\sim \\text{Bern}(\\zeta_{k}) \\quad \\text{the prior on initial state for each topic} \\\\ \n",
      "A^{(k)}_{1i} \\sim \\text{Be}(\\alpha_{ik}, \\beta_{ik}) \\quad \\text{state transition probabilities for each topic} \\\\\n",
      "N_{tu} \\sim \\text{Pois}\\left(\\sum_k z_{tk}\\lambda_{ku} \\right) \\quad \\text{spike count at time $t$ for unit $u$}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we have written $A$ as a column-stochastic matrix\n",
      "$$\n",
      "A = \n",
      "\\begin{pmatrix}\n",
      "\\nu_{0 \\rightarrow 0} & \\nu_{1 \\rightarrow 0} \\\\\n",
      "\\nu_{0 \\rightarrow 1} & \\nu_{1 \\rightarrow 1}\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "such that the evolution of an initial distribution $p(z_0) = \\pi_0$ evolves in time via\n",
      "$$\n",
      "p(z_t) = A^t\\pi_0\n",
      "$$\n",
      "\n",
      "In what follows, we will write $\\nu^{(k)}_{0 \\rightarrow 1} = \\nu_{0k}$ and $\\nu^{(k)}_{1 \\rightarrow 1} = \\nu_{1k}$, so that the within-state transitions have priors\n",
      "$$\n",
      "\\nu_{ik} \\sim \\text{Be}(\\alpha_{ik}, \\beta_{ik})\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Joint distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to find \n",
      "$$\n",
      "p(N, z, \\lambda) = p(N|z, \\lambda)p(z)p(\\lambda)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As above, we can easily write the first factor:\n",
      "$$\n",
      "p(N|z, \\lambda) \\propto \\prod_{tu} \\left(\\sum_k z_{tk}\\lambda_{ku} \\right)^{N_{tu}} e^{-\\sum_k z_{tk}\\lambda_{ku}}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, as before, this will not take the exponential form unless we introduce the auxiliary variables $n_{ktu}$ with $\\sum_k n_{ktu} = N_{tu}$. With this, we can expand the first factor in the above via the multinomial theorem to write\n",
      "$$\n",
      "p(n|z, \\lambda) \\propto \\prod_{kt} \\left( \\prod_u \\frac{\\lambda_{ku}^{n_{ktu}}}{n_{ktu}!} e^{-\\lambda_{ku}} \\right)^{z_{tk}}\n",
      "$$\n",
      "with $n_{ktu} = 0$ for $z_{tk} = 0$.\n",
      "\n",
      "That is, we have decoupled the total spike count into the results of separate Poisson processes for each latent factor, coupled through the constraint $\\sum_k n_{ktu} = N_{tu}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As above, the prior on $\\lambda$ is easy to write:\n",
      "$$\n",
      "p(\\lambda) \\propto \\prod_{ku} \\lambda_{ku}^{a_{ku} - 1} e^{-b_{ku}\\lambda_{ku}}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the case of $z$, it is unnecessary to calculate the full joint distribution. We merely note that it can be expanded as the conditional\n",
      "$$\n",
      "p(z|A) = \\prod_k p(z_{Tk}|z_{(T-1)k})\\ldots p(z_{2k}|z_{1k})p(z_{1k}|z_{0k})p(z_{0k})\n",
      "$$\n",
      "In addition, then, we need\n",
      "$$\n",
      "p(A) = p(\\nu) \\propto \\prod_{k} \\prod_{j \\in \\{0, 1\\}} \\nu_{jk} (1-\\nu_{jk})\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Marginals"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the marginal distributions, we can easily write\n",
      "$$\n",
      "\\lambda_{ku}|rest \\sim \\text{Ga}\\left( a_{ku} + \\sum_t n_{ktu}, b_{ku} + \\sum_t z_{tk} \\right) \\\\\n",
      "n_{\\bullet tu}|rest \\sim \\text{Mult}(N_{tu}, p_{tu})\n",
      "$$\n",
      "where $p_{ktu} \\equiv \\frac{z_{tk}\\lambda_{ku}}{\\sum_k z_{tk}\\lambda_{ku}}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The conditionals for $z_1 \\ldots z_T$ are slightly more complicated, given that $z_t$ is affected by both $z_{t-1}$ and $z_{t+1}$. Let $(z_{t+1}, z_{t-1}) = (i, j)$. Then we can write:\n",
      "$$\n",
      "p(z_{tk}=0|rest) \\propto A^{(k)}_{i \\rightarrow 0} A^{(k)}_{0 \\rightarrow j} = (1 - \\nu_{ik})\\nu_{0k}^{z_{t+1,k}}(1 -\\nu_{0k})^{1 - z_{t+1,k}} \\delta_{n_{ktu}, 0}\\\\\n",
      "p(z_{tk}=1|rest) \\propto \\left(\\prod_u \\frac{\\lambda_{ku}^{n_{ktu}}e^{-\\lambda_{ku}} }{n_{ktu}!} \\right)  A^{(k)}_{i \\rightarrow 1} A^{(k)}_{1 \\rightarrow j} = \\left(\\prod_u \\frac{\\lambda_{ku}^{n_{ktu}}e^{-\\lambda_{ku}} }{n_{ktu}!} \\right) \\nu_{ik}\\nu_{1k}^{z_{t+1,k}}(1 -\\nu_{1k})^{1 - z_{t+1,k}} \\\\\n",
      "p(z_{0k}=0|rest) \\propto (1 - \\zeta_k)\\nu_{0k}^{z_{t+1,k}}(1 -\\nu_{0k})^{1 - z_{t+1,k}} \\\\\n",
      "p(z_{0k}=1|rest) \\propto \\zeta_k\\nu_{1k}^{z_{t+1,k}}(1 -\\nu_{1k})^{1 - z_{t+1,k}}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we can calculate the marginals for the $\\nu_{ik}$. In the chain $z_k$, we have $T$ transitions with\n",
      "$$\n",
      "p(\\nu_{ik}|rest) \\propto \\nu_{ik}^{m_{i\\rightarrow 1} + \\alpha_{ik}} (1 -\\nu_{ik})^{m_{i\\rightarrow 0} + \\beta_{ik}}\n",
      "$$\n",
      "Thus\n",
      "$$\n",
      "\\nu_{ik}|rest \\sim \\text{Be}(m_{i\\rightarrow 1,k} + \\alpha_{ik}, m_{i\\rightarrow 0,k} + \\beta_{ik})\n",
      "$$\n",
      "where $m_{i\\rightarrow j,k}$ is the total number of transitions from $i\\rightarrow j$ in chain $k$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variational Ansatz"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once again, we choose a fully-factorized ansatz for the posterior:\n",
      "$$\n",
      "q(n, z, \\lambda, \\nu) = q(n)q(z)q(\\lambda)q(\\nu) = \n",
      "\\left(\\prod_{tu} q(n_{\\bullet tu}, z_{t\\bullet}) \\right) \\left(\\prod_{ku} q(\\lambda_{ku}) \\right) \n",
      " \\left(\\prod_{ik} q(\\nu_{ik}) \\right)\\\\\n",
      "\\lambda_{ku} \\sim \\text{Ga}(c_{ku}, d_{ku}) \\\\\n",
      "\\nu_{ik} \\sim \\text{Be}(\\gamma_{ik}, \\delta_{ik}) \\\\\n",
      "q(n_{\\bullet tu}, z_{t\\bullet}) = \\binom{N_{tu}}{n_{\\bullet tu}} \n",
      "\\prod_k \\xi_{tk}^{z_{tk}} (1 - \\xi_{tk})^{1 - z_{tk}} (z_{tk}\\pi_{ktu})^{n_{ktu}}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That is, we will model $n$ and $z$ jointly, in order to capture the fact that $z = 0 \\Rightarrow n = 0$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variational Updates"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Noting that the ansatz is a fully factorized exponential family form and that the single-variable conditionals are as well, we can use the Blei and Jordan result above \n",
      "$$\n",
      "\\eta_i = \\mathbb{E}_q[g_i(\\theta_{-i}, x)]\n",
      "$$\n",
      "where $\\eta_i$ is the natural parameter for variable $\\theta_i$ in the ansatz and $g_i(\\theta_{-i}, x)$ is the natural parameter conjugate to $\\theta_i$ in the conditional distribution.\n",
      "\n",
      "Using this, we can now write:\n",
      "$$\n",
      "c_{ku} \\leftarrow a_{ku} + \\sum_t N_{tu}\\pi_{ktu} \\\\\n",
      "d_{ku} \\leftarrow b_{ku} + \\sum_t \\xi_{tk} \\\\\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For $\\nu_{ik}$ we have\n",
      "$$\n",
      "\\gamma_{ik} \\leftarrow \\mathbb{E}_q[m_{i\\rightarrow 1, k} + \\alpha_{ik}] \n",
      "= \\sum_{t}\\xi_{tk} \\xi_{t-1,k}^i (1 - \\xi_{t-1,k})^{1-i} + \\alpha_{ik} \\\\\n",
      "\\delta_{ik} \\leftarrow \\mathbb{E}_q[m_{i\\rightarrow 0, k} + \\beta_{ik}] \n",
      "= \\sum_{t}(1 - \\xi_{tk}) \\xi_{t-1,k}^i (1 - \\xi_{t-1,k})^{1-i} + \\beta_{ik}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The trickiest updates are those for $\\xi$ and $\\pi$ (i.e., $z$ and $n$). To derive those, we will do the full derivation. To start, let \n",
      "$$\n",
      "h(n) = \\binom{N}{n}\n",
      "$$\n",
      "be the base measure of $q(n, z)$ above, where we suppress the $t$ and $u$ indices for clarity in what follows. Then\n",
      "$$\n",
      "\\mathbb{E}_q[\\log q] = \\mathbb{E}_q[\\log h] + \n",
      "\\sum_k \\mathbb{E}_q\n",
      "\\left[\n",
      "z_k \\log \\xi_k + (1 - z_k) \\log(1-\\xi_k) + n_k \\log \\pi_k + n_k \\log z_k\n",
      "\\right] \\\\\n",
      "= \\mathbb{E}_q[\\log h] + \n",
      "\\sum_k \\left[\n",
      "\\xi_k \\log \\xi_k + (1 - \\xi_k) \\log(1-\\xi_k) + N\\xi_k\\pi_k \\log \\pi_k\n",
      "\\right]\n",
      "$$\n",
      "where we have used the results\n",
      "$$\n",
      "\\mathbb{E}_q[z_k] = \\xi_k \\\\\n",
      "\\mathbb{E}_q[n_k] = \\xi_k N \\pi_k\n",
      "$$\n",
      "and\n",
      "$$\n",
      "\\mathbb{E}_q[n_k \\log z_k] = 0\n",
      "$$\n",
      "since $z_k = 0 \\Rightarrow n_k = 0$ and $z_k = 1 \\Rightarrow \\log z_k = 0$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Along similar lines\n",
      "$$\n",
      "\\mathbb{E}_q[\\log p(z, n|rest)] = \\mathbb{E}_q[\\log h] + \n",
      "\\mathbb{E}_q \\left[\n",
      "\\sum_{k = 1}^K n_k \\log \\frac{\\lambda_k z_k}{\\lambda_0 z_0} + N \\log \\frac{\\lambda_0 z_0}{\\lambda^T z)}\n",
      "\\right] \\\\\n",
      "= \\mathbb{E}_q[\\log h] + N \\sum_{k=1}^K \\left[\n",
      "\\pi_k \\xi_k (\\psi(c_k) - \\log d_k) - \\pi_k \\xi_0 (\\psi(c_0) - \\log d_0) \\right]\n",
      "+ N \\xi_0 (\\psi(c_0) - \\log d_0) - N \\mathbb{E}_q\\left[\\log \\lambda^T z\\right] \\\\\n",
      "= N \\sum_{k=1}^K \\pi_k \\xi_k (\\psi(c_k) - \\log d_k) + N \\left(1 - \\sum_{k=1}^K \\pi_k \\right) \\xi_0 (\\psi(c_0) - \\log d_0) - N \\mathbb{E}_q\\left[\\log \\lambda^T z\\right] \\\\\n",
      "= \\sum_{k=0}^K \\pi_k \\xi_k (\\psi(c_k) - \\log d_k) - N \\mathbb{E}_q\\left[\\log \\lambda^T z\\right]\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This last term we can simplify by using Paisley's first lower bound, which yields\n",
      "$$\n",
      "-\\mathbb{E}_q\\left[\\log(\\lambda^T z) \\right] = \n",
      "\\geq -\\log \\sum_j \\mathbb{E}_{q}[ \\lambda_j z_j  ]\n",
      "= -\\log \\bar{\\lambda}^T \\xi\n",
      "$$\n",
      "with $\\bar{\\lambda}_j \\equiv c_j/d_j$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, for the multinomial distribution, there are multiple ways to deal with the constraint that $\\sum_k \\pi_k = 1$. One way, which we will adopt, is to set $\\pi_0 = 1 - \\sum_{k=1}^K \\pi_k$ and only treat $\\pi_k$ as parameters for $K \\geq 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can thus take the derivative of the Lagrangian:\n",
      "$$\n",
      "\\frac{\\partial}{\\partial \\pi_k} \\left[\n",
      "\\mathbb{E}_q[\\log p(z, n|rest)] - \\mathbb{E}_q[\\log q]\n",
      "\\right] \\\\\n",
      "= \\frac{\\partial}{\\partial \\pi_k} \\left[\n",
      "\\mathbb{E}_q[\\log h] + \n",
      "N \\sum_k \\left(\\xi_k\\pi_k (\\psi(c_k) - \\log d_k)\\right) - N \\log \\bar{\\lambda}^T \\xi \\\\\n",
      "- ( \\mathbb{E}_q[\\log h] + \n",
      "\\sum_k \\left[\n",
      "\\xi_k \\log \\xi_k + (1 - \\xi_k) \\log(1-\\xi_k) + N\\xi_k\\pi_k \\log \\pi_k\n",
      "\\right] )\n",
      "\\right] \\\\\n",
      "= N\\left( \\xi_k (\\psi(c_k) - \\log d_k) - \n",
      "\\xi_0 (\\psi(c_0) - \\log d_0)\\right)\n",
      "- N\\left( \\xi_k (\\log \\pi_k + 1) - \\xi_0 (\\log \\pi_0 + 1) \\right) \\\\\n",
      "= N\\left[  \\xi_k (\\psi(c_k) - \\log d_k - \\log \\pi_k - 1 ) - \n",
      "\\xi_0 (\\psi(c_0) - \\log d_0 - \\log \\pi_0 - 1 ) \n",
      "\\right] = 0\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}