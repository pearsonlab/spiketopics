\documentclass[11pt]{article}
\usepackage{amssymb,amsmath}

\begin{document}
\title{The Gamma-Poisson Model}
\author{John Pearson}
\maketitle

\section{Observation Model}
We assume a Poisson process with $T$ observation times, $N$ observation units, and $K$ latent states. The counts for each unit at each time are then given by 

\begin{equation}
    N_{tu} \sim \mathrm{Poisson}(\mu_{tu})    
\end{equation}
with 
\begin{equation}
   \mu_{tu} = (ZB)_{tu} = \sum_k z_{tk} b_{ku} 
\end{equation}
wit $z$ the time series of latent states (assumed binary) and $b$ the regression coefficients. In addition, we assume the $z_t$ have independent Markov transition dynamics within each latent state (see below).

In addition, it will be more convenient for our purposes to rewrite the above expression using $b_{ku} = \log \lambda_{ku}$, so that 
\begin{equation}
    p(N_{tu}|\lambda, z) = \frac{\prod_k \lambda_{ku}^{N_{tu} z_{tk}}}{N_{tu}!} e^{-\prod_k \lambda_{ku}^{z_{tk}}}
\end{equation}
which can be rewritten
\begin{equation}
    \log p(N_{tu}|\lambda, z) = N_{tu} \sum_k z_{tk} \log \lambda_{ku} - \prod_k \lambda_{ku}^{z_{tk}} - \log N_{tu}!
\end{equation}
Clearly, if we treat $N$ and $z$ as constants, we have
\begin{equation}
    \lambda_{ku}|N, z \sim \mathrm{Gamma}(\sum_t N_{tu}z_{tk} + 1, \sum_t\prod_{k'\neq k} \lambda_{k'u}^{z_{tk'}})
\end{equation}

\section{Hidden Markov Model}
Here, we assume a hidden Markov model for each latent state. That is, we take
\begin{align}
    A_{ij} &\equiv p(z_{t+1}=i|z_t = j) \\
    \pi_i &\equiv p(z_0 = i)
\end{align}
(Note that this means that the \emph{columns} of $A$ sum to 1, which is the opposite of the usual convention. In other words, in matrix notation, $z_{t+1} = A \cdot z_t$.)

Given this notation, it is straightforward to write the probability of a sequence of hidden states, conditioned on the chain parameters
\begin{equation}
    \log p(z|A, \pi) = \sum_t \log A_{z_{t+1} z_t} + \log \pi_{z_0}
\end{equation}

\section{Priors}
We would also like to put priors on the parameters of the model. For $\lambda$, this is straightforward:
\begin{equation}
    \lambda_{ku} \sim \mathrm{Gamma}(c_{ku}, d_{ku})
\end{equation}

For the parameters of the Markov chain, we have the restriction that the columns must sum to 1, so we place priors on the probability of $z_{0k} = 1$
\begin{equation}
    \pi_{k} \sim \mathrm{Beta}(\rho_{k1}, \rho_{k2})
\end{equation}
and the probabilities of transition \emph{into} the $z_{tk} = 1$ state:
\begin{equation}
    A_{k1i} \sim \mathrm{Beta}(\nu_{ki1}, \nu_{ki2})
\end{equation}

\section{Variational Ansatz}
We would like to approximate the joint posterior density
\begin{equation}
    p(\lambda, z, A, \pi|N) \propto p(N, z|\lambda, A, \pi) p(\lambda) p(A) p(\pi)
\end{equation}
with a structured mean field form that factorizes over chains:
\begin{equation}
     q(\lambda, z, A, \pi) = \prod_{ku} q(\lambda_{ku}) q(z_k) q(A_k) q(\pi_k)
\end{equation} 
Given the form taken by the model above, we can readily write down the ansatz
\begin{align} 
    \lambda_{ku} &\sim \mathrm{Gamma}(\alpha_{ku}, \beta_{ku}) \\
    z_{1:T, k} &\sim \mathrm{HMM}(\eta_k, \tilde{A}_k, \tilde{\pi}_k) \\
    A_{k1i} &\sim \mathrm{Beta}(\gamma_{ki1}, \gamma_{ki2}) \\
    \pi_k &\sim \mathrm{Beta}(\delta_{k1}, \delta_{k2})
\end{align}
where $\mathrm{HMM}(\ldots)$ denotes the joint posterior over hidden states at each moment in time:
\begin{equation}
    p(z_{1:T}|N, \theta) = \frac{p(z_{1:T}, N|\theta)}{Z}
\end{equation}
where $Z \equiv p(N|\theta)$ and $\theta \equiv (\lambda, A, \pi)$. As we shall see, this posterior is straightforward to calculate via the forward-backward algorithm.

\section{HMM Inference}
Given parameters $\theta \equiv (\lambda, A, \pi)$, the well-known Forward-Backward Algorithm returns the following:
\begin{align}
    \xi_{t} \equiv p(z_{t}|N, \theta) \qquad &\text{posterior marginals}\\
    \Xi_{t, ij} \equiv p(z_{t+1} = j, z_{t} = i|N, \theta) \qquad &\text{two-slice marginals}\\
    \log Z_{t} = \log p(N_{t+1, \bullet}|N_{t\bullet}, \theta) \qquad &\text{partition function}
\end{align}
The first two will be helpful in calculating expections with respect to $q(z)$, while the last gives us the normalization for the joint posterior over all $z$: $\log Z = \sum_t \log Z_{t} = \log p(N_{1:T}|\theta)$. 

\section{Evidence Lower Bound (ELBO)}
We begin by writing the joint distribution of the data:
\begin{multline}
    \log p(N, \lambda, z, A,\pi) = \sum_{ktu}\left[ N_{tu} z_{tk} \log \lambda_{ku} - \prod_k \lambda_{ku}^{z_{tk}} - \log N_{tu}!\right] \\
    + \sum_{tk} \log (A_k)_{z_{t+1, k} z_{t, k}} + \log (\pi_k)_{z_{0k}} \\
    + \log p(\lambda) + \log p(A) + \log p(\pi) 
\end{multline}

We would like to calculate 
\begin{multline}
    \mathcal{L} = \mathbb{E}_q\left[\log \frac{p}{q}\right] = \mathbb{E}_{q(\pi)} \left[\log \frac{p(\pi)}{q(\pi)} \right] + \mathbb{E}_{q(A)} \left[\log \frac{p(A)}{q(A)} \right] + \mathbb{E}_{q(\lambda)} \left[\log \frac{p(\lambda)}{q(\lambda)} \right] \\ + \mathbb{E}_{q}\left[ \log \frac{p(N, z|\lambda, A, \pi)}{q(z)}\right] 
\end{multline}
which we will do in pieces. 

\subsection{$\pi$}
First, we have
\begin{equation}
    \mathbb{E}_{q(\pi)} \left[\log \frac{p(\pi)}{q(\pi)} \right] = \sum_k \left[(\rho_{k1} - 1)\overline{\log \pi_{k1}} + (\rho_{k2} - 1) \overline{\log \pi_{k0}} - \log B(\rho_{k1}, \rho_{k2}) + H(\pi_k) \right]
\end{equation}
with
\begin{align}
    \overline{\log \pi_{k1}} &= \psi(\delta_{k1}) - \psi(\delta_{k1} + \delta_{k2}) \\
    \overline{\log \pi_{k0}} &= \psi(\delta_{k2}) - \psi(\delta_{k1} + \delta_{k2})
\end{align}
with $\psi$ the digamma function and 
\begin{equation}
    H(\pi_k) = H_b(\delta_{k1}, \delta_{k2})
\end{equation}
with $H_b$ the entropy of the beta distribution:
\begin{equation}
    H_b(\alpha, \beta) = \log B(\alpha, \beta) - (\alpha - 1) \psi(\alpha) - (\beta - 1) \psi(\beta) + (\alpha + \beta - 2)\psi(\alpha + \beta)
\end{equation}

\subsection{$A$} 
Similarly,
\begin{equation}
    \mathbb{E}_{q(A)} \left[\log \frac{p(A)}{q(A)} \right] = 
\sum_{ik} \left[ (\nu_{ki1} - 1) \overline{\log A_{k1i}} + (\nu_{ki2} - 1) \overline{\log A_{k0i}} - \log B(\nu_{ki1}, \nu_{ki2}) + H(A_{k1i}) \right]
\end{equation}
where 
\begin{align}
    \overline{\log A_{k1i}} &= \psi(\gamma_{ki1}) - \psi(\gamma_{ki1} + \gamma_{ki2}) \\
    \overline{\log A_{k0i}} &= \psi(\gamma_{ki2}) - \psi(\gamma_{ki1} + \gamma_{ki2})
\end{align}
and again
\begin{equation}
    H(A_{k1i}) = H_b(\gamma_{ki1}, \gamma_{ki2})
\end{equation}

\subsection{$\lambda$}
Next, we want to calculate terms involving $\lambda$, for which we will use some properties of Gamma distributions:
\begin{align}
   \mathbb{E}[\lambda] &= \frac{\alpha}{\beta} \\
   \mathbb{E}[\log \lambda] &= \psi(\alpha) - \log \beta \\
   H_g(\alpha, \beta) &= \alpha - \log \beta + \log \Gamma(\alpha) + (1 - \alpha)\psi(\alpha) 
\end{align}
to write
\begin{equation}
    \mathbb{E}_{q(\lambda)} \left[\log \frac{p(\lambda)}{q(\lambda)} \right] = \sum_{ku} \left[ (c_{ku} - 1)\overline{\log \lambda_{ku}} + d_{ku} \frac{\alpha_{ku}}{\beta_{ku}} + H_g(\alpha_{ku}, \beta_{ku})\right]
\end{equation}
with
\begin{equation}
    \overline{\log \lambda_{ku}} = \psi(\alpha_{ku}) - \log \beta_{ku}
\end{equation}

\subsection{Observation model}
Finally, we would like to calculate
\begin{multline}
    \mathbb{E}_{q}\left[ \log \frac{p(N, z|\lambda, A, \pi)}{q(z)}\right] = \sum_{ktu} N_{tu}\xi_{tk}\overline{\log \lambda_{ku}} - \sum_{tu} \prod_k \left( 1 - \xi_{tk} + \xi_{tk} \frac{\alpha_{ku}}{\beta_{ku}}\right) 
    \\
    + \sum_{kt} \left[\mathrm{tr}\left(\Xi_{kt} \overline{\log A_k^T}\right) + \xi_{0k}^T \overline{\log \pi_k} \right]
    \\
    - \sum_{kt} \left[ \xi_{tk}^T \eta_{tk} + \mathrm{tr}\left(\Xi_{kt} \tilde{A}_k^T\right) + \xi_{0k}^T \tilde{\pi}_k - \log Z_{kt} \right]
\end{multline}
where we make use of the outputs of the forward-backward algorithm and the vector/matrix representations of $\tilde{A}$, $\tilde{\pi}$, $\eta$, $\xi$, and $\Xi$. 

\section{Variational Updates}
Technically, the variational parameters above are $(\alpha, \beta, \gamma, \delta, \eta, \tilde{A}, \tilde{\pi})$. However, $\xi = \xi(\eta, \tilde{A}, \tilde{\pi})$ and $\Xi = \Xi(\eta, \tilde{A}, \tilde{\pi})$ so we can instead treat these as the effective variational parameters. This then readily gives as updates:
\begin{align}
    \eta_{tk} &\leftarrow 
    \begin{pmatrix}
        -\sum_u F_{tku} \\
        \sum_u N_{tu} \overline{\log \lambda_{ku}} -
        \sum_u \frac{\alpha_{ku}}{\beta_{ku}} F_{tku} 
    \end{pmatrix} \\
    \tilde{A}_{k} &\leftarrow \overline{\log A_k} \\
    \tilde{\pi}_k &\leftarrow \overline{\log \pi_k}
\end{align}
where we define
\begin{equation}
    F_{tku} \equiv \prod_{j \neq k} \left( 1 - \xi_{tj} + \xi_{tj} \frac{\alpha_{ju}}{\beta_{ju}}\right)
\end{equation}
We note, after Beal, that as a result of these updates, $\tilde{A}$ and $\tilde{\pi}$ are subadditive (i.e., they do not sum to 1), but that the forward-backward algorithm nonetheless returns a correctly normalized posterior.


Similarly, variation with respect to the prior parameters for $(\lambda, A, \pi)$ gives
\begin{align}
    \alpha_{ku} &\leftarrow \sum_t N_{tu} \xi_{tk} + c_{ku} \\
    \beta_{ku} &\leftarrow \sum_t F_{tku}\xi_{tk} + d_{ku} \\
    \gamma_{ki1} &\leftarrow \nu_{ki1} + \sum_t \Xi_{kt, 1i} \\
    \gamma_{ki2} &\leftarrow \nu_{ki2} + \sum_t \Xi_{kt, 0i} \\
    \delta_{k1} &\leftarrow \rho_{k1} + \xi_{0k} \\
    \delta_{k2} &\leftarrow \rho_{k2} + 1 - \xi_{0k} 
\end{align}
These, then, give the proper updates for coordinate ascent.

\section{Multiple observations}
We have thus far used the time index $t$ assuming no ambiguity between stimulus time and presentation time. In the case that each stimulus is presented once, this presents no problem, but here we extend the analysis to the case where some stimuli are presented multiple times. To do so, we will continue to let $t$ designate the time within the stimulus but introduce an additional index $m$ that takes on a unique value for each stimulus presentation. Thus, different trials will have different values of $m$, but may have the same value of $t$.

We can easily modify the observation model to incorporate this setup. Instead of observing a single count $N_{tu}$ for a given unit at a given stimulus time, we observe a \emph{set} of counts $\{ N_{m u} \vert t(m) = t \}$. (Here, we abuse notation by writing $t$ as both the index and the mapping from presentation index to stimulus time.) If we assume trials are independent, the log likelihoods of these observations simply add, in which case we simply replace sums over $t$ with sums over $m$: 
\begin{align}
    \sum_{ktu} N_{tu} \xi_{tk} \overline{\log \lambda_{ku}} &\rightarrow \sum_{km u} N_{m u} \xi_{t(m) k} \overline{\log \lambda_{ku}} \\
    \sum_{tu} \prod_k \left( 1 - \xi_{tk} + \xi_{tk} \frac{\alpha_{ku}}{\beta_{ku}}\right) &\rightarrow \sum_{m u} \prod_k \left( 1 - \xi_{t(m) k} + \xi_{t(m) k} \frac{\alpha_{ku}}{\beta_{ku}}\right)
\end{align}
Clearly, if we group the sum over $m$ into sums over trials at the same stimulus time $t$, we can easily rewrite these expressions in the form
\begin{align}
    \sum_{km u} N_{m u} \xi_{t(m) k} \overline{\log \lambda_{ku}} &= \sum_{ktu} \hat{N}_{tu} \xi_{tk} \overline{\log \lambda_{ku}} \\
    \sum_{m u} \prod_k \left( 1 - \xi_{t(m) k} + \xi_{t(m) k} \frac{\alpha_{ku}}{\beta_{ku}}\right) &= \sum_{tu} M_{tu}\prod_k \left( 1 - \xi_{tk} + \xi_{tk} \frac{\alpha_{ku}}{\beta_{ku}}\right)
\end{align}
where we have defined
\begin{align}
    \hat{N}_{tu} = \sum_{m:\, t(m) = t} N_{m u} \\
    M_{tu} = \sum_{m:\, t(m) = t} 1
\end{align}
as the cumulative spike count and effective number of observations at each stimulus time, respectively. Note that the result of this process has simply been to replace $N \rightarrow \hat{N}$, $F \rightarrow M F$, meaning we can write modified update equations as follows:
\begin{align}
    \eta_{tk} &\leftarrow 
    \begin{pmatrix}
        -\sum_u M_{tu} F_{tku} \\
        \sum_u \hat{N}_{tu} \overline{\log \lambda_{ku}} -
        \sum_u M_{tu}\frac{\alpha_{ku}}{\beta_{ku}} F_{tku} 
    \end{pmatrix} \\
    \alpha_{ku} &\leftarrow \sum_t \hat{N}_{tu} \xi_{tk} + c_{ku} \\
    \beta_{ku} &\leftarrow \sum_t M_{tu} F_{tku}\xi_{tk} + d_{ku} \\
\end{align}
In fact, for simplicity in what follows, we will drop the hat notation on $N$, with the understanding that when $N$ appears with a $t$ index, it is the sum over all counts at that time.

\section{Overdispersion}
To model overdispersion in our count data, we can include an additional moment-by-moment fluctuation in the firing rate of each unit:
\begin{equation}
    (\lambda_{\mathrm{eff}})_{tu} = \prod_k \lambda_{ku}^{z_{tk}}\theta_{tu}
\end{equation}
If, like $\lambda$, $\theta$ is assumed to be gamma-distributed, then this is equivalent (upon marginalizing $\theta$) to a negative binomial model for the counts $N$. However, it will be simpler for the variational updating if we retain $\theta$ and include additional terms in the pieces of the evidence lower bound above:
\begin{align}
    \sum_{ktu} N_{tu} \xi_{tk} \overline{\log \lambda_{ku}} &\rightarrow \sum_{kmu} N_{mu} \xi_{t(m)k} \overline{\log \lambda_{ku}} + \sum_{mu} N_{mu} \overline{\log \theta_{mu}} \\
    - \sum_{tu} \prod_k \left( 1 - \xi_{tk} + \xi_{tk} \frac{\alpha_{ku}}{\beta_{ku}}\right) &\rightarrow - \sum_{tu} M_{tu} \frac{\omega_u}{\zeta_u} \prod_k \left( 1 - \xi_{tk} + \xi_{tk} \frac{\alpha_{ku}}{\beta_{ku}}\right)
\end{align}
Note in particular that the overdispersion term is assumed to always be present, and that it is unique to every stimulus presentation ($m$ index), not merely every moment during the stimulus itself ($t$ index). Nevertheless, the fact that the prior for this term depends only on the unit, $u$, means that we can aggregate sufficient statistics across repetitions, eliminating the $m$ index in favor of $t$.

Finally, we will need to add a piece to the variational objective combining the prior on $\theta$ and the variational posterior. We will assume that the overdispersion varies across units but not in time:
\begin{align}
    p(\theta_{mu}) &= \mathrm{Gamma}(s_u, r_u) \\
    q(\theta_{mu}) &= \mathrm{Gamma}(\omega_u, \zeta_u)
\end{align}
yielding
\begin{multline}
    \mathbb{E}_{q(\theta)} \left[ \log \frac{p(\theta)}{q(\theta)} \right] =
    \sum_{mu} \left[ (s_u - 1) \overline{\log \theta_{mu}} - r_u \frac{\omega_u}{\zeta_u} + H_g(\omega_u, \zeta_u) \right] =\\
    \sum_{u} M_u \left[ (s_u - 1) (\psi(\omega_u) - \log \zeta_u) - r_u \frac{\omega_u}{\zeta_u} + H_g(\omega_u, \zeta_u) \right]
\end{multline}
with $M_u \equiv \sum_t M_{tu}$ the total number of observations for unit $u$ across all time and $H_g$ the differential entropy for the gamma distribution, as above. By analogy with $\lambda$, we can easily write down the updates for $\omega$ and $\zeta$
\begin{align}
    \omega_u &\leftarrow \sum_m N_{mu} + s_u = \sum_t N_{tu} + s_u\\
    \zeta_u &\leftarrow \sum_{m} F_{t(m)u} + r_u = \sum_{t} M_{tu} F_{tu} + r_u
\end{align}
with 
\begin{equation}
    F_{tu} \equiv \prod_k \left( 1 - \xi_{tk} + \xi_{tk} \frac{\alpha_{ku}}{\beta_{ku}}\right)
\end{equation}
Similarly, the update for the local emission probability, $\eta$ takes the form
\begin{equation}
    \eta_{tk} \leftarrow 
    \begin{pmatrix}
        -\sum_{u} M_{tu} \frac{\omega_u}{\zeta_u} F_{tku} \\
        \sum_u N_{tu} \overline{\log \lambda_{ku}} -
        \sum_u M_{tu} \frac{\omega_u} {\zeta_u}\frac{\alpha_{ku}}{\beta_{ku}} F_{tku} 
    \end{pmatrix} 
\end{equation}
All other updates should remain unchanged.

\section{External covariates}
In addition, we might also want to include external covariates $X(t)$ with a linear effect on log firing rate:
\begin{equation}
    \log \lambda (t) \propto X(t) \cdot \mathbf{w}
\end{equation}
with $X(t)$ a matrix with one column for each (potentially) time-varying regressor, one row per unit, and $\mathbf{w}$ a column vector of weights. In typical linear models, we would choose a normal prior on the weights, but in this case, with the Poisson observation model, that results in a non-conjugate form of the variational objective (the regression piece in $\lambda$ is log-normal, which is non-conjugate to the Poisson). As such, we will rewrite the above as an $X$-dependent gain change of firing rate:
\begin{equation}
    \lambda_{u}(t) \propto \prod_i \upsilon_{iu}^{x_{iu}(t)}
\end{equation}
where $i$ indexes the particular regressor. Thus the total expression for firing rate for a given unit $u$ at time $t$ for a given presentation $m$ with overdispersion is
\begin{equation}
    \lambda_{mu} = \theta_{mu}\left(\prod_k \lambda_{ku}^{z_{t(m)k}} \right) \left(\prod_i \upsilon_{iu}^{x_{t(m)iu}} \right)
\end{equation}
Note that we have discretized $t$ as above and assume that the external covariates $x$ depend only on the stimulus time, not the particular presentation, so that they carry an index $t(m)$, while $\theta$ carries an index particular to each repeated presentation.

It will be convenient for us to assume that both the priors and the variational posterior for $\upsilon$ are gamma in form:
\begin{align}
    p(\upsilon_{iu}) &= \mathrm{Gamma}(v_{iu}, w_{iu}) \\
    q(\upsilon_{iu}) &= \mathrm{Gamma}(a_{iu}, b_{iu}) 
\end{align}
This contributes an additional factor to the ELBO
\begin{multline}
    \mathbb{E}_{q(\upsilon)} \left[ \log \frac{p(\upsilon)}{q(\upsilon)} \right] =
    \sum_{iu} \left[ (v_{iu} - 1) \overline{\log \upsilon_{iu}} - w_u \frac{a_{iu}}{b_{iu}} + H_g(a_{iu}, b_{iu}) \right] =\\
    \sum_{iu} \left[ (v_{iu} - 1) (\psi(a_{iu}) - \log b_{iu}) - w_u \frac{a_{iu}}{b_{iu}} + H_g(a_{iu}, b_{iu}) \right]
\end{multline}
plus corrections to the observation probability piece of $\mathbb{E}_q[\log p(N, z|\lambda, A, \pi) / q(z)]$:
\begin{multline}
    \sum_{ktu} N_{tu} \xi_{tk} \overline{\log \lambda_{ku}} + \sum_{mu} N_{mu} \overline{\log \theta_{mu}} + \sum_{tiu} N_{tu} x_{tiu} \overline{\log \upsilon_{iu}} \\
    - \sum_{mu} \theta_{mu} \prod_k \left( 1 - \xi_{t(m)k} + \xi_{t(m)k} \frac{\alpha_{ku}}{\beta_{ku}}\right)\prod_i \left(\frac{a_{iu}}{b_{iu}} \right)^{x_{t(m)iu}}
\end{multline}

This leads straightforwardly to updates for the posterior parameters of $q(\upsilon)$:
\begin{align}
    a_{iu} &\leftarrow \sum_t N_{tu} x_{tiu} + v_{iu} \\
    b_{iu} &\leftarrow b^*_{iu}
\end{align}
where $b^*_{iu}$ is the solution to the transcendental equation
\begin{equation}
    b_{iu} = w_{iu} + \sum_{m} \left[ \theta_{mu} \prod_k \left( 1 - \xi_{t(m)k} + \xi_{t(m)k} \frac{\alpha_{ku}}{\beta_{ku}}\right)\prod_{j \neq i} \left(\frac{a_{ju}}{b_{ju}} \right)^{x_{t(m)ju}} \right] \left( \frac{a_{iu}}{b_{iu}}\right)^{x_{t(m)iu} - 1}
\end{equation}

Possible strategies here:
\begin{itemize}
    \item Update $a$ exactly. Use a Newton's Method approach to solve for $b$.
    \item Update $a$ exactly. Use a single Newton step on $b$.
    \item Gradient descent on $a$ and $b$ together.
\end{itemize}

\end{document}